"""íŒ¨í„´ ê´€ë¦¬ì (Reference v3.0 ê¸°ë°˜ ê³ ë„í™”)

AIë¥¼ ì‚¬ìš©í•˜ì—¬ ì†Œì„¤ì˜ ìµœì  ì±•í„° ë¶„í•  íŒ¨í„´ì„ ì°¾ì•„ë‚´ê³  ê²€ì¦
NovelAIze-SSR v3.0ì˜ ê³ í’ˆì§ˆ í”„ë¡¬í”„íŠ¸ ë³µì› ë° 99% ì»¤ë²„ë¦¬ì§€ ì¶”ì  ë¡œì§ ì ìš©
"""

import re
import time
import os
from typing import Optional, Tuple, List, Dict, Any
from novel_total_processor.stages.sampler import Sampler
from novel_total_processor.stages.splitter import Splitter
from novel_total_processor.ai.gemini_client import GeminiClient
from novel_total_processor.utils.logger import get_logger

logger = get_logger(__name__)


class PatternManager:
    """AIë¥¼ ì‚¬ìš©í•˜ì—¬ ì†Œì„¤ì˜ ìµœì  ì±•í„° ë¶„í•  íŒ¨í„´ì„ ì°¾ì•„ë‚´ê³  ê²€ì¦ (v3.0 Reference)
    
    Enhanced Features:
    - Dynamic gap detection based on expected chapter count and average size
    - AI-based title candidate extraction with consensus voting
    - Multi-signal recovery for mixed/irregular chapter patterns
    """
    
    def __init__(self, client: GeminiClient):
        self.client = client
        self.splitter = Splitter()
        self.sampler = Sampler()
        self.consensus_votes = 3  # Number of AI calls for consensus voting
    
    def find_best_pattern(
        self,
        target_file: str,
        initial_samples: str,
        filename: Optional[str] = None,
        encoding: str = 'utf-8'
    ) -> Tuple[Optional[str], Optional[str]]:
        """ìµœì ì˜ íŒ¨í„´ íƒìƒ‰ (v3.0 Plan C ì •ë°€ ì¶”ì  í¬í•¨)"""
        
        # 1. ê¸°ëŒ€ í™”ìˆ˜ ì¶”ì¶œ (Hotfix v5: ì‘ê°€ëª… ìˆ«ì ì˜¤ì¸ì‹ ë°©ì§€)
        expected_count = 0
        if filename:
            # ìš°ì„ ìˆœìœ„ 1: ëª…ì‹œì  ë²”ìœ„ (ì˜ˆ: 1~370í™”, 1-370)
            range_match = re.search(r'(?:~|-)(\d+)(?:í™”|íšŒ)?', filename)
            if range_match:
                expected_count = int(range_match.group(1))
            else:
                # ìš°ì„ ìˆœìœ„ 2: ëª…ì‹œì  ì´ í™”ìˆ˜ (ì˜ˆ: ì´370í™”, (370í™”), [370])
                total_match = re.search(r'(?:ì´|\(|\[)(\d+)(?:í™”|íšŒ|\]|\))', filename)
                if total_match:
                    expected_count = int(total_match.group(1))
                else:
                    # ìš°ì„ ìˆœìœ„ 3: ë§ˆì§€ë§‰ ìˆ«ì (í•˜ì§€ë§Œ ì‘ê°€ëª… ë“± ì˜¤ì¸ ê°€ëŠ¥ì„± ìˆìŒ -> ë³´ìˆ˜ì  ì ìš©)
                    # "burn7" ê°™ì€ ì¼€ì´ìŠ¤ ë°©ì§€ë¥¼ ìœ„í•´, ìˆ«ìê°€ 3ìë¦¬ ì´ìƒì¼ ë•Œë§Œ ì‹ ë¢°í•˜ê±°ë‚˜ ê±´ë„ˆëœ€
                    # ì—¬ê¸°ì„œëŠ” ì•ˆì „í•˜ê²Œ 0ìœ¼ë¡œ ë‘ê³ , ëª…í™•í•˜ì§€ ì•Šìœ¼ë©´ AIì— ì˜ì¡´í•˜ì§€ ì•Šë„ë¡ í•¨
                    pass 
            
            if expected_count > 0:
                logger.info(f"   ğŸ¯ [Target] íŒŒì¼ëª…ì—ì„œ ëª©í‘œ í™”ìˆ˜ ì‹ë³„: {expected_count}í™”")

        # 2. AI ë¶„ì„ (v3.0 ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
        logger.info(f"   -> ì±•í„° ì œëª© íŒ¨í„´ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... (Reference Mode)")
        pattern = self._analyze_pattern_v3(initial_samples)
        
        if not pattern or pattern == "NO_PATTERN_FOUND":
            pattern, _ = self._try_fallback(target_file, encoding=encoding)
            return (pattern, None)

        # 3. ì»¤ë²„ë¦¬ì§€ ê²€ì¦ ë° ì •ë°€ ì¶”ì  (Plan C)
        stats = self.splitter.verify_pattern(target_file, pattern, encoding=encoding)
        
        # v3.0 ê¸°ì¤€ 99% ë¯¸ë‹¬ ì‹œ ì •ë°€ ì¶”ì  ì‹œì‘
        if not stats.get('coverage_ok'):
            cur_ratio = stats.get('last_match_ratio', 0)
            logger.warning(f"   âš ï¸ íŒ¨í„´ ì»¤ë²„ë¦¬ì§€ ë‚®ìŒ ({cur_ratio*100:.1f}%). ì •ë°€ ì¶”ì (Plan C)ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            pattern = self._run_adaptive_retry_v3(target_file, pattern, stats, encoding=encoding)
            stats = self.splitter.verify_pattern(target_file, pattern, encoding=encoding)

        # 4. Zero Tolerance (100% ì¼ì¹˜ ë³´ì •)
        if expected_count > 0 and stats.get('match_count', 0) != expected_count:
            logger.info(f"   ğŸ”„ [M-45] í™”ìˆ˜ ì •í•©ì„± ë³´ì • ì¤‘ ({stats.get('match_count')}/{expected_count})")
            pattern = self.refine_pattern_with_goal_v3(target_file, pattern, expected_count, encoding=encoding)
            
        return (pattern, None)
    

    def _analyze_gap_pattern(self, sample_text: str, current_pattern: str) -> Optional[str]:
        """[Hotfix v7] ëˆ„ë½ êµ¬ê°„ ì „ìš© ì •ë°€ ë¶„ì„ (Context-Aware)"""
        prompt = f"""=== pattern_refinement ===
You are an expert in Regex. We are trying to split a novel into chapters.
We already have a pattern: `{current_pattern}`
However, we missed some chapters in the following text chunk.

[Tasks]
1. Analyze the text and find the Chapter Title pattern used inside this specific chunk.
2. It might be slightly different from the existing pattern (e.g., "1í™”" vs "Chapter 1").
3. Create a Python Regex for this NEW pattern.
   - **DO NOT** return the existing pattern again.
   - **DO NOT** match general sentences or page numbers.
   - **ONLY** match headlines that look like chapter titles.

[Text Chunk (Missed Area)]
{sample_text[:30000]}

[Output]
Return ONLY the raw Regex string. No markdown.
"""
        return self._generate_regex_from_ai(prompt)

    def _analyze_pattern_v3(self, sample_text: str) -> Optional[str]:
        """NovelAIze-SSR v3.0 ì›ë³¸ í”„ë¡¬í”„íŠ¸ ë³µì›"""
        prompt = f"""=== pattern_analysis ===
You are an expert in Regex (Regular Expressions) and Text Analysis.
Analyze the following Novel Text Samples and identify the Pattern used for Chapter Titles.

[Tasks]
1. Find all consistent patterns that denote a new chapter start.
   **CRITICAL: Detect Mixed or Inconsistent patterns.**
   If the novel uses multiple formats (e.g., some chapters use "1í™”", while others use "Chapter 1" or "Ep.1"), identify ALL of them.
2. Create a Python Compatible Regular Expression (Regex) to match these chapter titles.
   - Use the `|` (OR) operator to combine multiple patterns if necessary.
   - Use `\\s*` for flexible whitespace and `\\d+` for numbers.
3. OUTPUT ONLY the raw Regex string. No markdown, no content.
   - If no pattern found, return "NO_PATTERN_FOUND".

[Novel Text Samples]
{sample_text[:30000]}
"""
        return self._generate_regex_from_ai(prompt)

    def _generate_regex_from_ai(self, prompt: str) -> Optional[str]:
        """AI ì‘ë‹µ ì²˜ë¦¬ ê³µí†µ ë¡œì§"""
        try:
            response = self.client.generate_content(prompt)
            
            # Fix #2: Check for None or empty response before calling .strip()
            if response is None or not response:
                logger.warning("   âš ï¸  AI returned None or empty response, skipping")
                return None
            
            # ë§ˆí¬ë‹¤ìš´ ë° ë¶ˆí•„ìš” í…ìŠ¤íŠ¸ ì •ì œ
            result = response.strip().replace("```python", "").replace("```re", "").replace("```", "").replace("r'", "").replace("'", "").strip()
            if "NO_PATTERN_FOUND" in result: return None
            # ì¤„ë°”ê¿ˆì´ ìˆëŠ” ê²½ìš° ì²« ì¤„ë§Œ ì‚¬ìš©
            result = result.splitlines()[0] if result else None
            
            # Fix #3: Enhanced regex validation and sanitization
            if result:
                # Validate pattern: reject leading '?' or other invalid patterns
                if result.startswith('?'):
                    logger.warning(f"   âš ï¸  Rejecting invalid pattern (starts with '?'): {result}")
                    return None
                
                # Check for unescaped patterns that might cause issues
                if '(?P<' in result and ')' not in result[result.index('(?P<'):]:
                    logger.warning(f"   âš ï¸  Rejecting pattern with unclosed named group: {result}")
                    return None
                
                try:
                    re.compile(result)
                except re.error as e:
                    logger.error(f"   âŒ AI ìƒì„± ì •ê·œì‹ ì˜¤ë¥˜: {e} (Pattern: {result})")
                    return None
            return result
        except Exception as e:
            logger.error(f"   âŒ AI ë¶„ì„ ì¤‘ ì—ëŸ¬: {e}")
            return None

    def _run_adaptive_retry_v3(self, target_file: str, current_pattern: str, verify_stats: dict, encoding: str = 'utf-8') -> str:
        """v3.0 ì •ë°€ ì¶”ì  ë¡œì§ (ìµœëŒ€ 10íšŒ)"""
        retry_count = 0
        max_retries = 10
        pattern = current_pattern
        stats = verify_stats
        
        while not stats['coverage_ok'] and retry_count < max_retries:
            retry_count += 1
            fail_pos = stats['last_match_pos']
            
            # ì‹¤íŒ¨ ì§€ì ë¶€í„° ë‹¤ì‹œ ìƒ˜í”Œë§
            retry_sample = self.sampler.extract_samples_from(target_file, fail_pos, length=30000, encoding=encoding)
            if not retry_sample: break
                
            logger.info(f"   ğŸ”„ [Retry {retry_count}/{max_retries}] ëˆ„ë½ ì§€ì ({fail_pos}) ë¶„ì„ ì¤‘...")
            new_pattern = self._analyze_pattern_v3(retry_sample)
            
            if new_pattern and new_pattern != "NO_PATTERN_FOUND":
                combined_pattern = f"{pattern}|{new_pattern}"
                new_stats = self.splitter.verify_pattern(target_file, combined_pattern, encoding=encoding)
                
                # ì¡°ê¸ˆì´ë¼ë„ ë‚˜ì•„ì§€ë©´ ì ìš©
                new_ratio = new_stats.get('last_match_ratio', 0)
                old_ratio = stats.get('last_match_ratio', 0)
                new_tail = new_stats.get('tail_size', 9999999)
                old_tail = stats.get('tail_size', 9999999)

                if new_ratio > old_ratio or new_tail < old_tail:
                    pattern = combined_pattern
                    stats = new_stats
                    if stats.get('coverage_ok'):
                        logger.info(f"   âœ¨ [Plan C Success] ëª©í‘œ ì»¤ë²„ë¦¬ì§€ ë‹¬ì„±!")
                        break
                else:
                    logger.info("   âŒ ê°œì„ ë˜ì§€ ì•ŠìŒ. ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰...")
            else:
                break
        return pattern

    def refine_pattern_with_goal_v3(self, target_file: str, current_pattern: str, expected_count: int, encoding: str = 'utf-8') -> str:
        """100% ì¼ì¹˜ë¥¼ ìœ„í•œ ìµœì¢… ë³´ì • (v3.0 í™•ì¥) - ë™ì  ê°­ ë¶„ì„ ë° íƒ€ì´í‹€ í›„ë³´ íƒì§€ í¬í•¨"""
        matches = self.splitter.find_matches_with_pos(target_file, current_pattern, encoding=encoding)
        actual_count = len(matches)
        
        if actual_count == expected_count: return current_pattern
        
        # ê³¼ë§¤ì¹­ ì‹œ: ìˆ«ì ì‹œí€€ìŠ¤ í•„í„°ë§ ê°•í™”
        if actual_count > expected_count:
            logger.info(f"   ğŸ”„ ê³¼ë§¤ì¹­ ì œê±° ì‹œë„ ({actual_count}ch -> {expected_count}ch)")
            # ê°€ì¥ í™•ì‹¤í•œ ìˆ«ì íŒ¨í„´ë“¤ ì‹œë„
            for ptn in [r"(?:ì œ\s*)?\d+\s*í™”", r"\d+\s*í™”", r"\[\d+\]", r"Chapter\s*\d+"]:
                s = self.splitter.verify_pattern(target_file, ptn, encoding=encoding)
                if s['match_count'] == expected_count: return ptn
        
        # ë¶€ì¡± ì‹œ: ë™ì  ê°­ ë¶„ì„ ë° íƒ€ì´í‹€ í›„ë³´ íƒì§€
        if actual_count < expected_count:
            missing_count = expected_count - actual_count
            logger.info(f"   ğŸ”„ ë¶€ì¡± í™”ìˆ˜ ì¶”ì  ì¤‘ (ëˆ„ë½: {missing_count}ê°œ)")
            
            # Use dynamic gap detection
            gaps = self.find_dynamic_gaps(target_file, matches, expected_count)
            
            # [Hotfix v4] í™”ìˆ˜ í‡´ë³´ ë°©ì§€ (Strict Improvement Rule)
            best_pattern = current_pattern
            best_count = actual_count
            
            # Track title candidates for fallback
            all_title_candidates = []
            
            for gap in gaps:
                sample = self.sampler.extract_samples_from(target_file, gap['start'], length=30000, encoding=encoding)
                if not sample: continue
                
                # Try pattern refinement first
                new_p = self._analyze_gap_pattern(sample, best_pattern)
                if new_p:
                    test_p = f"{best_pattern}|{new_p}"
                    test_s = self.splitter.verify_pattern(target_file, test_p, encoding=encoding)
                    new_count = test_s.get('match_count', 0)
                    
                    # 1. í™”ìˆ˜ê°€ ê¸°ì¡´ë³´ë‹¤ ëŠ˜ì–´ë‚¬ê³  2. ëª©í‘œì¹˜ë¥¼ ë„˜ì§€ ì•Šì„ ë•Œë§Œ ìˆ˜ìš©
                    if new_count > best_count and new_count <= expected_count:
                        logger.info(f"   âœ¨ íŒ¨í„´ ë³´ê°• ì„±ê³µ: {best_count}í™” -> {new_count}í™”")
                        best_pattern = test_p
                        best_count = new_count
                        if best_count == expected_count: break
                    else:
                        logger.info(f"   âŒ ë³´ê°• íŒ¨í„´ ê±°ì ˆ (í™”ìˆ˜ ë³€í™”: {best_count} -> {new_count})")
                
                # If pattern didn't work, try title candidate extraction
                if best_count < expected_count:
                    candidates = self.extract_title_candidates(sample, best_pattern)
                    all_title_candidates.extend(candidates)
            
            # If we still have missing chapters and found title candidates, log them
            if best_count < expected_count and all_title_candidates:
                logger.info(f"   ğŸ“ Found {len(all_title_candidates)} title candidates for manual/fallback processing")
                # Store candidates for later use by stage4_splitter
                # We'll pass this information back through the pattern
                # For now, just use the improved pattern
            
            return best_pattern

        return current_pattern

    def find_dynamic_gaps(self, target_file: str, matches: list, expected_count: int) -> list:
        """Dynamic gap detection based on average chapter size and expected count
        
        Uses adaptive thresholds instead of fixed 100KB gaps. The threshold is calculated
        as 1.5x the average chapter size to account for novels with varying chapter lengths.
        
        Args:
            target_file: Path to the file
            matches: List of match positions
            expected_count: Expected number of chapters
            
        Returns:
            List of gap dictionaries with start, end, size, and priority
        """
        if not matches or expected_count <= 0:
            return []
        
        total_size = os.path.getsize(target_file)
        
        # Calculate average expected chapter size
        avg_chapter_size = total_size / expected_count if expected_count > 0 else 100000
        
        # Dynamic threshold constants
        GAP_MULTIPLIER = 1.5  # Gaps must be 1.5x average to be significant
        MIN_GAP_SIZE = 50000  # Minimum 50KB regardless of average (prevents tiny gaps)
        
        # Dynamic threshold: gaps larger than 1.5x average chapter size
        dynamic_threshold = max(avg_chapter_size * GAP_MULTIPLIER, MIN_GAP_SIZE)
        
        gaps = []
        
        # Check gap before first match
        if matches[0]['pos'] > dynamic_threshold:
            gaps.append({
                'start': 0,
                'end': matches[0]['pos'],
                'size': matches[0]['pos'],
                'priority': matches[0]['pos'] / avg_chapter_size
            })
        
        # Check gaps between matches
        for i in range(len(matches) - 1):
            gap_size = matches[i + 1]['pos'] - matches[i]['pos']
            if gap_size > dynamic_threshold:
                gaps.append({
                    'start': matches[i]['pos'],
                    'end': matches[i + 1]['pos'],
                    'size': gap_size,
                    'priority': gap_size / avg_chapter_size
                })
        
        # Check gap after last match
        tail_size = total_size - matches[-1]['pos']
        if tail_size > dynamic_threshold:
            gaps.append({
                'start': matches[-1]['pos'],
                'end': total_size,
                'size': tail_size,
                'priority': tail_size / avg_chapter_size
            })
        
        # Sort by priority (largest gaps relative to average first)
        gaps.sort(key=lambda x: x['priority'], reverse=True)
        
        logger.info(f"   ğŸ“Š Dynamic gap analysis: {len(gaps)} gaps found (threshold: {dynamic_threshold/1024:.1f}KB)")
        
        return gaps[:10]  # Return top 10 gaps

    def extract_title_candidates(self, window_text: str, current_pattern: str) -> List[str]:
        """AI-based title candidate extraction for a specific window
        
        Uses consensus voting across multiple AI calls for robustness.
        
        Args:
            window_text: Text window to analyze
            current_pattern: Current regex pattern (for context)
            
        Returns:
            List of title candidate lines
        """
        prompt = f"""=== title_candidate_extraction ===
You are an expert in analyzing novel text structures.

[Task]
Find all lines that could be chapter titles in the following text.
Return ONLY the actual title lines, one per line, nothing else.

A chapter title is:
- Usually short (1-50 characters)
- Contains numbers, episode markers, or chapter indicators
- Stands out from regular narrative text
- May use brackets, special formatting, or numbering

[Current Pattern Context]
We already found some chapters with pattern: {current_pattern}
But we're missing chapters in this specific area.

[Text Window]
{window_text[:20000]}

[Output Format]
Return only the title lines, one per line. No explanations, no markdown.
If no titles found, return "NO_TITLES_FOUND".
"""
        
        all_candidates = []
        
        # Consensus voting: call AI multiple times
        for vote in range(self.consensus_votes):
            try:
                response = self.client.generate_content(prompt)
                if response and "NO_TITLES_FOUND" not in response:
                    lines = [line.strip() for line in response.strip().split('\n') if line.strip()]
                    all_candidates.extend(lines)
                time.sleep(0.5)  # Rate limiting
            except Exception as e:
                logger.warning(f"   âš ï¸ Title candidate extraction vote {vote+1} failed: {e}")
        
        # Count occurrences of each candidate (consensus filtering)
        from collections import Counter
        candidate_counts = Counter(all_candidates)
        
        # Majority voting: Keep candidates that appear in at least half the votes (rounded up)
        # This implements a simple consensus mechanism for robustness
        CONSENSUS_THRESHOLD_RATIO = 0.5  # Require at least 50% agreement
        consensus_threshold = max(1, int(self.consensus_votes * CONSENSUS_THRESHOLD_RATIO))
        
        consensus_candidates = [
            candidate for candidate, count in candidate_counts.items()
            if count >= consensus_threshold
        ]
        
        logger.info(f"   ğŸ“‹ Title candidates: {len(consensus_candidates)} found via consensus")
        
        return consensus_candidates

    def _try_fallback(self, target_file: str, encoding: str = 'utf-8') -> Tuple[Optional[str], Optional[str]]:
        for ptn in [r"\d+\s*í™”", r"ì œ\s*\d+\s*í™”", r"\[\d+\]"]:
            stats = self.splitter.verify_pattern(target_file, ptn, encoding=encoding)
            if stats['match_count'] > 0: return (ptn, None)
        return (None, None)
