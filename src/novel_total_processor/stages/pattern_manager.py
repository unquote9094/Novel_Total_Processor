"""íŒ¨í„´ ê´€ë¦¬ì (Reference v3.0 ê¸°ë°˜ ê³ ë„í™”)

AIë¥¼ ì‚¬ìš©í•˜ì—¬ ì†Œì„¤ì˜ ìµœì  ì±•í„° ë¶„í•  íŒ¨í„´ì„ ì°¾ì•„ë‚´ê³  ê²€ì¦
NovelAIze-SSR v3.0ì˜ ê³ í’ˆì§ˆ í”„ë¡¬í”„íŠ¸ ë³µì› ë° 99% ì»¤ë²„ë¦¬ì§€ ì¶”ì  ë¡œì§ ì ìš©
"""

import re
import time
import os
from typing import Optional, Tuple, List, Dict, Any
from novel_total_processor.stages.sampler import Sampler
from novel_total_processor.stages.splitter import Splitter
from novel_total_processor.ai.gemini_client import GeminiClient
from novel_total_processor.utils.logger import get_logger

logger = get_logger(__name__)


class PatternManager:
    """AIë¥¼ ì‚¬ìš©í•˜ì—¬ ì†Œì„¤ì˜ ìµœì  ì±•í„° ë¶„í•  íŒ¨í„´ì„ ì°¾ì•„ë‚´ê³  ê²€ì¦ (v3.0 Reference)
    
    Enhanced Features:
    - Dynamic gap detection based on expected chapter count and average size
    - AI-based title candidate extraction with consensus voting
    - Multi-signal recovery for mixed/irregular chapter patterns
    """
    
    def __init__(self, client: GeminiClient):
        self.client = client
        self.splitter = Splitter()
        self.sampler = Sampler()
        self.consensus_votes = 3  # Number of AI calls for consensus voting
    
    def find_best_pattern(
        self,
        target_file: str,
        initial_samples: str,
        filename: Optional[str] = None,
        encoding: str = 'utf-8'
    ) -> Tuple[Optional[str], Optional[str]]:
        """ìµœì ì˜ íŒ¨í„´ íƒìƒ‰ (v3.0 Plan C ì •ë°€ ì¶”ì  í¬í•¨)"""
        
        # 1. ê¸°ëŒ€ í™”ìˆ˜ ì¶”ì¶œ (Hotfix v5: ì‘ê°€ëª… ìˆ«ì ì˜¤ì¸ì‹ ë°©ì§€)
        expected_count = 0
        if filename:
            # ìš°ì„ ìˆœìœ„ 1: ëª…ì‹œì  ë²”ìœ„ (ì˜ˆ: 1~370í™”, 1-370)
            range_match = re.search(r'(?:~|-)(\d+)(?:í™”|íšŒ)?', filename)
            if range_match:
                expected_count = int(range_match.group(1))
            else:
                # ìš°ì„ ìˆœìœ„ 2: ëª…ì‹œì  ì´ í™”ìˆ˜ (ì˜ˆ: ì´370í™”, (370í™”), [370])
                total_match = re.search(r'(?:ì´|\(|\[)(\d+)(?:í™”|íšŒ|\]|\))', filename)
                if total_match:
                    expected_count = int(total_match.group(1))
                else:
                    # ìš°ì„ ìˆœìœ„ 3: ë§ˆì§€ë§‰ ìˆ«ì (í•˜ì§€ë§Œ ì‘ê°€ëª… ë“± ì˜¤ì¸ ê°€ëŠ¥ì„± ìˆìŒ -> ë³´ìˆ˜ì  ì ìš©)
                    # "burn7" ê°™ì€ ì¼€ì´ìŠ¤ ë°©ì§€ë¥¼ ìœ„í•´, ìˆ«ìê°€ 3ìë¦¬ ì´ìƒì¼ ë•Œë§Œ ì‹ ë¢°í•˜ê±°ë‚˜ ê±´ë„ˆëœ€
                    # ì—¬ê¸°ì„œëŠ” ì•ˆì „í•˜ê²Œ 0ìœ¼ë¡œ ë‘ê³ , ëª…í™•í•˜ì§€ ì•Šìœ¼ë©´ AIì— ì˜ì¡´í•˜ì§€ ì•Šë„ë¡ í•¨
                    pass 
            
            if expected_count > 0:
                logger.info(f"   ğŸ¯ [Target] íŒŒì¼ëª…ì—ì„œ ëª©í‘œ í™”ìˆ˜ ì‹ë³„: {expected_count}í™”")

        # 2. AI ë¶„ì„ (v3.0 ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
        logger.info(f"   -> ì±•í„° ì œëª© íŒ¨í„´ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... (Reference Mode)")
        pattern = self._analyze_pattern_v3(initial_samples)
        
        if not pattern or pattern == "NO_PATTERN_FOUND":
            pattern, _ = self._try_fallback(target_file, encoding=encoding)
            return (pattern, None)

        # 3. ì»¤ë²„ë¦¬ì§€ ê²€ì¦ ë° ì •ë°€ ì¶”ì  (Plan C)
        stats = self.splitter.verify_pattern(target_file, pattern, encoding=encoding)
        
        # v3.0 ê¸°ì¤€ 99% ë¯¸ë‹¬ ì‹œ ì •ë°€ ì¶”ì  ì‹œì‘
        if not stats.get('coverage_ok'):
            cur_ratio = stats.get('last_match_ratio', 0)
            logger.warning(f"   âš ï¸ íŒ¨í„´ ì»¤ë²„ë¦¬ì§€ ë‚®ìŒ ({cur_ratio*100:.1f}%). ì •ë°€ ì¶”ì (Plan C)ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            pattern = self._run_adaptive_retry_v3(target_file, pattern, stats, encoding=encoding)
            stats = self.splitter.verify_pattern(target_file, pattern, encoding=encoding)

        # 4. Zero Tolerance (100% ì¼ì¹˜ ë³´ì •)
        if expected_count > 0 and stats.get('match_count', 0) != expected_count:
            logger.info(f"   ğŸ”„ [M-45] í™”ìˆ˜ ì •í•©ì„± ë³´ì • ì¤‘ ({stats.get('match_count')}/{expected_count})")
            pattern, _ = self.refine_pattern_with_goal_v3(target_file, pattern, expected_count, encoding=encoding)
            
        return (pattern, None)
    

    def _analyze_gap_pattern(self, sample_text: str, current_pattern: str) -> Optional[str]:
        """[Hotfix v7] ëˆ„ë½ êµ¬ê°„ ì „ìš© ì •ë°€ ë¶„ì„ (Context-Aware) + Enhanced with number relaxation"""
        prompt = f"""=== pattern_refinement ===
You are an expert in Regex. We are trying to split a novel into chapters.
We already have a pattern: `{current_pattern}`
However, we missed some chapters in the following text chunk.

[Tasks]
1. Analyze the text and find the Chapter Title pattern used inside this specific chunk.

2. Consider these possibilities:
   - The same format as existing pattern, but WITHOUT number requirements
     (e.g., if current is "< .*?\\(\\d+\\) >", try "< .*? >" for titles without numbers)
   - A slightly different format (e.g., "1í™”" vs "Chapter 1" vs "Ep.1")
   - Titles that match the visual structure but are missing numbers

3. Create a Python Regex for this pattern.
   - **EXCLUDE end markers**: Lines ending with "ë", "ì™„", "END", "fin", "ì¢…ë£Œ"
   - **DO NOT** return the existing pattern unchanged
   - **DO NOT** match general sentences, dialogue, or page numbers
   - **ONLY** match headlines that look like chapter titles
   - Make number patterns OPTIONAL with \\d* instead of \\d+ if titles vary

[Current Pattern]
{current_pattern}

[Text Chunk (Missed Area)]
{sample_text[:30000]}

[Output]
Return ONLY the raw Regex string. No markdown, no explanations.
"""
        return self._generate_regex_from_ai(prompt)

    def _analyze_pattern_v3(self, sample_text: str) -> Optional[str]:
        """NovelAIze-SSR v3.0 ì›ë³¸ í”„ë¡¬í”„íŠ¸ ë³µì› + Enhanced with Korean novel patterns"""
        prompt = f"""=== pattern_analysis ===
You are an expert in Regex (Regular Expressions) and Text Analysis.
Analyze the following Novel Text Samples and identify the Pattern used for Chapter Titles.

[Common Korean Novel Chapter Formats]
Examples of real chapter title patterns used in Korean novels:
- Numbered: "Ní™”", "ì œNí™”", "NíšŒ", "ì œNì¥", "Chapter N", "Ep.N", "Episode N", "Nè©±", "ç¬¬Nè©±"
- Bracketed: "< ì œëª© >", "ã€ ì œëª© ã€‘", "[ ì œëª© ]", "[Ní™”]", "<Ní™”>"
- Decorated: "â€• ì œëª© â€•", "â˜… ì œëª©", "â—† ì œëª© â—†", "â–  ì œëª©", "â–£ Ní™”"
- Special: "í”„ë¡¤ë¡œê·¸", "ì—í•„ë¡œê·¸", "ì™¸ì „", "ë²ˆì™¸", "í›„ê¸°", "ì‘ê°€ì˜ ë§"
- Mixed: Some chapters may have numbers, others may not (e.g., "< ì—í”¼ì†Œë“œ(3) >" and "< ì—°ìŠµìƒ ë©´ì ‘ >")

[CRITICAL WARNINGS]
1. **START vs END Markers**: 
   - Some novels use PAIRED structures: "< ì œëª© >" (START) and "< ì œëª© > ë" (END)
   - Your regex MUST match ONLY the START markers
   - **EXCLUDE** any lines ending with: "ë", "ì™„", "END", "fin", "ì¢…ë£Œ", "ë—", "end", "å®Œ"
   - Use negative lookahead if needed: (?!.*ë\\s*$)

2. **Number Flexibility**:
   - Numbers may be OPTIONAL in titles
   - Some chapters have numbers ("< ì—í”¼ì†Œë“œ(3) >"), others don't ("< ì—°ìŠµìƒ ë©´ì ‘ >")
   - Do NOT require \\d+ if the pattern works without it

3. **Pattern Precision**:
   - Match complete title lines, not just fragments
   - Avoid matching dialogue, body text, or page numbers
   - Look for consistent formatting (brackets, spacing, decoration)

[Tasks]
1. Find all consistent patterns that denote a new chapter START.
   **CRITICAL: Detect Mixed or Inconsistent patterns.**
   If the novel uses multiple formats (e.g., some use "1í™”", others use "Chapter 1"), identify ALL of them.

2. Create a Python Compatible Regular Expression (Regex) to match these chapter START titles.
   - Use the `|` (OR) operator to combine multiple patterns if necessary.
   - Use `\\s*` for flexible whitespace and `\\d*` or `\\d+` for numbers (make optional if needed).
   - **MUST exclude end markers** (lines ending with "ë", "ì™„", "END", etc.)

3. OUTPUT ONLY the raw Regex string. No markdown, no explanations.
   - If no pattern found, return "NO_PATTERN_FOUND".

[Novel Text Samples]
{sample_text[:30000]}
"""
        return self._generate_regex_from_ai(prompt)

    def _generate_regex_from_ai(self, prompt: str) -> Optional[str]:
        """AI ì‘ë‹µ ì²˜ë¦¬ ê³µí†µ ë¡œì§"""
        try:
            response = self.client.generate_content(prompt)
            
            # Fix #2: Check for None or empty response before calling .strip()
            if response is None or not response:
                logger.warning("   âš ï¸  AI returned None or empty response, skipping")
                return None
            
            # ë§ˆí¬ë‹¤ìš´ ë° ë¶ˆí•„ìš” í…ìŠ¤íŠ¸ ì •ì œ
            result = response.strip().replace("```python", "").replace("```re", "").replace("```", "").replace("r'", "").replace("'", "").strip()
            if "NO_PATTERN_FOUND" in result: return None
            # ì¤„ë°”ê¿ˆì´ ìˆëŠ” ê²½ìš° ì²« ì¤„ë§Œ ì‚¬ìš©
            result = result.splitlines()[0] if result else None
            
            # Fix #3: Enhanced regex validation and sanitization
            if result:
                # Validate pattern: reject leading '?' or other invalid patterns
                if result.startswith('?'):
                    logger.warning(f"   âš ï¸  Rejecting invalid pattern (starts with '?'): {result}")
                    return None
                
                # Check for properly matched parentheses and valid named groups
                # Count opening and closing parentheses
                open_parens = result.count('(')
                close_parens = result.count(')')
                if open_parens != close_parens:
                    logger.warning(f"   âš ï¸  Rejecting pattern with mismatched parentheses: {result}")
                    return None
                
                try:
                    re.compile(result)
                except re.error as e:
                    logger.error(f"   âŒ AI ìƒì„± ì •ê·œì‹ ì˜¤ë¥˜: {e} (Pattern: {result})")
                    return None
            return result
        except Exception as e:
            logger.error(f"   âŒ AI ë¶„ì„ ì¤‘ ì—ëŸ¬: {e}")
            return None

    def _run_adaptive_retry_v3(self, target_file: str, current_pattern: str, verify_stats: dict, encoding: str = 'utf-8') -> str:
        """v3.0 ì •ë°€ ì¶”ì  ë¡œì§ (ìµœëŒ€ 10íšŒ)"""
        retry_count = 0
        max_retries = 3
        pattern = current_pattern
        stats = verify_stats
        
        while not stats['coverage_ok'] and retry_count < max_retries:
            retry_count += 1
            fail_pos = stats['last_match_pos']
            
            # ì‹¤íŒ¨ ì§€ì ë¶€í„° ë‹¤ì‹œ ìƒ˜í”Œë§
            retry_sample = self.sampler.extract_samples_from(target_file, fail_pos, length=30000, encoding=encoding)
            if not retry_sample: break
                
            logger.info(f"   ğŸ”„ [Retry {retry_count}/{max_retries}] ëˆ„ë½ ì§€ì ({fail_pos}) ë¶„ì„ ì¤‘...")
            new_pattern = self._analyze_pattern_v3(retry_sample)
            
            if new_pattern and new_pattern != "NO_PATTERN_FOUND":
                combined_pattern = f"{pattern}|{new_pattern}"
                new_stats = self.splitter.verify_pattern(target_file, combined_pattern, encoding=encoding)
                
                # ì¡°ê¸ˆì´ë¼ë„ ë‚˜ì•„ì§€ë©´ ì ìš©
                new_ratio = new_stats.get('last_match_ratio', 0)
                old_ratio = stats.get('last_match_ratio', 0)
                new_tail = new_stats.get('tail_size', 9999999)
                old_tail = stats.get('tail_size', 9999999)

                if new_ratio > old_ratio or new_tail < old_tail:
                    pattern = combined_pattern
                    stats = new_stats
                    if stats.get('coverage_ok'):
                        logger.info(f"   âœ¨ [Plan C Success] ëª©í‘œ ì»¤ë²„ë¦¬ì§€ ë‹¬ì„±!")
                        break
                else:
                    logger.info("   âŒ ê°œì„ ë˜ì§€ ì•ŠìŒ. ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰...")
            else:
                break
        return pattern

    def refine_pattern_with_goal_v3(self, target_file: str, current_pattern: str, expected_count: int, encoding: str = 'utf-8', max_gaps: int = 3) -> Tuple[str, int]:
        """100% ì¼ì¹˜ë¥¼ ìœ„í•œ ìµœì¢… ë³´ì • (v3.0 í™•ì¥) - 3-Level Escalation
        
        Level 1: AI regex generation (already done by caller)
        Level 2: Code-level auto validation and fixing
        Level 3: Direct AI title search in gaps (if Level 1+2 < 95%)
        
        Args:
            target_file: Target file path
            current_pattern: Current regex pattern
            expected_count: Expected number of chapters
            encoding: File encoding
            max_gaps: Maximum number of gaps to analyze (default: 3) to cap AI calls
            
        Returns:
            Tuple of (refined_pattern, rejection_count)
        """
        matches = self.splitter.find_matches_with_pos(target_file, current_pattern, encoding=encoding)
        actual_count = len(matches)
        
        if actual_count == expected_count: 
            return (current_pattern, 0)
        
        # Level 2: Auto-validation and fixing (before AI pattern refinement)
        if expected_count > 0 and actual_count != expected_count:
            logger.info(f"   ğŸ”§ Applying Level 2 auto-validation...")
            auto_fixed_pattern, auto_count = self.auto_validate_and_fix(
                target_file, current_pattern, expected_count, encoding
            )
            
            # If auto-fix achieved the goal, return immediately
            if auto_count == expected_count:
                logger.info(f"   âœ… [Level 2 Success] Auto-fix achieved target: {auto_count}/{expected_count}")
                return (auto_fixed_pattern, 0)
            
            # If auto-fix improved significantly, use it as the new baseline
            if auto_count > actual_count:
                logger.info(f"   âœ¨ [Level 2 Improved] Using auto-fixed pattern: {actual_count} -> {auto_count}")
                current_pattern = auto_fixed_pattern
                actual_count = auto_count
                matches = self.splitter.find_matches_with_pos(target_file, current_pattern, encoding=encoding)
        
        # ê³¼ë§¤ì¹­ ì‹œ: ìˆ«ì ì‹œí€€ìŠ¤ í•„í„°ë§ ê°•í™”
        if actual_count > expected_count:
            logger.info(f"   ğŸ”„ ê³¼ë§¤ì¹­ ì œê±° ì‹œë„ ({actual_count}ch -> {expected_count}ch)")
            # ê°€ì¥ í™•ì‹¤í•œ ìˆ«ì íŒ¨í„´ë“¤ ì‹œë„
            for ptn in [r"(?:ì œ\s*)?\d+\s*í™”", r"\d+\s*í™”", r"\[\d+\]", r"Chapter\s*\d+"]:
                s = self.splitter.verify_pattern(target_file, ptn, encoding=encoding)
                if s['match_count'] == expected_count: 
                    return (ptn, 0)
        
        # ë¶€ì¡± ì‹œ: ë™ì  ê°­ ë¶„ì„ ë° íƒ€ì´í‹€ í›„ë³´ íƒì§€
        if actual_count < expected_count:
            missing_count = expected_count - actual_count
            logger.info(f"   ğŸ”„ ë¶€ì¡± í™”ìˆ˜ ì¶”ì  ì¤‘ (ëˆ„ë½: {missing_count}ê°œ)")
            
            # Use dynamic gap detection
            gaps = self.find_dynamic_gaps(target_file, matches, expected_count)
            
            # Limit gaps to max_gaps to cap AI calls
            limited_gaps = gaps[:max_gaps]
            logger.info(f"   ğŸ“Š Gap ë¶„ì„ ì œí•œ: {len(limited_gaps)}/{len(gaps)} gaps (MAX_GAPS_TO_ANALYZE={max_gaps})")
            
            # [Hotfix v4] í™”ìˆ˜ í‡´ë³´ ë°©ì§€ (Strict Improvement Rule)
            best_pattern = current_pattern
            best_count = actual_count
            
            # Track title candidates for fallback and rejection count
            all_title_candidates = []
            rejection_count = 0
            
            for gap in limited_gaps:
                sample = self.sampler.extract_samples_from(target_file, gap['start'], length=30000, encoding=encoding)
                if not sample: continue
                
                # Try pattern refinement first
                new_p = self._analyze_gap_pattern(sample, best_pattern)
                if new_p:
                    test_p = f"{best_pattern}|{new_p}"
                    test_s = self.splitter.verify_pattern(target_file, test_p, encoding=encoding)
                    new_count = test_s.get('match_count', 0)
                    
                    # 1. í™”ìˆ˜ê°€ ê¸°ì¡´ë³´ë‹¤ ëŠ˜ì–´ë‚¬ê³  2. ëª©í‘œì¹˜ë¥¼ ë„˜ì§€ ì•Šì„ ë•Œë§Œ ìˆ˜ìš©
                    if new_count > best_count and new_count <= expected_count:
                        logger.info(f"   âœ¨ íŒ¨í„´ ë³´ê°• ì„±ê³µ: {best_count}í™” -> {new_count}í™”")
                        best_pattern = test_p
                        best_count = new_count
                        rejection_count = 0  # Reset rejection count on success
                        if best_count == expected_count: break
                    else:
                        rejection_count += 1
                        logger.info(f"   âŒ ë³´ê°• íŒ¨í„´ ê±°ì ˆ (í™”ìˆ˜ ë³€í™”: {best_count} -> {new_count}, ì—°ì† ê±°ì ˆ: {rejection_count})")
                
                # If pattern didn't work, try title candidate extraction
                if best_count < expected_count:
                    candidates = self.extract_title_candidates(sample, best_pattern)
                    all_title_candidates.extend(candidates)
            
            # If we still have missing chapters and found title candidates, log them
            if best_count < expected_count and all_title_candidates:
                logger.info(f"   ğŸ“ Found {len(all_title_candidates)} title candidates for manual/fallback processing")
                # Store candidates for later use by stage4_splitter
                # We'll pass this information back through the pattern
                # For now, just use the improved pattern
            
            # Level 3: Direct AI title search if still below 95% accuracy
            if best_count < expected_count * 0.95:
                logger.info(f"   ğŸš€ [Level 3 Trigger] Current accuracy: {best_count}/{expected_count} ({best_count/expected_count*100:.1f}%)")
                logger.info(f"   -> Activating Level 3: Direct AI title search...")
                
                # Get existing matches with text for context
                existing_matches = self._find_matches_with_text(target_file, best_pattern, encoding)
                
                # Call Level 3 direct search
                found_titles = self.direct_ai_title_search(
                    target_file, best_pattern, expected_count, existing_matches, encoding
                )
                
                if found_titles:
                    logger.info(f"   âœ¨ [Level 3] Found {len(found_titles)} additional titles via AI search")
                    
                    # Build pattern from these examples
                    reverse_pattern = self._build_pattern_from_examples(found_titles)
                    
                    if reverse_pattern:
                        # Combine with existing pattern
                        combined = f"{best_pattern}|{reverse_pattern}"
                        
                        # Test the combined pattern
                        test_s = self.splitter.verify_pattern(target_file, combined, encoding=encoding)
                        new_count = test_s.get('match_count', 0)
                        
                        # Accept if it improves and doesn't over-match (within 5% tolerance)
                        if new_count > best_count and new_count <= expected_count * 1.05:
                            logger.info(f"   âœ… [Level 3 Success] Pattern improved: {best_count} -> {new_count}")
                            best_pattern = combined
                            best_count = new_count
                        else:
                            logger.info(f"   âŒ [Level 3] Reverse pattern didn't improve ({new_count} matches)")
                    else:
                        logger.warning(f"   âš ï¸  [Level 3] Failed to build reverse pattern from examples")
                else:
                    logger.info(f"   â„¹ï¸  [Level 3] No additional titles found by AI")
            
            return (best_pattern, rejection_count)

        return (current_pattern, 0)

    def find_dynamic_gaps(self, target_file: str, matches: list, expected_count: int) -> list:
        """Dynamic gap detection based on average chapter size and expected count
        
        Uses adaptive thresholds instead of fixed 100KB gaps. The threshold is calculated
        as 1.5x the average chapter size to account for novels with varying chapter lengths.
        
        Args:
            target_file: Path to the file
            matches: List of match positions
            expected_count: Expected number of chapters
            
        Returns:
            List of gap dictionaries with start, end, size, and priority
        """
        if not matches or expected_count <= 0:
            return []
        
        total_size = os.path.getsize(target_file)
        
        # Calculate average expected chapter size
        avg_chapter_size = total_size / expected_count if expected_count > 0 else 100000
        
        # Dynamic threshold constants
        GAP_MULTIPLIER = 1.5  # Gaps must be 1.5x average to be significant
        MIN_GAP_SIZE = 50000  # Minimum 50KB regardless of average (prevents tiny gaps)
        
        # Dynamic threshold: gaps larger than 1.5x average chapter size
        dynamic_threshold = max(avg_chapter_size * GAP_MULTIPLIER, MIN_GAP_SIZE)
        
        gaps = []
        
        # Check gap before first match
        if matches[0]['pos'] > dynamic_threshold:
            gaps.append({
                'start': 0,
                'end': matches[0]['pos'],
                'size': matches[0]['pos'],
                'priority': matches[0]['pos'] / avg_chapter_size
            })
        
        # Check gaps between matches
        for i in range(len(matches) - 1):
            gap_size = matches[i + 1]['pos'] - matches[i]['pos']
            if gap_size > dynamic_threshold:
                gaps.append({
                    'start': matches[i]['pos'],
                    'end': matches[i + 1]['pos'],
                    'size': gap_size,
                    'priority': gap_size / avg_chapter_size
                })
        
        # Check gap after last match
        tail_size = total_size - matches[-1]['pos']
        if tail_size > dynamic_threshold:
            gaps.append({
                'start': matches[-1]['pos'],
                'end': total_size,
                'size': tail_size,
                'priority': tail_size / avg_chapter_size
            })
        
        # Sort by priority (largest gaps relative to average first)
        gaps.sort(key=lambda x: x['priority'], reverse=True)
        
        logger.info(f"   ğŸ“Š Dynamic gap analysis: {len(gaps)} gaps found (threshold: {dynamic_threshold/1024:.1f}KB)")
        
        return gaps[:10]  # Return top 10 gaps

    def extract_title_candidates(self, window_text: str, current_pattern: str) -> List[str]:
        """AI-based title candidate extraction for a specific window
        
        Uses consensus voting across multiple AI calls for robustness.
        
        Args:
            window_text: Text window to analyze
            current_pattern: Current regex pattern (for context)
            
        Returns:
            List of title candidate lines
        """
        prompt = f"""=== title_candidate_extraction ===
You are an expert in analyzing novel text structures.

[Task]
Find all lines that could be chapter titles in the following text.
Return ONLY the actual title lines, one per line, nothing else.

A chapter title is:
- Usually short (1-50 characters)
- May or may not contain numbers (both are valid)
- May contain episode markers or chapter indicators
- Stands out from regular narrative text
- May use brackets, special formatting, or numbering
- Examples: "< ì œëª© >", "ì œ3í™”", "Chapter 5", "í”„ë¡¤ë¡œê·¸", "ì—í•„ë¡œê·¸(1)"

**IMPORTANT**: 
- Titles WITHOUT numbers are equally valid as titles WITH numbers
- DO NOT exclude titles just because they lack numbers
- EXCLUDE lines ending with "ë", "ì™„", "END", "fin" (these are END markers, not titles)

[Current Pattern Context]
We already found some chapters with pattern: {current_pattern}
But we're missing chapters in this specific area.

[Text Window]
{window_text[:20000]}

[Output Format]
Return only the title lines, one per line. No explanations, no markdown.
If no titles found, return "NO_TITLES_FOUND".
"""
        
        all_candidates = []
        
        # Consensus voting: call AI multiple times
        for vote in range(self.consensus_votes):
            try:
                response = self.client.generate_content(prompt)
                if response and "NO_TITLES_FOUND" not in response:
                    lines = [line.strip() for line in response.strip().split('\n') if line.strip()]
                    all_candidates.extend(lines)
                time.sleep(0.5)  # Rate limiting
            except Exception as e:
                logger.warning(f"   âš ï¸ Title candidate extraction vote {vote+1} failed: {e}")
        
        # Count occurrences of each candidate (consensus filtering)
        from collections import Counter
        candidate_counts = Counter(all_candidates)
        
        # Majority voting: Keep candidates that appear in at least half the votes (rounded up)
        # This implements a simple consensus mechanism for robustness
        CONSENSUS_THRESHOLD_RATIO = 0.5  # Require at least 50% agreement
        consensus_threshold = max(1, int(self.consensus_votes * CONSENSUS_THRESHOLD_RATIO))
        
        consensus_candidates = [
            candidate for candidate, count in candidate_counts.items()
            if count >= consensus_threshold
        ]
        
        logger.info(f"   ğŸ“‹ Title candidates: {len(consensus_candidates)} found via consensus")
        
        return consensus_candidates

    def auto_validate_and_fix(
        self, 
        target_file: str, 
        current_pattern: str, 
        expected_count: int,
        encoding: str = 'utf-8'
    ) -> Tuple[str, int]:
        """Level 2: Code-level automatic validation and fixing (no AI calls)
        
        Automatically detects and fixes common pattern issues:
        1. End marker contamination (lines ending with "ë", "ì™„", "END", etc.)
        2. Close duplicate matches (start/end pairs too close together)
        3. Number requirement relaxation (remove \\d+ to match unnumbered titles)
        4. Negative lookahead for end marker exclusion
        
        Args:
            target_file: Path to target file
            current_pattern: Current regex pattern
            expected_count: Expected number of chapters
            encoding: File encoding
            
        Returns:
            Tuple of (cleaned_pattern, match_count)
        """
        logger.info("   ğŸ”§ [Level 2] Auto-validation and fixing pattern...")
        
        # Get initial matches with their text content
        matches = self._find_matches_with_text(target_file, current_pattern, encoding)
        initial_count = len(matches)
        
        logger.info(f"   ğŸ“Š Initial matches: {initial_count}")
        
        # Step 1: Detect and separate end markers
        end_keywords = ['ë', 'ì™„', 'END', 'end', 'fin', 'Fin', 'ì¢…ë£Œ', 'ë—', 'å®Œ']
        start_matches, end_matches = self._separate_start_end_matches(matches, end_keywords)
        
        if end_matches:
            logger.info(f"   âš ï¸  Detected {len(end_matches)} end markers in matches")
            logger.info(f"   âœ‚ï¸  Removed end markers: {initial_count} -> {len(start_matches)} matches")
            matches = start_matches
        
        # Step 2: Remove close duplicates (likely start/end pairs)
        MIN_GAP = 500  # Minimum 500 chars between chapter starts
        cleaned_matches = self._remove_close_duplicates(matches, MIN_GAP)
        
        if len(cleaned_matches) < len(matches):
            logger.info(f"   ğŸ” Removed {len(matches) - len(cleaned_matches)} close duplicates")
            matches = cleaned_matches
        
        current_count = len(matches)
        logger.info(f"   ğŸ“Š After cleanup: {current_count} matches")
        
        # Step 3: If still under 95% of expected, try relaxing number requirements
        if expected_count > 0 and current_count < expected_count * 0.95:
            relaxed_pattern = self._relax_number_requirement(current_pattern)
            
            if relaxed_pattern != current_pattern:
                logger.info(f"   ğŸ”„ Trying relaxed pattern (numbers optional)...")
                logger.info(f"   Old: {current_pattern}")
                logger.info(f"   New: {relaxed_pattern}")
                
                # Test relaxed pattern
                relaxed_matches = self._find_matches_with_text(target_file, relaxed_pattern, encoding)
                # Clean end markers again
                relaxed_matches, _ = self._separate_start_end_matches(relaxed_matches, end_keywords)
                relaxed_matches = self._remove_close_duplicates(relaxed_matches, MIN_GAP)
                
                relaxed_count = len(relaxed_matches)
                logger.info(f"   ğŸ“Š Relaxed pattern matches: {relaxed_count}")
                
                # Accept if improved and not over-matching (with 5% tolerance)
                if relaxed_count > current_count and relaxed_count <= expected_count * 1.05:
                    logger.info(f"   âœ… Relaxed pattern accepted: {current_count} -> {relaxed_count}")
                    current_pattern = relaxed_pattern
                    current_count = relaxed_count
                else:
                    logger.info(f"   âŒ Relaxed pattern rejected (over-match or no improvement)")
        
        # Step 4: Add negative lookahead for end markers if not present
        if any(keyword in current_pattern for keyword in end_keywords):
            # Pattern already has end marker logic, skip
            pass
        else:
            # Add negative lookahead to exclude end markers
            enhanced_pattern = self._add_end_marker_exclusion(current_pattern, end_keywords)
            if enhanced_pattern != current_pattern:
                logger.info(f"   ğŸ›¡ï¸  Added end marker exclusion to pattern")
                current_pattern = enhanced_pattern
        
        logger.info(f"   âœ… [Level 2] Auto-validation complete: {current_count} matches")
        
        return current_pattern, current_count
    
    def _find_matches_with_text(self, target_file: str, pattern: str, encoding: str) -> List[Dict[str, Any]]:
        """Find pattern matches with their text content"""
        matches = []
        try:
            compiled_pattern = re.compile(pattern)
            with open(target_file, 'r', encoding=encoding, errors='replace') as f:
                pos = 0
                for line_num, line in enumerate(f):
                    if compiled_pattern.search(line.strip()):
                        matches.append({
                            'pos': pos,
                            'line_num': line_num,
                            'text': line.strip()
                        })
                    pos += len(line.encode(encoding, errors='replace'))
        except Exception as e:
            logger.warning(f"   âš ï¸  Error finding matches: {e}")
        
        return matches
    
    def _separate_start_end_matches(
        self, 
        matches: List[Dict[str, Any]], 
        end_keywords: List[str]
    ) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """Separate start markers from end markers"""
        start_matches = []
        end_matches = []
        
        for match in matches:
            text = match['text']
            # Check if line ends with any end keyword
            is_end = False
            for keyword in end_keywords:
                # Check for keyword at end of line (with optional whitespace/punctuation)
                if re.search(rf'{keyword}\s*[>ã€‘\])\)]*\s*$', text):
                    is_end = True
                    break
            
            if is_end:
                end_matches.append(match)
            else:
                start_matches.append(match)
        
        return start_matches, end_matches
    
    def _remove_close_duplicates(
        self, 
        matches: List[Dict[str, Any]], 
        min_gap: int
    ) -> List[Dict[str, Any]]:
        """Remove matches that are too close together (likely start/end pairs)"""
        if not matches:
            return matches
        
        cleaned = [matches[0]]  # Keep first match
        
        for i in range(1, len(matches)):
            gap = matches[i]['pos'] - matches[i-1]['pos']
            if gap >= min_gap:
                cleaned.append(matches[i])
            else:
                logger.debug(f"   Removing close duplicate: '{matches[i]['text']}' (gap: {gap} chars)")
        
        return cleaned
    
    def _relax_number_requirement(self, pattern: str) -> str:
        """Relax number requirements in pattern with multiple strategies
        
        Strategy 1: \\d+ -> \\d* (make numbers optional)
        Strategy 2: \\(\\d+\\) or \\(\\d*\\) -> (?:\\(\\d*\\))? (make entire parenthesized number optional)
        Strategy 3: Remove number requirements entirely, keeping only structure
        
        Returns the best variation based on testing
        """
        variations = []
        
        # Strategy 1: \\d+ -> \\d* (original approach)
        v1 = pattern.replace(r'\d+', r'\d*')
        if v1 != pattern:
            variations.append(('strategy1_digit_optional', v1))
        
        # Strategy 2: Make parenthesized numbers completely optional
        # Match patterns like \\(\\d+\\) or \\(\\d*\\) and make them optional
        v2 = re.sub(r'\\?\(\\d[+*]\\?\)', r'(?:\\(\\d*\\))?', pattern)
        if v2 != pattern:
            variations.append(('strategy2_parens_optional', v2))
        
        # Strategy 3: Combine both strategies
        v3 = re.sub(r'\\?\(\\d[+*]\\?\)', r'(?:\\(\\d*\\))?', v1)
        if v3 != pattern and v3 != v1 and v3 != v2:
            variations.append(('strategy3_combined', v3))
        
        # If no variations were created, return original
        if not variations:
            return pattern
        
        # Log the variations for debugging
        logger.info(f"   ğŸ”„ Generated {len(variations)} relaxation variations:")
        for name, var_pattern in variations:
            logger.info(f"      - {name}: {var_pattern[:80]}{'...' if len(var_pattern) > 80 else ''}")
        
        # Return the most aggressive variation (strategy 3 if available, else strategy 2, else strategy 1)
        # This gives the best chance to match titles without numbers or parentheses
        return variations[-1][1] if variations else pattern
    
    def _add_end_marker_exclusion(self, pattern: str, end_keywords: List[str]) -> str:
        """Add negative lookahead to exclude end markers"""
        # Create a negative lookahead pattern for all end keywords
        # Pattern: (?!.*(?:ë|ì™„|END|fin)\\s*$)
        
        exclusion_pattern = '|'.join(re.escape(kw) for kw in end_keywords)
        negative_lookahead = f'(?!.*(?:{exclusion_pattern})\\s*[>ã€‘\\])\\)]*\\s*$)'
        
        # Add at the beginning of the pattern if not already present
        if '(?!' not in pattern:
            enhanced = negative_lookahead + pattern
            return enhanced
        
        return pattern
    
    def direct_ai_title_search(
        self,
        target_file: str,
        current_pattern: str,
        expected_count: int,
        existing_matches: List[Dict[str, Any]],
        encoding: str = 'utf-8'
    ) -> List[str]:
        """Level 3: Direct AI title search using 30 samples
        
        When Level 1 (regex) and Level 2 (auto-fix) don't achieve 95% accuracy,
        ask AI to directly find chapter titles by examining 30 evenly distributed samples
        from the entire file.
        
        Args:
            target_file: Path to target file
            current_pattern: Current pattern (for context)
            expected_count: Expected number of chapters
            existing_matches: Already found matches with position and text
            encoding: File encoding
            
        Returns:
            List of title lines found by AI
        """
        logger.info("   ğŸ” [Level 3] Direct AI title search using 30 samples...")
        
        # Get examples of existing titles for context
        example_titles = [m['text'] for m in existing_matches[:10]]
        
        # Extract 30 samples from the entire file (not just gaps)
        logger.info(f"   ğŸ“Š Extracting 30 samples from file for comprehensive search...")
        samples_text = self.sampler.extract_samples(target_file, encoding=encoding)
        
        if not samples_text:
            logger.warning("   âš ï¸  Failed to extract samples")
            return []
        
        all_found_titles = []
        
        # Split samples into manageable chunks for AI processing
        # Each chunk should be around 20000 chars to fit in AI context
        MAX_CHUNK_SIZE = 20000
        chunks = []
        current_chunk = ""
        
        for line in samples_text.split('\n'):
            if len(current_chunk) + len(line) + 1 > MAX_CHUNK_SIZE:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = line + '\n'
            else:
                current_chunk += line + '\n'
        
        if current_chunk:
            chunks.append(current_chunk)
        
        logger.info(f"   ğŸ“¦ Split samples into {len(chunks)} chunks for AI processing")
        
        # Process each chunk
        for i, chunk_text in enumerate(chunks):
            logger.info(f"   ğŸ” Processing chunk {i+1}/{len(chunks)} ({len(chunk_text)} chars)")
            
            # Ask AI to find titles directly
            prompt = f"""=== direct_title_search ===
You are an expert in Korean novel structure analysis.

[Task]
Find ALL chapter title lines in the text below.
Look at the examples and find similar titles in the text.

[Examples of Chapter Titles Already Found]
{chr(10).join(f'- {title}' for title in example_titles) if example_titles else '(No examples yet - find chapter title patterns)'}

[Instructions]
1. Find lines with the SAME format/structure as the examples (or similar patterns if no examples)
2. Include titles WITH numbers and WITHOUT numbers (both are valid)
3. EXCLUDE lines ending with "ë", "ì™„", "END", "fin" (end markers)
4. EXCLUDE dialogue, body text, and page numbers
5. Return ONLY the actual title lines found

[Text to Search]
{chunk_text}

[Output]
List each found title on a separate line.
If no titles found, return "NO_TITLES_FOUND".
"""
            
            try:
                response = self.client.generate_content(prompt)
                if response and "NO_TITLES_FOUND" not in response:
                    found = [line.strip() for line in response.strip().split('\n') 
                            if line.strip() and len(line.strip()) < 100]
                    
                    if found:
                        logger.info(f"   âœ¨ Found {len(found)} titles in chunk {i+1}: {found[:3]}...")
                        all_found_titles.extend(found)
                
                time.sleep(0.5)  # Rate limiting
                
            except Exception as e:
                logger.warning(f"   âš ï¸  Direct search in chunk {i+1} failed: {e}")
        
        # Remove duplicates while preserving order
        unique_titles = []
        seen = set()
        for title in all_found_titles:
            normalized = title.strip()
            if normalized not in seen:
                seen.add(normalized)
                unique_titles.append(title)
        
        logger.info(f"   ğŸ“ [Level 3] Total unique titles found: {len(unique_titles)} (from {len(all_found_titles)} total)")
        
        return unique_titles
    
    def _build_pattern_from_examples(self, title_examples: List[str]) -> Optional[str]:
        """Build regex pattern from actual title examples (reverse extraction)
        
        Takes a list of actual chapter title lines found by AI and asks AI to
        generate a regex pattern that matches all of them.
        
        Args:
            title_examples: List of actual title lines found
            
        Returns:
            Regex pattern string or None if failed
        """
        if not title_examples:
            logger.warning("   âš ï¸  No title examples provided for reverse pattern extraction")
            return None
        
        logger.info(f"   ğŸ”„ [Reverse Extraction] Building pattern from {len(title_examples)} examples...")
        
        # Limit to 30 examples to keep prompt size reasonable
        sample_titles = title_examples[:30]
        
        prompt = f"""=== reverse_pattern_extraction ===
You are a regex expert specialized in Korean novel chapter title patterns.

Below are ACTUAL chapter title lines found in a Korean novel.
Create a Python regex pattern that matches ALL of these titles.

[Title Examples]
{chr(10).join(f'- {t}' for t in sample_titles)}

[Rules]
- The regex must match ALL examples above
- EXCLUDE lines ending with "ë", "ì™„", "END", "fin" (end markers)
- Use negative lookahead if needed: (?!.*ë\\s*$)
- Keep the pattern as precise as possible to avoid false matches
- The pattern should generalize to similar titles (not just literal matches)
- Use character classes, quantifiers, and groups appropriately

[Output Format]
Output ONLY the raw regex pattern. No markdown, no explanation, no code blocks.
Just the regex string itself.

Example output format: ^\\s*<\\s*.+?\\s*>\\s*$
"""
        
        try:
            response = self.client.generate_content(prompt)
            if response:
                # Clean up the response (remove markdown, extra whitespace)
                pattern = response.strip()
                
                # Remove markdown code blocks if present
                if pattern.startswith('```'):
                    lines = pattern.split('\n')
                    pattern = '\n'.join(l for l in lines if not l.startswith('```'))
                    pattern = pattern.strip()
                
                # Validate it's a valid regex
                try:
                    re.compile(pattern)
                    logger.info(f"   âœ… [Reverse Extraction] Generated pattern: {pattern[:80]}{'...' if len(pattern) > 80 else ''}")
                    return pattern
                except re.error as e:
                    logger.error(f"   âŒ [Reverse Extraction] Invalid regex generated: {e}")
                    return None
            
        except Exception as e:
            logger.error(f"   âŒ [Reverse Extraction] Failed to generate pattern: {e}")
        
        return None
    
    def _try_fallback(self, target_file: str, encoding: str = 'utf-8') -> Tuple[Optional[str], Optional[str]]:
        for ptn in [r"\d+\s*í™”", r"ì œ\s*\d+\s*í™”", r"\[\d+\]"]:
            stats = self.splitter.verify_pattern(target_file, ptn, encoding=encoding)
            if stats['match_count'] > 0: return (ptn, None)
        return (None, None)
