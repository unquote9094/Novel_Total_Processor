"""Stage 0: íŒŒì¼ ì¸ë±ì‹±

ë‹¤ì¤‘ í´ë” ì¬ê·€ ìŠ¤ìº”, XXHash ê³„ì‚°, ì¤‘ë³µ ê°ì§€, DB ë“±ë¡
"""

import os
import xxhash
import chardet
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from novel_total_processor.utils.logger import get_logger
from novel_total_processor.db.schema import Database
from novel_total_processor.config.loader import get_config

logger = get_logger(__name__)

# í—ˆìš© í™•ì¥ì
ALLOWED_EXTENSIONS = {".txt", ".epub"}

# ì œì™¸í•  í™•ì¥ì (ì‹¤í–‰ íŒŒì¼, ì‹œìŠ¤í…œ íŒŒì¼ ë“±)
EXCLUDED_EXTENSIONS = {
    ".exe", ".bat", ".cmd", ".sh", ".ps1",
    ".dll", ".so", ".dylib",
    ".json", ".xml", ".yml", ".yaml",
    ".zip", ".rar", ".7z", ".tar", ".gz"
}

# ìµœì†Œ íŒŒì¼ í¬ê¸° (512ë°”ì´íŠ¸ ë¯¸ë§Œ ì œì™¸)
MIN_FILE_SIZE = 512


@dataclass
class FileInfo:
    """íŒŒì¼ ì •ë³´"""
    path: str
    name: str
    ext: str
    size: int
    hash: str
    encoding: Optional[str] = None


class FileScanner:
    """íŒŒì¼ ìŠ¤ìºë„ˆ (Stage 0)"""
    
    def __init__(self, db: Database):
        """
        Args:
            db: Database ì¸ìŠ¤í„´ìŠ¤
        """
        self.db = db
        self.config = get_config()
        logger.info("FileScanner initialized")
    
    def scan_folders(self, folders: Optional[List[str]] = None) -> List[FileInfo]:
        """í´ë” ì¬ê·€ ìŠ¤ìº”
        
        Args:
            folders: ìŠ¤ìº”í•  í´ë” ëª©ë¡ (Noneì´ë©´ configì—ì„œ ì½ìŒ)
        
        Returns:
            FileInfo ë¦¬ìŠ¤íŠ¸
        """
        if folders is None:
            folders = self.config.paths.source_folders or []
        
        # Noneì´ë‚˜ ë¹ˆ ë¬¸ìì—´ í•„í„°ë§
        folders = [f for f in folders if f]
        
        logger.info(f"Scanning {len(folders)} folders...")
        
        # ì‹¤ì¢…ëœ íŒŒì¼ ì •ë¦¬ (M-30: Prune Missing Files)
        self.prune_missing_files()
        
        all_files: List[FileInfo] = []
        
        for folder in folders:
            folder_path = Path(folder)
            if not folder_path.exists():
                logger.warning(f"Folder not found: {folder}")
                continue
            
            logger.info(f"Scanning: {folder}")
            files = self._scan_folder(folder_path)
            all_files.extend(files)
            logger.info(f"  Found {len(files)} files")
        
        logger.info(f"âœ… Total files found: {len(all_files)}")
        return all_files
    
    def _scan_folder(self, folder: Path) -> List[FileInfo]:
        """ë‹¨ì¼ í´ë” ì¬ê·€ ìŠ¤ìº”
        
        Args:
            folder: í´ë” ê²½ë¡œ
        
        Returns:
            FileInfo ë¦¬ìŠ¤íŠ¸
        """
        files: List[FileInfo] = []
        
        filenames_all = []
        for root, _, filenames in os.walk(folder):
            for filename in filenames:
                filenames_all.append(Path(root) / filename)
        
        # ì´ë¦„ìˆœ ì •ë ¬ (ID ìˆœì„œ ê³ ì •)
        for file_path in sorted(filenames_all):
                
                # í™•ì¥ì í•„í„°
                ext = file_path.suffix.lower()
                if ext not in ALLOWED_EXTENSIONS:
                    continue
                
                # í¬ê¸° í•„í„°
                try:
                    size = file_path.stat().st_size
                    if size < MIN_FILE_SIZE:
                        logger.debug(f"Skipped (too small): {file_path}")
                        continue
                except Exception as e:
                    logger.warning(f"Failed to get size: {file_path} - {e}")
                    continue
                
                # ì¸ì½”ë”© ê°ì§€ ë° ìë™ UTF-8 ë³€í™˜ (TXTë§Œ)
                encoding = None
                if ext == ".txt":
                    encoding = self._detect_encoding(file_path)
                    if encoding:
                        # UTF-8ì´ ì•„ë‹ˆë©´ ë³€í™˜ ì‹¤í–‰
                        self._ensure_utf8(file_path, encoding)
                        encoding = "utf-8"  # ë³€í™˜ í›„ì—” utf-8ì„
                
                # íŒŒì¼ ì •ë³´ ë‹¤ì‹œ ì½ê¸° (ë³€í™˜ í›„ í¬ê¸°/í•´ì‹œê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ)
                try:
                    size = file_path.stat().st_size
                    file_hash = self._calculate_hash(file_path)
                except Exception as e:
                    logger.error(f"Failed to process after conversion: {file_path} - {e}")
                    continue

                files.append(FileInfo(
                    path=str(file_path.absolute()),
                    name=file_path.name,
                    ext=ext,
                    size=size,
                    hash=file_hash,
                    encoding=encoding
                ))
        
        return files

    def _ensure_utf8(self, file_path: Path, encoding: str) -> bool:
        """íŒŒì¼ì„ UTF-8ë¡œ ë³€í™˜ ë° ì €ì¥
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            encoding: í˜„ì¬ ê°ì§€ëœ ì¸ì½”ë”©
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if encoding.lower() in ['utf-8', 'utf-8-sig', 'ascii']:
            return True
            
        try:
            logger.info(f"   ğŸ”„ Converting to UTF-8: {file_path.name} ({encoding} -> utf-8)")
            
            # 1. ê°ì§€ëœ ì¸ì½”ë”©ìœ¼ë¡œ ì½ê¸°
            with open(file_path, 'r', encoding=encoding, errors='replace') as f:
                content = f.read()
            
            # 2. UTF-8ë¡œ ë®ì–´ì“°ê¸°
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
                
            return True
        except Exception as e:
            logger.error(f"   âŒ Conversion failed: {file_path} - {e}")
            return False
    
    def _calculate_hash(self, file_path: Path) -> str:
        """XXHash ê³„ì‚°
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
        
        Returns:
            16ì§„ìˆ˜ í•´ì‹œ ë¬¸ìì—´
        """
        hasher = xxhash.xxh64()
        
        with open(file_path, "rb") as f:
            while chunk := f.read(8192):
                hasher.update(chunk)
        
        return hasher.hexdigest()
    
    def _detect_encoding(self, file_path: Path, sample_size: int = 10000) -> Optional[str]:
        """ì¸ì½”ë”© ê°ì§€
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            sample_size: ìƒ˜í”Œ í¬ê¸° (ë°”ì´íŠ¸)
        
        Returns:
            ì¸ì½”ë”© ì´ë¦„ (ì˜ˆ: 'utf-8', 'cp949')
        """
        try:
            with open(file_path, "rb") as f:
                sample = f.read(sample_size)
            
            result = chardet.detect(sample)
            encoding = result.get("encoding")
            confidence = result.get("confidence", 0)
            
            if confidence > 0.7:
                logger.debug(f"Encoding detected: {encoding} ({confidence:.2f}) - {file_path.name}")
                return encoding
            else:
                logger.debug(f"Low confidence encoding: {encoding} ({confidence:.2f}) - {file_path.name}")
                return None
        except Exception as e:
            logger.warning(f"Encoding detection failed: {file_path} - {e}")
            return None
    
    def detect_duplicates(self, files: List[FileInfo]) -> Dict[str, List[FileInfo]]:
        """ì¤‘ë³µ íŒŒì¼ ê°ì§€
        
        Args:
            files: FileInfo ë¦¬ìŠ¤íŠ¸
        
        Returns:
            í•´ì‹œë³„ ì¤‘ë³µ íŒŒì¼ ê·¸ë£¹ {hash: [FileInfo, ...]}
        """
        hash_map: Dict[str, List[FileInfo]] = {}
        
        for file in files:
            if file.hash not in hash_map:
                hash_map[file.hash] = []
            hash_map[file.hash].append(file)
        
        # ì¤‘ë³µë§Œ í•„í„°ë§ (2ê°œ ì´ìƒ)
        duplicates = {h: fs for h, fs in hash_map.items() if len(fs) > 1}
        
        if duplicates:
            logger.warning(f"âš ï¸ Found {len(duplicates)} duplicate groups ({sum(len(fs) for fs in duplicates.values())} files)")
            for hash_val, dup_files in duplicates.items():
                logger.debug(f"  Hash {hash_val[:8]}... : {len(dup_files)} files")
                for f in dup_files:
                    logger.debug(f"    - {f.name}")
        else:
            logger.info("âœ… No duplicates found")
        
        return duplicates
    
    def save_to_db(self, files: List[FileInfo], duplicates: Dict[str, List[FileInfo]]) -> int:
        """DBì— ì €ì¥
        
        Args:
            files: FileInfo ë¦¬ìŠ¤íŠ¸
            duplicates: ì¤‘ë³µ íŒŒì¼ ê·¸ë£¹
        
        Returns:
            ì €ì¥ëœ íŒŒì¼ ìˆ˜
        """
        logger.info("Saving to database...")
        conn = self.db.connect()
        cursor = conn.cursor()
        
        saved_count = 0
        
        for file in files:
            # ì¤‘ë³µ ì²´í¬
            is_duplicate = 0
            duplicate_of = None
            
            if file.hash in duplicates:
                dup_group = duplicates[file.hash]
                # ì²« ë²ˆì§¸ íŒŒì¼ì„ ì›ë³¸ìœ¼ë¡œ
                if file.path != dup_group[0].path:
                    is_duplicate = 1
                    # ì›ë³¸ íŒŒì¼ ID ì°¾ê¸°
                    cursor.execute(
                        "SELECT id FROM files WHERE file_path = ?",
                        (dup_group[0].path,)
                    )
                    result = cursor.fetchone()
                    if result:
                        duplicate_of = result[0]
            
            try:
                cursor.execute("""
                    INSERT OR REPLACE INTO files 
                    (file_path, file_name, file_ext, file_size, file_hash, encoding, is_duplicate, duplicate_of)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    file.path,
                    file.name,
                    file.ext,
                    file.size,
                    file.hash,
                    file.encoding,
                    is_duplicate,
                    duplicate_of
                ))
                
                # processing_state ì´ˆê¸°í™”
                file_id = cursor.lastrowid
                cursor.execute("""
                    INSERT OR REPLACE INTO processing_state (file_id, stage0_indexed)
                    VALUES (?, 1)
                """, (file_id,))
                
                saved_count += 1
            except Exception as e:
                logger.error(f"Failed to save: {file.path} - {e}")
        
        conn.commit()
        logger.info(f"âœ… Saved {saved_count} files to database")
        return saved_count
    
    def run(self) -> Tuple[int, int]:
        """Stage 0 ì‹¤í–‰
        
        Returns:
            (ì´ íŒŒì¼ ìˆ˜, ì¤‘ë³µ íŒŒì¼ ìˆ˜)
        """
        logger.info("=" * 50)
        logger.info("Stage 0: File Indexing")
        logger.info("=" * 50)
        
        # 1. ìŠ¤ìº”
        files = self.scan_folders()
        
        if not files:
            logger.warning("No files found!")
            return 0, 0
        
        # 2. ì¤‘ë³µ ê°ì§€
        duplicates = self.detect_duplicates(files)
        duplicate_count = sum(len(fs) - 1 for fs in duplicates.values())
        
        # 3. DB ì €ì¥
        saved = self.save_to_db(files, duplicates)
        
        logger.info("=" * 50)
        logger.info(f"âœ… Stage 0 Complete: {saved} files indexed, {duplicate_count} duplicates")
        logger.info("=" * 50)
        
        return saved, duplicate_count
    def prune_missing_files(self) -> None:
        """DBì—ëŠ” ìˆìœ¼ë‚˜ ë””ìŠ¤í¬ì— ì‹¤ì¬í•˜ì§€ ì•ŠëŠ” íŒŒì¼ ë ˆì½”ë“œ ì •ë¦¬ (M-30)"""
        conn = self.db.connect()
        cursor = conn.cursor()
        
        cursor.execute("SELECT id, file_path FROM files")
        rows = cursor.fetchall()
        
        missing_ids = []
        for fid, fpath in rows:
            if not Path(fpath).exists():
                missing_ids.append(fid)
        
        if missing_ids:
            logger.info(f"ğŸ—‘ï¸  ì‹¤ì¢…ëœ íŒŒì¼ {len(missing_ids)}ê°œë¥¼ DBì—ì„œ ì •ë¦¬ ì¤‘...")
            # íŒŒì´í”„ë¼ì¸ ìƒíƒœ ë° íŒ¨í„´ ë°ì´í„°ë„ í•¨ê»˜ ì‚­ì œ (CASCADE ì œì•½ ì¡°ê±´ì´ ì—†ì„ ê²½ìš° ëŒ€ë¹„)
            placeholders = ",".join(["?"] * len(missing_ids))
            
            cursor.execute(f"DELETE FROM processing_state WHERE file_id IN ({placeholders})", missing_ids)
            cursor.execute(f"DELETE FROM episode_patterns WHERE file_id IN ({placeholders})", missing_ids)
            cursor.execute(f"DELETE FROM rename_plan WHERE file_id IN ({placeholders})", missing_ids)
            cursor.execute(f"DELETE FROM files WHERE id IN ({placeholders})", missing_ids)
            
            conn.commit()
            logger.info(f"âœ… ì •ë¦¬ ì™„ë£Œ")
