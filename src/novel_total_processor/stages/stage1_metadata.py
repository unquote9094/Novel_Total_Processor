"""Stage 1: ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘

Geminië¡œ íŒŒì¼ëª… ë¶„ì„ â†’ Perplexityë¡œ ì›¹ ê²€ìƒ‰ â†’ DB ì €ì¥
"""

import json
import time
from typing import List, Dict, Any, Optional
from novel_total_processor.utils.logger import get_logger
from novel_total_processor.db.schema import Database
from novel_total_processor.ai.gemini_client import GeminiClient, NovelMetadata
from novel_total_processor.ai.perplexity_client import PerplexityClient
from novel_total_processor.config.loader import get_config

logger = get_logger(__name__)


class MetadataCollector:
    """ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ê¸° (Stage 1)"""
    
    def __init__(self, db: Database):
        """
        Args:
            db: Database ì¸ìŠ¤í„´ìŠ¤
        """
        self.db = db
        self.config = get_config()
        self.gemini = GeminiClient()
        self.perplexity = PerplexityClient()
        logger.info("MetadataCollector initialized")
    
    def get_pending_files(self, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """Stage 1 ëŒ€ê¸° ì¤‘ì¸ íŒŒì¼ ì¡°íšŒ
        
        Args:
            limit: ìµœëŒ€ íŒŒì¼ ìˆ˜
        
        Returns:
            íŒŒì¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸ [{"id": int, "filename": str, "hash": str}, ...]
        """
        conn = self.db.connect()
        cursor = conn.cursor()
        
        query = """
            SELECT f.id, f.file_name, f.file_hash
            FROM files f
            JOIN processing_state ps ON f.id = ps.file_id
            WHERE ps.stage0_indexed = 1 AND ps.stage1_meta = 0
            AND f.is_duplicate = 0
            ORDER BY f.id ASC
        """
        
        if limit:
            query += f" LIMIT {limit}"
        
        cursor.execute(query)
        rows = cursor.fetchall()
        
        files = [
            {"id": row[0], "filename": row[1], "hash": row[2]}
            for row in rows
        ]
        
        logger.info(f"Found {len(files)} files pending for Stage 1")
        return files
    
    def process_file(self, file_id: int, filename: str, file_hash: str) -> bool:
        """ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
        
        Args:
            file_id: íŒŒì¼ ID
            filename: íŒŒì¼ëª…
            file_hash: íŒŒì¼ í•´ì‹œ
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            # 1. Geminië¡œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ìµœëŒ€ 3íšŒ ì¬ì‹œë„)
            metadata = None
            for attempt in range(1, 4):
                logger.info(f"   [AI 1/2] Gemini searching (Attempt {attempt}/3): {filename}")
                metadata = self.gemini.extract_metadata_from_filename(filename, file_hash)
                
                # ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì–»ì—ˆëŠ”ì§€ í™•ì¸ (ì œëª© ì™¸ì— ì‘ê°€, ì¥ë¥´, íƒœê·¸ ì¤‘ í•˜ë‚˜ë¼ë„ ìˆëŠ” ê²½ìš°)
                if metadata and metadata.title and (metadata.author or metadata.genre or metadata.tags):
                    logger.info("   âœ… Gemini search successful (info found)")
                    logger.info(f"      - Title: {metadata.title}")
                    logger.info(f"      - Author: {metadata.author}")
                    logger.info(f"      - Rating: {metadata.rating}")
                    logger.info(f"      - Status: {metadata.status}")
                    break
                else:
                    logger.warning(f"   âš ï¸ Gemini result insufficient. {'Retrying...' if attempt < 3 else 'Giving up.'}")
                    if attempt < 3:
                        time.sleep(1)
            
            # 2. Perplexityë¡œ ë³´ì¡° ê²€ìƒ‰ (ìµœëŒ€ 3íšŒ ì¬ì‹œë„)
            extra_info = None
            if self.perplexity.enabled and metadata.title:
                for attempt in range(1, 4):
                    logger.info(f"   [AI 2/2] Perplexity searching (Attempt {attempt}/3): {metadata.title}")
                    extra_info = self.perplexity.search_novel_info(metadata.title, metadata.author)
                    
                    # ì œëª© ì™¸ì— ë‹¤ë¥¸ ìœ ì˜ë¯¸í•œ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
                    if extra_info and (extra_info.get("author") or extra_info.get("genre") or extra_info.get("rating")):
                        logger.info("   âœ… Perplexity search successful (info found)")
                        break
                    else:
                        logger.warning(f"   âš ï¸ Perplexity result insufficient. {'Retrying...' if attempt < 3 else 'Giving up.'}")
                        if attempt < 3:
                            time.sleep(1)
            
            # 3. ë°ì´í„° ë³‘í•© (Merge) - ê³ ë„í™” ë²„ì „
            # ì›ì¹™: 1) ìš°ì„ ìˆœìœ„ í”Œë«í¼ ì •ë³´ä¼˜å…ˆ, 2) ë‚ ì§œê°€ ë” ìµœì‹ ì¸ ì •ë³´ä¼˜å…ˆ
            if extra_info:
                # í—¬í¼: ë‚ ì§œ ë¹„êµ
                def is_newer(d1: Optional[str], d2: Optional[str]) -> bool:
                    if not d1: return False
                    if not d2: return True
                    # YYYY-MM-DD í¬ë§· ê°€ì •
                    return d1 > d2

                # í—¬í¼: ìš°ì„ ìˆœìœ„ ì‚¬ì´íŠ¸ ì—¬ë¶€
                priority_sites = ["ë…¸ë²¨í”¼ì•„", "ë¬¸í”¼ì•„", "ì¡°ì•„ë¼", "ë¦¬ë””", "ì¹´ì¹´ì˜¤", "ë„¤ì´ë²„", "ë¸”ë¼ì´ìŠ¤", "ì›ìŠ¤í† ë¦¬"]
                def has_priority(p: Optional[str]) -> bool:
                    if not p: return False
                    return any(s in p for s in priority_sites)

                # ë³‘í•© ê²°ì • ë¡œì§ (Base: Gemini, Extra: Perplexity)
                p_newer = is_newer(extra_info.get("last_updated"), metadata.last_updated)
                p_priority = has_priority(extra_info.get("platform"))
                g_priority = has_priority(metadata.platform)

                # 0) ì œëª©: Perplexityê°€ ê³µì‹ ì‚¬ì´íŠ¸(ìš°ì„ ìˆœìœ„) ì œëª©ì„ ì°¾ì•˜ê±°ë‚˜ ë” ìµœì‹ ì¼ ê²½ìš° ì±„íƒ
                if extra_info.get("title") and (p_priority or p_newer or metadata.title.startswith("#")):
                    metadata.title = extra_info["title"]

                # 1) ì‘ê°€, ì¥ë¥´: Geminiê°€ ëª» ì°¾ì•˜ê±°ë‚˜ Perplexityê°€ ìš°ì„ ìˆœìœ„/ìµœì‹ ì¼ ê²½ìš°
                if extra_info.get("author") and (not metadata.author or p_priority or p_newer):
                    metadata.author = extra_info["author"]
                
                # M-42: ì¥ë¥´ ë³‘í•© (í•˜ë‚˜ë§Œ íƒí•˜ì§€ ì•Šê³  í†µí•©)
                if extra_info.get("genre"):
                    if metadata.genre and metadata.genre != extra_info["genre"]:
                        genres = {g.strip() for g in (metadata.genre + "," + extra_info["genre"]).split(",") if g.strip()}
                        metadata.genre = ", ".join(sorted(genres))
                    else:
                        metadata.genre = extra_info["genre"]

                # 2) ìƒíƒœ, í™”ìˆ˜: ìµœì‹  ì •ë³´(ë‚ ì§œ)ê°€ ê°€ì¥ ì¤‘ìš”í•˜ë˜, 'ì™„ê²°'ì€ ë¬´ì¡°ê±´ ìš°ì„ 
                p_status = extra_info.get("status")
                if p_status:
                    if "ì™„ê²°" in str(p_status) or "ì™„ê²°" in str(metadata.status):
                        metadata.status = "ì™„ê²°"
                    elif not metadata.status or p_newer:
                        metadata.status = p_status
                
                if extra_info.get("episode_range") and (not metadata.episode_range or p_newer):
                    metadata.episode_range = extra_info["episode_range"]
                
                # 3) ë‚ ì§œ ë° í”Œë«í¼ ì—…ë°ì´íŠ¸
                if p_newer:
                    metadata.last_updated = extra_info["last_updated"]
                if p_priority:
                    metadata.platform = extra_info["platform"]

                # 4) ë³„ì : Perplexity ê²ƒì´ ìœ íš¨(0.0 ì•„ë‹˜)í•˜ê³  ìµœì‹ ì´ê±°ë‚˜ Geminiê°€ ì—†ì„ ë•Œ
                p_rating = extra_info.get("rating")
                if p_rating and p_rating > 0 and (not metadata.rating or p_newer):
                    metadata.rating = p_rating

                # 5) í‘œì§€: ìµœì‹ ì´ê±°ë‚˜ ìš°ì„ ìˆœìœ„ ì‚¬ì´íŠ¸ ê²ƒ ìš°ì„ 
                if extra_info.get("cover_url") and (not metadata.cover_url or p_priority or p_newer):
                    metadata.cover_url = extra_info["cover_url"]

                # 6) íƒœê·¸: ë³‘í•© ë° ì„±ì¸ë¬¼ íŒë³„ (M-42)
                if extra_info.get("tags"):
                    all_tags = set(metadata.tags or []) | set(extra_info["tags"])
                    
                    # ì„±ì¸ë¬¼ íŒë³„ í‚¤ì›Œë“œ
                    adult_keywords = ["ì„±ì¸", "19ê¸ˆ", "ì•¼ê²œ", "R19", "ë…¸ë¸”ë ˆìŠ¤", "ì„±ì¸ë¬¼"]
                    is_adult = any(kw in str(all_tags) for kw in adult_keywords)
                    if is_adult:
                        if not metadata.genre: metadata.genre = "ì„±ì¸ë¬¼"
                        elif "ì„±ì¸ë¬¼" not in metadata.genre:
                            metadata.genre = "ì„±ì¸ë¬¼, " + metadata.genre
                            
                    metadata.tags = list(all_tags)[:15] # íƒœê·¸ ìˆ˜ ì•½ê°„ í™•ì¥

                # 7) ê³µì‹ URL ë³‘í•© (M-49)
                if extra_info.get("source_url") and not metadata.official_url:
                    metadata.official_url = extra_info["source_url"]

            # 3.5 êµ¬ê¸€ ì´ë¯¸ì§€ ê²€ìƒ‰ ë³´ê°• (í‘œì§€ê°€ ì—†ê±°ë‚˜ ì €í™”ì§ˆì¼ ê²½ìš°)
            if not metadata.cover_url or "novelpia_books_icon" in metadata.cover_url:
                logger.info(f"   ğŸ” Cover missing or low quality. Trying dedicated Google Image search...")
                # Geminiì˜ Google Search Groundingì„ ë‹¤ì‹œ í™œìš©í•˜ì—¬ ì „ìš© ì´ë¯¸ì§€ ì¿¼ë¦¬ ì‹¤í–‰
                img_prompt = f'"{metadata.title}" {metadata.author or ""} ì†Œì„¤ ê³µì‹ ë‹¨í–‰ë³¸ í‘œì§€ ì´ë¯¸ì§€ ê³ í™”ì§ˆ direct image url format'
                img_metadata = self.gemini.extract_metadata_from_filename(img_prompt, f"img_{file_hash}")
                if img_metadata.cover_url and "novelpia_books_icon" not in img_metadata.cover_url:
                    metadata.cover_url = img_metadata.cover_url
                    logger.info(f"   âœ… Found better cover via Google search: {metadata.cover_url}")
            
            # 4. í‘œì§€ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            cover_path = None
            final_cover_url = metadata.cover_url
            if final_cover_url:
                logger.info(f"   ğŸ–¼ï¸ Downloading cover: {final_cover_url}")
                cover_path = self.perplexity.download_cover(final_cover_url, file_id)

            # 5. ìµœì¢… ë³‘í•© ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            logger.info(f"   [Final Merged Result]")
            logger.info(f"     â€¢ Title: {metadata.title}")
            logger.info(f"     â€¢ Author: {metadata.author}")
            logger.info(f"     â€¢ Genre: {metadata.genre}")
            logger.info(f"     â€¢ Rating: {metadata.rating}")
            logger.info(f"     â€¢ Platform: {metadata.platform}")
            logger.info(f"     â€¢ Updated: {metadata.last_updated}")
            logger.info(f"     â€¢ Tags: {', '.join(metadata.tags) if metadata.tags else '[]'}")
            logger.info(f"     â€¢ Status: {metadata.status}")
            logger.info(f"     â€¢ Official URL: {metadata.official_url}")
            logger.info(f"     â€¢ Cover: {'[Success]' if cover_path else '[No/Failed]'}")
            
            # DB ì •ë³´ ì—…ë°ì´íŠ¸ (platform, last_updated ë“±ì€ novel_extra ë˜ëŠ” ê¸°ì¡´ í…Œì´ë¸” í™•ì¥ í•„ìš”í•˜ë‚˜ í˜„ì¬ëŠ” ë¡œê·¸ ì¶œë ¥ ìœ„ì£¼)

            # M-46: íŒŒì¼ëª… íŒíŠ¸ ê°•ì œ ë™ê¸°í™” (AIê°€ ë†“ì³¤ì„ ê²½ìš° ëŒ€ë¹„)
            self._apply_filename_hints(metadata, filename)
            
            # 6. DB ì €ì¥
            self._save_to_db(file_id, metadata, extra_info, cover_path)
            
            logger.debug(f"âœ… Processed: {filename}")
            return True
        
        except Exception as e:
            logger.error(f"Failed to process {filename}: {e}")
            self._mark_error(file_id, str(e))
            return False
    
    def _apply_filename_hints(self, metadata: NovelMetadata, filename: str) -> None:
        """íŒŒì¼ëª…ì—ì„œ í™”ìˆ˜ íŒíŠ¸ ë“±ì„ ì¶”ì¶œí•˜ì—¬ ë©”íƒ€ë°ì´í„° ë³´ê°• (M-46)"""
        import re
        # (1~321) ë˜ëŠ” (321) ë“±ì—ì„œ í™”ìˆ˜ ì¶”ì¶œ
        hint_nums = re.findall(r'\((\d+~\d+)\)', filename)
        hint_range = None
        if not hint_nums:
            hint_nums = re.findall(r'\((\d+)\)', filename)
            if hint_nums: hint_range = f"1~{hint_nums[0]}í™”"
        else:
            hint_range = f"{hint_nums[0]}í™”"
            
        if hint_range:
            # ê¸°ì¡´ ì •ë³´ê°€ ë¶€ì‹¤í•˜ê±°ë‚˜ 'Unknown'ì´ë©´ ë®ì–´ì”€
            if not metadata.episode_range or "Unknown" in str(metadata.episode_range):
                metadata.episode_range = hint_range
                logger.info(f"   âœ¨ [Hint Apply] íŒŒì¼ëª…ì—ì„œ í™”ìˆ˜ ì •ë³´ ì¶”ì¶œ: {hint_range}")
        
        # Unknown ë¬¸ìì—´ ì œê±° (ì‘ê°€, ì¥ë¥´ ë“±)
        if metadata.author and "Unknown" in metadata.author: metadata.author = None
        if metadata.genre and "Unknown" in metadata.genre: metadata.genre = None
        if metadata.status and "Unknown" in metadata.status: metadata.status = None

    def _save_to_db(
        self,
        file_id: int,
        metadata: NovelMetadata,
        extra_info: Optional[Dict[str, Any]],
        cover_path: Optional[str] = None
    ) -> None:
        """DBì— ì €ì¥
        
        Args:
            file_id: íŒŒì¼ ID
            metadata: Gemini ë©”íƒ€ë°ì´í„°
            extra_info: Perplexity ì¶”ê°€ ì •ë³´
        """
        conn = self.db.connect()
        cursor = conn.cursor()
        
        # novels í…Œì´ë¸” ì‚½ì…/ì—…ë°ì´íŠ¸
        cursor.execute("""
            INSERT INTO novels (title, author, genre, tags, status, episode_range, rating, cover_path, platform, last_updated, official_url)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            metadata.title,
            metadata.author,
            metadata.genre,
            ", ".join(metadata.tags) if metadata.tags else None,
            metadata.status,
            metadata.episode_range,
            metadata.rating,
            cover_path,
            metadata.platform,
            metadata.last_updated,
            metadata.official_url
        ))
        
        novel_id = cursor.lastrowid
        
        # íŒŒì¼ í…Œì´ë¸”ì— novel_id ì—°ê²°
        cursor.execute("UPDATE files SET novel_id = ? WHERE id = ?", (novel_id, file_id))
        
        # novel_extra í…Œì´ë¸” (Perplexity ì •ë³´)
        if extra_info:
            cursor.execute("""
                INSERT INTO novel_extra (novel_id, source_url)
                VALUES (?, ?)
            """, (
                novel_id,
                extra_info.get("source_url")
            ))
        
        # processing_state ì—…ë°ì´íŠ¸
        cursor.execute("""
            UPDATE processing_state
            SET stage1_meta = 1, last_stage = 'stage1', updated_at = datetime('now','localtime')
            WHERE file_id = ?
        """, (file_id,))
        
        conn.commit()
    
    def _mark_error(self, file_id: int, error_msg: str) -> None:
        """ì—ëŸ¬ ê¸°ë¡
        
        Args:
            file_id: íŒŒì¼ ID
            error_msg: ì—ëŸ¬ ë©”ì‹œì§€
        """
        conn = self.db.connect()
        cursor = conn.cursor()
        
        cursor.execute("""
            UPDATE processing_state
            SET last_error = ?, last_stage = 'stage1', updated_at = datetime('now','localtime')
            WHERE file_id = ?
        """, (error_msg, file_id))
        
        conn.commit()
    
    def run(self, limit: Optional[int] = None, batch_size: int = 10) -> Dict[str, int]:
        """Stage 1 ì‹¤í–‰
        
        Args:
            limit: ì²˜ë¦¬í•  ìµœëŒ€ íŒŒì¼ ìˆ˜
            batch_size: ë°°ì¹˜ í¬ê¸°
        
        Returns:
            {"total": int, "success": int, "failed": int}
        """
        logger.info("=" * 50)
        logger.info("Stage 1: Metadata Collection")
        logger.info("=" * 50)
        
        # ëŒ€ê¸° íŒŒì¼ ì¡°íšŒ
        files = self.get_pending_files(limit)
        
        if not files:
            logger.warning("No files to process")
            return {"total": 0, "success": 0, "failed": 0}
        
        # ì²˜ë¦¬
        success_count = 0
        failed_count = 0
        
        for i, file in enumerate(files):
            logger.info(f"[{i+1}/{len(files)}] {file['filename']}")
            
            if self.process_file(file["id"], file["filename"], file["hash"]):
                success_count += 1
            else:
                failed_count += 1
        
        logger.info("=" * 50)
        logger.info(f"âœ… Stage 1 Complete: {success_count} success, {failed_count} failed")
        logger.info("=" * 50)
        
        return {
            "total": len(files),
            "success": success_count,
            "failed": failed_count
        }
