"""Stage 1: ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘

Geminië¡œ íŒŒì¼ëª… ë¶„ì„ â†’ Perplexityë¡œ ì›¹ ê²€ìƒ‰ â†’ DB ì €ì¥
"""

import json
import time
from typing import List, Dict, Any, Optional
from novel_total_processor.utils.logger import get_logger
from novel_total_processor.db.schema import Database
from novel_total_processor.ai.gemini_client import GeminiClient, NovelMetadata
from novel_total_processor.ai.perplexity_client import PerplexityClient
from novel_total_processor.config.loader import get_config
from novel_total_processor.utils.text_cleaner import clean_search_title, extract_episode_range_numeric

logger = get_logger(__name__)


class MetadataCollector:
    """ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ê¸° (Stage 1)"""
    
    def __init__(self, db: Database):
        """
        Args:
            db: Database ì¸ìŠ¤í„´ìŠ¤
        """
        self.db = db
        self.config = get_config()
        self.gemini = GeminiClient()
        self.perplexity = PerplexityClient()
        logger.info("MetadataCollector initialized")
    
    def _check_metadata_sufficient(self, metadata: Optional[NovelMetadata]) -> bool:
        """ë©”íƒ€ë°ì´í„°ê°€ ìµœì†Œ ì„±ê³µ ê¸°ì¤€ì„ ë§Œì¡±í•˜ëŠ”ì§€ í™•ì¸
        
        ìµœì†Œ ì„±ê³µ ê¸°ì¤€: title + author + genre ì¤‘ 2ê°œ ì´ìƒ ì¡´ì¬
        
        Args:
            metadata: ê²€ì¦í•  ë©”íƒ€ë°ì´í„°
        
        Returns:
            ì¶©ë¶„í•œ ì •ë³´ê°€ ìˆìœ¼ë©´ True, ì•„ë‹ˆë©´ False
        """
        if not metadata:
            return False
        
        # titleì€ í•­ìƒ ìˆë‹¤ê³  ê°€ì • (íŒŒì¼ëª…ì—ì„œ ìµœì†Œí•œ ì¶”ì¶œ)
        # author, genre ì¤‘ ìµœì†Œ 1ê°œ ì´ìƒ ìˆì–´ì•¼ í•¨
        has_title = True  # íŒŒì¼ëª…ì—ì„œ ìµœì†Œí•œ ì œëª©ì€ ì¶”ì¶œë¨
        has_author = bool(metadata.author and metadata.author.strip() and "Unknown" not in metadata.author)
        has_genre = bool(metadata.genre and metadata.genre.strip() and "Unknown" not in metadata.genre)
        
        # title + (author or genre) ì¡°í•©ì´ë©´ ì¶©ë¶„
        count = sum([has_title, has_author, has_genre])
        
        if count >= 2:
            return True
        
        logger.debug(f"   Insufficient metadata: author={has_author}, genre={has_genre}")
        return False
    
    def _merge_metadata(self, base: NovelMetadata, extra: Dict[str, Any]) -> NovelMetadata:
        """ë©”íƒ€ë°ì´í„° ë³‘í•© (ê°œì„ ëœ ë¡œì§)
        
        ë³‘í•© ìš°ì„ ìˆœìœ„:
        1. ë” í° episode_range ê°’ ìš°ì„ 
        2. ë” ìµœì‹  last_updated ê°’ ìš°ì„ 
        3. í”Œë«í¼ ìš°ì„ ìˆœìœ„ (ë…¸ë²¨í”¼ì•„, ë„¤ì´ë²„ ì‹œë¦¬ì¦ˆ, ë¦¬ë””, ë„¤ì´ë²„ ì›¹ì†Œì„¤, ì¹´ì¹´ì˜¤, ë¬¸í”¼ì•„, ì¡°ì•„ë¼)
        
        Args:
            base: ê¸°ë³¸ ë©”íƒ€ë°ì´í„° (Gemini ê²°ê³¼)
            extra: ì¶”ê°€ ì •ë³´ (Perplexity ê²°ê³¼)
        
        Returns:
            ë³‘í•©ëœ ë©”íƒ€ë°ì´í„°
        """
        # í”Œë«í¼ ìš°ì„ ìˆœìœ„ ì •ì˜
        priority_platforms = [
            "ë…¸ë²¨í”¼ì•„",
            "ë„¤ì´ë²„ ì‹œë¦¬ì¦ˆ",
            "ë¦¬ë””",
            "ë„¤ì´ë²„ ì›¹ì†Œì„¤",
            "ì¹´ì¹´ì˜¤",
            "ë¬¸í”¼ì•„",
            "ì¡°ì•„ë¼"
        ]
        
        def get_platform_priority(platform: Optional[str]) -> int:
            """í”Œë«í¼ ìš°ì„ ìˆœìœ„ ì ìˆ˜ ë°˜í™˜ (ë‚®ì„ìˆ˜ë¡ ìš°ì„ )"""
            if not platform:
                return 999
            for i, p in enumerate(priority_platforms):
                if p in platform:
                    return i
            return 900  # ê¸°íƒ€ í”Œë«í¼
        
        def is_newer(d1: Optional[str], d2: Optional[str]) -> bool:
            """ë‚ ì§œ ë¹„êµ (d1ì´ d2ë³´ë‹¤ ìµœì‹ ì´ë©´ True)"""
            if not d1:
                return False
            if not d2:
                return True
            return d1 > d2
        
        # ì—í”¼ì†Œë“œ ë²”ìœ„ ë¹„êµ
        base_ep_num = extract_episode_range_numeric(base.episode_range)
        extra_ep_num = extract_episode_range_numeric(extra.get("episode_range"))
        
        # í”Œë«í¼ ìš°ì„ ìˆœìœ„ ë¹„êµ
        base_priority = get_platform_priority(base.platform)
        extra_priority = get_platform_priority(extra.get("platform"))
        
        # ë‚ ì§œ ë¹„êµ
        extra_is_newer = is_newer(extra.get("last_updated"), base.last_updated)
        
        # ë³‘í•© ë¡œì§
        logger.info("   [Merge Decision]")
        
        # ì œëª©: Perplexityê°€ ìš°ì„ ìˆœìœ„ í”Œë«í¼ì´ê±°ë‚˜ ë” ìµœì‹ ì¼ ê²½ìš° ì±„íƒ
        if extra.get("title"):
            if extra_priority < base_priority or extra_is_newer or (base.title and base.title.startswith("#")):
                logger.info(f"     â†’ Title: Using Perplexity result (priority or newer)")
                base.title = extra["title"]
        
        # ì‘ê°€: ì—†ê±°ë‚˜ Perplexityê°€ ìš°ì„ ì¼ ê²½ìš°
        if extra.get("author"):
            if not base.author or extra_priority < base_priority or extra_is_newer:
                if base.author != extra["author"]:
                    logger.info(f"     â†’ Author: '{base.author}' â†’ '{extra['author']}' (priority or newer)")
                base.author = extra["author"]
        
        # ì¥ë¥´: ë³‘í•© (í†µí•©)
        if extra.get("genre"):
            if base.genre and base.genre != extra["genre"]:
                genres = {g.strip() for g in (base.genre + "," + extra["genre"]).split(",") if g.strip()}
                merged_genre = ", ".join(sorted(genres))
                logger.info(f"     â†’ Genre: Merged '{base.genre}' + '{extra['genre']}' = '{merged_genre}'")
                base.genre = merged_genre
            elif not base.genre:
                base.genre = extra["genre"]
        
        # ìƒíƒœ: 'ì™„ê²°'ì€ ë¬´ì¡°ê±´ ìš°ì„ , ê·¸ ì™¸ëŠ” ìµœì‹  ì •ë³´ ìš°ì„ 
        if extra.get("status"):
            if "ì™„ê²°" in str(extra["status"]) or "ì™„ê²°" in str(base.status):
                base.status = "ì™„ê²°"
                logger.info(f"     â†’ Status: 'ì™„ê²°' (prioritized)")
            elif not base.status or extra_is_newer:
                base.status = extra["status"]
        
        # ì—í”¼ì†Œë“œ ë²”ìœ„: ë” í° ê°’ ìš°ì„ , ê°™ìœ¼ë©´ ìµœì‹  ì •ë³´ ìš°ì„ 
        if extra.get("episode_range"):
            if base_ep_num and extra_ep_num:
                if extra_ep_num > base_ep_num:
                    logger.info(f"     â†’ Episode Range: {base.episode_range} â†’ {extra['episode_range']} (larger)")
                    base.episode_range = extra["episode_range"]
                elif extra_ep_num == base_ep_num and extra_is_newer:
                    logger.info(f"     â†’ Episode Range: {base.episode_range} â†’ {extra['episode_range']} (same, but newer)")
                    base.episode_range = extra["episode_range"]
            elif not base.episode_range or extra_is_newer:
                base.episode_range = extra["episode_range"]
        
        # ë‚ ì§œ: ìµœì‹  ì •ë³´ ì‚¬ìš©
        if extra_is_newer:
            logger.info(f"     â†’ Last Updated: {base.last_updated} â†’ {extra['last_updated']} (newer)")
            base.last_updated = extra["last_updated"]
        
        # í”Œë«í¼: ìš°ì„ ìˆœìœ„ê°€ ë†’ìœ¼ë©´ êµì²´
        if extra_priority < base_priority:
            logger.info(f"     â†’ Platform: '{base.platform}' â†’ '{extra['platform']}' (higher priority)")
            base.platform = extra["platform"]
        
        # í‰ì : ë” ë†’ì€ í‰ì  ìš°ì„ , ê±°ì˜ ê°™ìœ¼ë©´ ìµœì‹  ì •ë³´
        extra_rating = extra.get("rating")
        if extra_rating and extra_rating > 0:
            if not base.rating or extra_rating > base.rating:
                logger.info(f"     â†’ Rating: {base.rating} â†’ {extra_rating} (higher)")
                base.rating = extra_rating
            elif extra_is_newer and abs(extra_rating - base.rating) < 0.1:
                base.rating = extra_rating
        
        # í‘œì§€: ìš°ì„ ìˆœìœ„ í”Œë«í¼ì´ê±°ë‚˜ ìµœì‹ ì¼ ê²½ìš°
        if extra.get("cover_url"):
            if not base.cover_url or extra_priority < base_priority or extra_is_newer:
                logger.info(f"     â†’ Cover: Using Perplexity result")
                base.cover_url = extra["cover_url"]
        
        # íƒœê·¸: ë³‘í•© ë° ì„±ì¸ë¬¼ íŒë³„
        if extra.get("tags"):
            all_tags = set(base.tags or []) | set(extra["tags"])
            
            # ì„±ì¸ë¬¼ íŒë³„
            adult_keywords = ["ì„±ì¸", "19ê¸ˆ", "ì•¼ê²œ", "R19", "ë…¸ë¸”ë ˆìŠ¤", "ì„±ì¸ë¬¼"]
            is_adult = any(kw in str(all_tags) for kw in adult_keywords)
            if is_adult:
                if not base.genre:
                    base.genre = "ì„±ì¸ë¬¼"
                elif "ì„±ì¸ë¬¼" not in base.genre:
                    base.genre = "ì„±ì¸ë¬¼, " + base.genre
            
            merged_tags_count = len(all_tags)
            logger.info(f"     â†’ Tags: Merged ({merged_tags_count} total tags)")
            base.tags = list(all_tags)[:15]
        
        # ê³µì‹ URL: ì—†ìœ¼ë©´ ì¶”ê°€
        if extra.get("source_url") and not base.official_url:
            logger.info(f"     â†’ Official URL: {extra['source_url']}")
            base.official_url = extra["source_url"]
        
        return base
    
    def get_pending_files(self, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """Stage 1 ëŒ€ê¸° ì¤‘ì¸ íŒŒì¼ ì¡°íšŒ
        
        Args:
            limit: ìµœëŒ€ íŒŒì¼ ìˆ˜
        
        Returns:
            íŒŒì¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸ [{"id": int, "filename": str, "hash": str}, ...]
        """
        conn = self.db.connect()
        cursor = conn.cursor()
        
        query = """
            SELECT f.id, f.file_name, f.file_hash
            FROM files f
            JOIN processing_state ps ON f.id = ps.file_id
            WHERE ps.stage0_indexed = 1 AND ps.stage1_meta = 0
            AND f.is_duplicate = 0
            ORDER BY f.id ASC
        """
        
        if limit:
            query += f" LIMIT {limit}"
        
        cursor.execute(query)
        rows = cursor.fetchall()
        
        files = [
            {"id": row[0], "filename": row[1], "hash": row[2]}
            for row in rows
        ]
        
        logger.info(f"Found {len(files)} files pending for Stage 1")
        return files
    
    def process_file(self, file_id: int, filename: str, file_hash: str) -> bool:
        """ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
        
        Args:
            file_id: íŒŒì¼ ID
            filename: íŒŒì¼ëª…
            file_hash: íŒŒì¼ í•´ì‹œ
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            # 1. Geminië¡œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ìµœëŒ€ 3íšŒ ì¬ì‹œë„)
            metadata = None
            for attempt in range(1, 4):
                logger.info(f"   [AI 1/2] Gemini searching (Attempt {attempt}/3): {filename}")
                metadata = self.gemini.extract_metadata_from_filename(filename, file_hash)
                
                # ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì–»ì—ˆëŠ”ì§€ í™•ì¸
                if self._check_metadata_sufficient(metadata):
                    logger.info("   âœ… Gemini search successful (sufficient info found)")
                    logger.info(f"      - Title: {metadata.title}")
                    logger.info(f"      - Author: {metadata.author}")
                    logger.info(f"      - Genre: {metadata.genre}")
                    logger.info(f"      - Rating: {metadata.rating}")
                    logger.info(f"      - Status: {metadata.status}")
                    logger.info(f"      - Tags: {', '.join(metadata.tags) if metadata.tags else '[]'}")
                    if metadata.official_url:
                        logger.info(f"      - Official URL: {metadata.official_url}")
                    break
                else:
                    logger.warning(f"   âš ï¸ Gemini result insufficient (missing author or genre). {'Retrying with variant query...' if attempt < 3 else 'Giving up.'}")
                    if attempt < 3:
                        time.sleep(1)
            
            # 2. Perplexityë¡œ ë³´ì¡° ê²€ìƒ‰ (ìµœëŒ€ 3íšŒ ì¬ì‹œë„)
            extra_info = None
            if self.perplexity.enabled and metadata and metadata.title:
                # ì •ë¦¬ëœ ì œëª© ì‚¬ìš©
                search_title = clean_search_title(metadata.title)
                
                for attempt in range(1, 4):
                    logger.info(f"   [AI 2/2] Perplexity searching (Attempt {attempt}/3): {search_title}")
                    extra_info = self.perplexity.search_novel_info(search_title, metadata.author)
                    
                    # ì œëª© ì™¸ì— ë‹¤ë¥¸ ìœ ì˜ë¯¸í•œ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
                    if extra_info and (extra_info.get("author") or extra_info.get("genre") or extra_info.get("rating")):
                        logger.info("   âœ… Perplexity search successful (info found)")
                        break
                    else:
                        logger.warning(f"   âš ï¸ Perplexity result insufficient. {'Retrying with variant query...' if attempt < 3 else 'Giving up.'}")
                        if attempt < 3:
                            time.sleep(1)
            
            # 3. ë°ì´í„° ë³‘í•© (Merge) - ê³ ë„í™” ë²„ì „
            if extra_info:
                metadata = self._merge_metadata(metadata, extra_info)
            
            # 3.5 êµ¬ê¸€ ì´ë¯¸ì§€ ê²€ìƒ‰ ë³´ê°• (í‘œì§€ê°€ ì—†ê±°ë‚˜ ì €í™”ì§ˆì¼ ê²½ìš°)
            if not metadata.cover_url or "novelpia_books_icon" in metadata.cover_url:
                logger.info(f"   ğŸ” Cover missing or low quality. Trying dedicated Google Image search...")
                # Geminiì˜ Google Search Groundingì„ ë‹¤ì‹œ í™œìš©í•˜ì—¬ ì „ìš© ì´ë¯¸ì§€ ì¿¼ë¦¬ ì‹¤í–‰
                img_prompt = f'"{metadata.title}" {metadata.author or ""} ì†Œì„¤ ê³µì‹ ë‹¨í–‰ë³¸ í‘œì§€ ì´ë¯¸ì§€ ê³ í™”ì§ˆ direct image url format'
                img_metadata = self.gemini.extract_metadata_from_filename(img_prompt, f"img_{file_hash}")
                if img_metadata and img_metadata.cover_url and "novelpia_books_icon" not in img_metadata.cover_url:
                    metadata.cover_url = img_metadata.cover_url
                    logger.info(f"   âœ… Found better cover via Google search: {metadata.cover_url}")
            
            # 4. í‘œì§€ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            cover_path = None
            final_cover_url = metadata.cover_url
            if final_cover_url:
                logger.info(f"   ğŸ–¼ï¸ Downloading cover: {final_cover_url}")
                cover_path = self.perplexity.download_cover(final_cover_url, file_id)

            # 5. ìµœì¢… ë³‘í•© ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            logger.info(f"   [Final Merged Result]")
            logger.info(f"     â€¢ Title: {metadata.title}")
            logger.info(f"     â€¢ Author: {metadata.author}")
            logger.info(f"     â€¢ Genre: {metadata.genre}")
            logger.info(f"     â€¢ Rating: {metadata.rating}")
            logger.info(f"     â€¢ Platform: {metadata.platform}")
            logger.info(f"     â€¢ Updated: {metadata.last_updated}")
            logger.info(f"     â€¢ Tags: {', '.join(metadata.tags) if metadata.tags else '[]'}")
            logger.info(f"     â€¢ Status: {metadata.status}")
            if metadata.official_url:
                logger.info(f"     â€¢ Official URL: {metadata.official_url}")
            logger.info(f"     â€¢ Cover: {'[Success]' if cover_path else '[No/Failed]'}")
            
            # M-46: íŒŒì¼ëª… íŒíŠ¸ ê°•ì œ ë™ê¸°í™” (AIê°€ ë†“ì³¤ì„ ê²½ìš° ëŒ€ë¹„)
            self._apply_filename_hints(metadata, filename)
            
            # 6. DB ì €ì¥
            self._save_to_db(file_id, metadata, extra_info, cover_path)
            
            logger.debug(f"âœ… Processed: {filename}")
            return True
        
        except Exception as e:
            logger.error(f"Failed to process {filename}: {e}")
            self._mark_error(file_id, str(e))
            return False
    
    def _apply_filename_hints(self, metadata: NovelMetadata, filename: str) -> None:
        """íŒŒì¼ëª…ì—ì„œ í™”ìˆ˜ íŒíŠ¸ ë“±ì„ ì¶”ì¶œí•˜ì—¬ ë©”íƒ€ë°ì´í„° ë³´ê°• (M-46)"""
        import re
        # (1~321) ë˜ëŠ” (321) ë“±ì—ì„œ í™”ìˆ˜ ì¶”ì¶œ
        hint_nums = re.findall(r'\((\d+~\d+)\)', filename)
        hint_range = None
        if not hint_nums:
            hint_nums = re.findall(r'\((\d+)\)', filename)
            if hint_nums: hint_range = f"1~{hint_nums[0]}í™”"
        else:
            hint_range = f"{hint_nums[0]}í™”"
            
        if hint_range:
            # ê¸°ì¡´ ì •ë³´ê°€ ë¶€ì‹¤í•˜ê±°ë‚˜ 'Unknown'ì´ë©´ ë®ì–´ì”€
            if not metadata.episode_range or "Unknown" in str(metadata.episode_range):
                metadata.episode_range = hint_range
                logger.info(f"   âœ¨ [Hint Apply] íŒŒì¼ëª…ì—ì„œ í™”ìˆ˜ ì •ë³´ ì¶”ì¶œ: {hint_range}")
        
        # Unknown ë¬¸ìì—´ ì œê±° (ì‘ê°€, ì¥ë¥´ ë“±)
        if metadata.author and "Unknown" in metadata.author: metadata.author = None
        if metadata.genre and "Unknown" in metadata.genre: metadata.genre = None
        if metadata.status and "Unknown" in metadata.status: metadata.status = None

    def _save_to_db(
        self,
        file_id: int,
        metadata: NovelMetadata,
        extra_info: Optional[Dict[str, Any]],
        cover_path: Optional[str] = None
    ) -> None:
        """DBì— ì €ì¥
        
        Args:
            file_id: íŒŒì¼ ID
            metadata: Gemini ë©”íƒ€ë°ì´í„°
            extra_info: Perplexity ì¶”ê°€ ì •ë³´
        """
        conn = self.db.connect()
        cursor = conn.cursor()
        
        # novels í…Œì´ë¸” ì‚½ì…/ì—…ë°ì´íŠ¸
        cursor.execute("""
            INSERT INTO novels (title, author, genre, tags, status, episode_range, rating, cover_path, platform, last_updated, official_url)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            metadata.title,
            metadata.author,
            metadata.genre,
            ", ".join(metadata.tags) if metadata.tags else None,
            metadata.status,
            metadata.episode_range,
            metadata.rating,
            cover_path,
            metadata.platform,
            metadata.last_updated,
            metadata.official_url
        ))
        
        novel_id = cursor.lastrowid
        
        # íŒŒì¼ í…Œì´ë¸”ì— novel_id ì—°ê²°
        cursor.execute("UPDATE files SET novel_id = ? WHERE id = ?", (novel_id, file_id))
        
        # novel_extra í…Œì´ë¸” (Perplexity ì •ë³´)
        if extra_info:
            cursor.execute("""
                INSERT INTO novel_extra (novel_id, source_url)
                VALUES (?, ?)
            """, (
                novel_id,
                extra_info.get("source_url")
            ))
        
        # processing_state ì—…ë°ì´íŠ¸
        cursor.execute("""
            UPDATE processing_state
            SET stage1_meta = 1, last_stage = 'stage1', updated_at = datetime('now','localtime')
            WHERE file_id = ?
        """, (file_id,))
        
        conn.commit()
    
    def _mark_error(self, file_id: int, error_msg: str) -> None:
        """ì—ëŸ¬ ê¸°ë¡
        
        Args:
            file_id: íŒŒì¼ ID
            error_msg: ì—ëŸ¬ ë©”ì‹œì§€
        """
        conn = self.db.connect()
        cursor = conn.cursor()
        
        cursor.execute("""
            UPDATE processing_state
            SET last_error = ?, last_stage = 'stage1', updated_at = datetime('now','localtime')
            WHERE file_id = ?
        """, (error_msg, file_id))
        
        conn.commit()
    
    def run(self, limit: Optional[int] = None, batch_size: int = 10) -> Dict[str, int]:
        """Stage 1 ì‹¤í–‰
        
        Args:
            limit: ì²˜ë¦¬í•  ìµœëŒ€ íŒŒì¼ ìˆ˜
            batch_size: ë°°ì¹˜ í¬ê¸°
        
        Returns:
            {"total": int, "success": int, "failed": int}
        """
        logger.info("=" * 50)
        logger.info("Stage 1: Metadata Collection")
        logger.info("=" * 50)
        
        # ëŒ€ê¸° íŒŒì¼ ì¡°íšŒ
        files = self.get_pending_files(limit)
        
        if not files:
            logger.warning("No files to process")
            return {"total": 0, "success": 0, "failed": 0}
        
        # ì²˜ë¦¬
        success_count = 0
        failed_count = 0
        
        for i, file in enumerate(files):
            logger.info(f"[{i+1}/{len(files)}] {file['filename']}")
            
            if self.process_file(file["id"], file["filename"], file["hash"]):
                success_count += 1
            else:
                failed_count += 1
        
        logger.info("=" * 50)
        logger.info(f"âœ… Stage 1 Complete: {success_count} success, {failed_count} failed")
        logger.info("=" * 50)
        
        return {
            "total": len(files),
            "success": success_count,
            "failed": failed_count
        }
