"""Stage 4: ì±•í„° ë¶„í• 

AI íŒ¨í„´ ë¶„ì„ â†’ ì •ê·œì‹ â†’ ì±•í„° ë¶„í•  â†’ ë³¸í¸/ì™¸ì „ ë¶„ë¥˜
NovelAIze-SSR v3.0 í¬íŒ… + ì±•í„° ì œëª© ë¶„ì„ ì¶”ê°€
"""

import json
import re
from pathlib import Path
from typing import Dict, Any, Optional, List
from novel_total_processor.utils.logger import get_logger
from novel_total_processor.db.schema import Database
from novel_total_processor.config.loader import get_config
from novel_total_processor.ai.gemini_client import GeminiClient
from novel_total_processor.stages.sampler import Sampler
from novel_total_processor.stages.pattern_manager import PatternManager
from novel_total_processor.stages.splitter import Splitter
from novel_total_processor.stages.chapter import Chapter

logger = get_logger(__name__)


class ChapterSplitRunner:
    """Stage 4: ì±•í„° ë¶„í•  ë©”ì¸ ì‹¤í–‰ê¸°"""
    
    def __init__(self, db: Database):
        """
        Args:
            db: Database ì¸ìŠ¤í„´ìŠ¤
        """
        self.db = db
        self.config = get_config()
        self.client = GeminiClient()
        self.sampler = Sampler()
        self.pattern_manager = PatternManager(self.client)
        self.splitter = Splitter()
        
        # ìºì‹œ ë””ë ‰í† ë¦¬
        self.cache_dir = Path("data/cache/chapter_split")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("ChapterSplitRunner initialized")
    
    def get_pending_files(self, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """Stage 4 ëŒ€ê¸° ì¤‘ì¸ íŒŒì¼ ì¡°íšŒ
        
        Args:
            limit: ìµœëŒ€ íŒŒì¼ ìˆ˜
        
        Returns:
            íŒŒì¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        conn = self.db.connect()
        cursor = conn.cursor()
        
        query = """
            SELECT f.id, f.file_path, f.file_name, f.file_hash, f.encoding
            FROM files f
            JOIN processing_state ps ON f.id = ps.file_id
            WHERE ps.stage1_meta = 1 AND ps.stage4_split = 0
            AND f.is_duplicate = 0 AND f.file_ext = '.txt'
        """
        
        if limit:
            query += f" LIMIT {limit}"
        
        cursor.execute(query)
        rows = cursor.fetchall()
        
        files = []
        for row in rows:
            files.append({
                "file_id": row[0],
                "file_path": row[1],
                "file_name": row[2],
                "file_hash": row[3],
                "encoding": row[4]
            })
        
        logger.info(f"Found {len(files)} files pending for Stage 4")
        return files
    
    def split_chapters(self, file_info: Dict[str, Any]) -> Dict[str, Any]:
        """ì±•í„° ë¶„í•  ì‹¤í–‰
        
        Args:
            file_info: íŒŒì¼ ì •ë³´
        
        Returns:
            ë¶„í•  ê²°ê³¼ {"chapters": List[Chapter], "summary": dict}
        """
        file_path = file_info["file_path"]
        file_hash = file_info["file_hash"]
        
        # 1. ìƒ˜í”Œ ì¶”ì¶œ
        logger.info(f"   -> ìƒ˜í”Œ ì¶”ì¶œ ì¤‘... (30ê°œ ê· ë“± ìƒ˜í”Œ)")
        samples = self.sampler.extract_samples(file_path)
        
        # 2. AI íŒ¨í„´ ë¶„ì„
        logger.info(f"   -> AI íŒ¨í„´ ë¶„ì„ ì¤‘...")
        chapter_pattern, subtitle_pattern = self.pattern_manager.find_best_pattern(
            file_path,
            samples
        )
        
        if not chapter_pattern:
            raise ValueError("ì±•í„° íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        logger.info(f"   âœ… ìµœì¢… íŒ¨í„´: {chapter_pattern}")
        if subtitle_pattern:
            logger.info(f"   âœ… ì†Œì œëª© íŒ¨í„´: {subtitle_pattern}")
        
        # 3. ì±•í„° ë¶„í• 
        logger.info(f"   -> ì±•í„° ë¶„í•  ì¤‘...")
        chapters = list(self.splitter.split(file_path, chapter_pattern, subtitle_pattern))
        
        logger.info(f"   âœ… ì´ {len(chapters)}ê°œ ì±•í„° ë¶„í•  ì™„ë£Œ")
        
        # 4. ì±•í„° ì œëª© ë¶„ì„ (ë³¸í¸/ì™¸ì „/ì—í•„ë¡œê·¸ ë¶„ë¥˜)
        summary = self._analyze_chapter_types(chapters)
        
        # 5. ê²°ê³¼ ì €ì¥
        result = {
            "chapters": [
                {
                    "cid": ch.cid,
                    "title": ch.title,
                    "subtitle": ch.subtitle,
                    "length": ch.length,
                    "chapter_type": ch.chapter_type
                }
                for ch in chapters
            ],
            "summary": summary,
            "patterns": {
                "chapter_pattern": chapter_pattern,
                "subtitle_pattern": subtitle_pattern
            }
        }
        
        # ìºì‹œ ì €ì¥
        cache_path = self.cache_dir / f"{file_hash}.json"
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        logger.info(f"   âœ… ìºì‹œ ì €ì¥: {cache_path}")
        
        return result
    
    def _analyze_chapter_types(self, chapters: List[Chapter]) -> Dict[str, Any]:
        """ì±•í„° ì œëª© ë¶„ì„í•˜ì—¬ ë³¸í¸/ì™¸ì „/ì—í•„ë¡œê·¸ ë¶„ë¥˜
        
        Args:
            chapters: ì±•í„° ë¦¬ìŠ¤íŠ¸
        
        Returns:
            {"ë³¸í¸": {"start": 1, "end": 340, "count": 340}, ...}
        """
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
        main_keywords = ["í™”", "chapter", "ì œ"]
        extra_keywords = ["ì™¸ì „", "ë²ˆì™¸", "íŠ¹ë³„í¸", "side story"]
        epilogue_keywords = ["ì—í•„ë¡œê·¸", "epilogue", "í›„ì¼ë‹´", "ì™„ê²°"]
        author_keywords = ["ì‘ê°€ì˜ ë§", "ì‘ê°€ í›„ê¸°", "í›„ê¸°"]
        
        summary = {
            "ë³¸í¸": {"chapters": [], "count": 0},
            "ì™¸ì „": {"chapters": [], "count": 0},
            "ì—í•„ë¡œê·¸": {"chapters": [], "count": 0},
            "ì‘ê°€ì˜ ë§": {"chapters": [], "count": 0},
            "ê¸°íƒ€": {"chapters": [], "count": 0},
            "total": len(chapters)
        }
        
        for ch in chapters:
            title_lower = ch.title.lower()
            
            # ì‘ê°€ì˜ ë§
            if any(kw in title_lower for kw in author_keywords):
                ch.chapter_type = "ì‘ê°€ì˜ ë§"
                summary["ì‘ê°€ì˜ ë§"]["chapters"].append(ch.cid)
            
            # ì—í•„ë¡œê·¸
            elif any(kw in title_lower for kw in epilogue_keywords):
                ch.chapter_type = "ì—í•„ë¡œê·¸"
                summary["ì—í•„ë¡œê·¸"]["chapters"].append(ch.cid)
            
            # ì™¸ì „
            elif any(kw in title_lower for kw in extra_keywords):
                ch.chapter_type = "ì™¸ì „"
                summary["ì™¸ì „"]["chapters"].append(ch.cid)
            
            # ë³¸í¸ (ê¸°ë³¸ê°’)
            else:
                ch.chapter_type = "ë³¸í¸"
                summary["ë³¸í¸"]["chapters"].append(ch.cid)
        
        # ê° íƒ€ì…ë³„ ì‹œì‘/ë í™”ìˆ˜ ê³„ì‚°
        for type_name, info in summary.items():
            if type_name == "total":
                continue
            
            if info["chapters"]:
                info["count"] = len(info["chapters"])
                info["start"] = min(info["chapters"]) + 1  # cidëŠ” 0ë¶€í„°, í™”ìˆ˜ëŠ” 1ë¶€í„°
                info["end"] = max(info["chapters"]) + 1
            else:
                info["start"] = 0
                info["end"] = 0
        
        logger.info(f"   ğŸ“Š ì±•í„° ë¶„ë¥˜:")
        logger.info(f"      ë³¸í¸: {summary['ë³¸í¸']['count']}ê°œ ({summary['ë³¸í¸']['start']}~{summary['ë³¸í¸']['end']}í™”)")
        if summary['ì™¸ì „']['count'] > 0:
            logger.info(f"      ì™¸ì „: {summary['ì™¸ì „']['count']}ê°œ ({summary['ì™¸ì „']['start']}~{summary['ì™¸ì „']['end']}í™”)")
        if summary['ì—í•„ë¡œê·¸']['count'] > 0:
            logger.info(f"      ì—í•„ë¡œê·¸: {summary['ì—í•„ë¡œê·¸']['count']}ê°œ")
        if summary['ì‘ê°€ì˜ ë§']['count'] > 0:
            logger.info(f"      ì‘ê°€ì˜ ë§: {summary['ì‘ê°€ì˜ ë§']['count']}ê°œ")
        
        return summary
    
    def save_to_db(self, file_id: int, result: Dict[str, Any]) -> None:
        """DBì— ì €ì¥
        
        Args:
            file_id: íŒŒì¼ ID
            result: ë¶„í•  ê²°ê³¼
        """
        conn = self.db.connect()
        cursor = conn.cursor()
        
        summary = result["summary"]
        
        # novels í…Œì´ë¸” ì—…ë°ì´íŠ¸ (ì±•í„° ìˆ˜ ì €ì¥)
        cursor.execute("""
            UPDATE novels
            SET chapter_count = ?, updated_at = datetime('now','localtime')
            WHERE id = (SELECT novel_id FROM files WHERE id = ?)
        """, (summary["total"], file_id))
        
        # processing_state ì—…ë°ì´íŠ¸
        cursor.execute("""
            UPDATE processing_state
            SET stage4_split = 1, last_stage = 'stage4', updated_at = datetime('now','localtime')
            WHERE file_id = ?
        """, (file_id,))
        
        conn.commit()
    
    def run(self, limit: Optional[int] = None) -> Dict[str, int]:
        """Stage 4 ì‹¤í–‰
        
        Args:
            limit: ì²˜ë¦¬í•  ìµœëŒ€ íŒŒì¼ ìˆ˜
        
        Returns:
            {"total": int, "success": int, "failed": int}
        """
        logger.info("=" * 50)
        logger.info("Stage 4: Chapter Splitting")
        logger.info("=" * 50)
        
        # ëŒ€ê¸° íŒŒì¼ ì¡°íšŒ
        files = self.get_pending_files(limit)
        
        if not files:
            logger.warning("No files to process")
            return {"total": 0, "success": 0, "failed": 0}
        
        # ì²˜ë¦¬
        success_count = 0
        failed_count = 0
        
        for i, file_info in enumerate(files):
            logger.info(f"[{i+1}/{len(files)}] {file_info['file_name']}")
            
            try:
                result = self.split_chapters(file_info)
                self.save_to_db(file_info["file_id"], result)
                success_count += 1
            except Exception as e:
                logger.error(f"Failed to split chapters: {e}")
                failed_count += 1
        
        logger.info("=" * 50)
        logger.info(f"âœ… Stage 4 Complete: {success_count} success, {failed_count} failed")
        logger.info("=" * 50)
        
        return {
            "total": len(files),
            "success": success_count,
            "failed": failed_count
        }
