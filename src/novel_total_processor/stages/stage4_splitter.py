"""Stage 4: ì±•í„° ë¶„í• 

AI íŒ¨í„´ ë¶„ì„ â†’ ì •ê·œì‹ â†’ ì±•í„° ë¶„í•  â†’ ë³¸í¸/ì™¸ì „ ë¶„ë¥˜
NovelAIze-SSR v3.0 í¬íŒ… + ì±•í„° ì œëª© ë¶„ì„ ì¶”ê°€

NOTE: Pattern recognition and generation uses GeminiClient only.
Perplexity is NOT used for pattern analysis - it's reserved for
Stage 1 metadata search/grounding only.
"""

import json
import re
import os
import tempfile
import traceback
from pathlib import Path
from typing import Dict, Any, Optional, List
from novel_total_processor.utils.logger import get_logger
from novel_total_processor.db.schema import Database
from novel_total_processor.config.loader import get_config
from novel_total_processor.ai.gemini_client import GeminiClient
from novel_total_processor.stages.sampler import Sampler
from novel_total_processor.stages.pattern_manager import PatternManager
from novel_total_processor.stages.splitter import Splitter
from novel_total_processor.stages.chapter import Chapter
from novel_total_processor.stages.stage3_filename import FilenameGenerator
from novel_total_processor.stages.structural_analyzer import StructuralAnalyzer
from novel_total_processor.stages.ai_scorer import AIScorer
from novel_total_processor.stages.global_optimizer import GlobalOptimizer
from novel_total_processor.stages.topic_change_detector import TopicChangeDetector

logger = get_logger(__name__)


class ChapterSplitRunner:
    """Stage 4: ì±•í„° ë¶„í•  ë©”ì¸ ì‹¤í–‰ê¸°"""
    
    # Enhanced recovery constants
    MAX_RETRIES = 5  # Increased from 3 to support more recovery attempts
    TITLE_CANDIDATE_RETRY_THRESHOLD = 2  # Start using title candidates after this many retries
    MAX_GAPS_TO_ANALYZE = 3  # Limit gap analysis to top N gaps for efficiency
    ESTIMATED_AVG_LINE_BYTES = 1000  # Estimated average bytes per line for position calculations
    
    # Quality validation constants
    MIN_VALID_CHAPTER_LENGTH = 100  # Minimum characters for a valid chapter
    MAX_EMPTY_CHAPTER_RATIO = 0.1  # Maximum ratio of empty chapters (10%)
    MIN_AVG_CHAPTER_LENGTH = 500  # Minimum average chapter length in characters
    MIN_DISTANCE_FROM_ANCHOR = 10  # Minimum line distance from anchors when filtering candidates
    
    def __init__(self, db: Database):
        """
        Args:
            db: Database ì¸ìŠ¤í„´ìŠ¤
        """
        self.db = db
        self.config = get_config()
        self.client = GeminiClient()
        self.sampler = Sampler()
        self.pattern_manager = PatternManager(self.client)
        self.splitter = Splitter()
        self.filename_generator = FilenameGenerator(self.db)
        
        # Advanced escalation components
        self.structural_analyzer = StructuralAnalyzer()
        self.ai_scorer = AIScorer(self.client)
        self.global_optimizer = GlobalOptimizer()
        self.topic_detector = TopicChangeDetector(self.client)
        
        # ìºì‹œ ë””ë ‰í† ë¦¬
        self.cache_dir = Path("data/cache/chapter_split")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("ChapterSplitRunner initialized")
    
    def get_pending_files(self, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """Stage 4 ëŒ€ê¸° ì¤‘ì¸ íŒŒì¼ ì¡°íšŒ (M-45: Force/Retry ì§€ì›)"""
        conn = self.db.connect()
        cursor = conn.cursor()
        
        # [M-45 ë³´ê°•] stage1_metaê°€ 1ì¸ë°, stage4_splitì´ 0ì´ê±°ë‚˜, 
        # í˜¹ì€ í™”ìˆ˜ ì •í•©ì„±ì´ ì‹¤íŒ¨í•˜ì—¬ ì¬ì‘ì—…ì´ í•„ìš”í•œ íŒŒì¼ì„ ëª¨ë‘ ê°€ì ¸ì˜´
        query = """
            SELECT f.id, f.file_path, f.file_name, f.file_hash, f.encoding
            FROM files f
            JOIN processing_state ps ON f.id = ps.file_id
            WHERE ps.stage1_meta = 1 
            AND (ps.stage4_split = 0 OR ps.stage4_split = 1) -- í…ŒìŠ¤íŠ¸ ë° ì¬ë¶„ì„ì„ ìœ„í•´ ì™„ë£Œëœ íŒŒì¼ë„ í¬í•¨
            AND f.is_duplicate = 0 AND f.file_ext IN ('.txt', '.epub')
            ORDER BY ps.stage4_split ASC, f.id ASC -- ë¯¸ì™„ë£Œ íŒŒì¼ì„ ìš°ì„ ìˆœìœ„ë¡œ
        """
        
        if limit:
            query += f" LIMIT {limit}"
        
        cursor.execute(query)
        rows = cursor.fetchall()
        
        files = []
        for row in rows:
            files.append({
                "file_id": row[0],
                "file_path": row[1],
                "file_name": row[2],
                "file_hash": row[3],
                "encoding": row[4]
            })
        
        logger.info(f"Found {len(files)} files pending for Stage 4")
        return files
    
    def split_chapters(self, file_info: Dict[str, Any]) -> Dict[str, Any]:
        """ì±•í„° ë¶„í•  ì‹¤í–‰
        
        Args:
            file_info: íŒŒì¼ ì •ë³´
        
        Returns:
            ë¶„í•  ê²°ê³¼ {"chapters": List[Chapter], "summary": dict}
        """
        file_path = file_info["file_path"]
        file_hash = file_info["file_hash"]
        encoding = file_info.get("encoding", "utf-8") or "utf-8"
        
        if file_path.lower().endswith('.epub'):
            # 1. EPUB ë‚´ë¶€ ì±•í„° ë¶„ì„ (Duokan ë“± í‘œì¤€ êµ¬ì¡°)
            logger.info(f"   -> EPUB ë‚´ë¶€ êµ¬ì¡° ì •ë°€ ë¶„ì„ ì¤‘...")
            from ebooklib import epub
            book = epub.read_epub(file_path)
            chapters = []
            
            # Spine ìˆœì„œëŒ€ë¡œ ë³¸ë¬¸ ì•„ì´í…œë§Œ ì¶”ì¶œ
            cid = 1
            content_items = []
            
            # Spineì— ë“±ë¡ëœ ì•„ì´í…œ ID ëª©ë¡
            spine_ids = [s[0] for s in book.spine if isinstance(s, tuple)]
            
            for item_id in spine_ids:
                item = book.get_item_with_id(item_id)
                if not item or item.get_type() != 9: # ITEM_DOCUMENT
                    continue
                
                name = item.get_name().lower()
                # ë¹„ë³¸ë¬¸ ì„¹ì…˜ ì œì™¸ (M-32)
                if any(x in name for x in ['cover', 'nav', 'toc', 'titlepage', 'metadata']):
                    continue
                
                content = item.get_content().decode('utf-8', errors='ignore')
                
                # ë³¸ë¬¸ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì œì™¸ (ì˜ˆ: ë‹¨ìˆœ ì´ë¯¸ì§€ í˜ì´ì§€ë‚˜ ê³µë°±)
                text_only = re.sub(r'<[^>]*>', '', content).strip()
                if len(text_only) < 50 and 'img' not in content.lower():
                    continue
                
                # ì œëª© ì¶”ì¶œ
                title = item.get_name()
                match = re.search(r'<(?:h1|h2|title)[^>]*>(.*?)</(?:h1|h2|title)>', content, re.IGNORECASE | re.DOTALL)
                if match:
                    title = re.sub(r'<[^>]*>', '', match.group(1)).strip()
                
                # ì œëª©ì— ìˆœë²ˆ ë¶€ì—¬ (M-32)
                # ë§Œì•½ ì œëª©ì— ì´ë¯¸ ìˆ«ìê°€ ìˆë‹¤ë©´ ìµœëŒ€í•œ í™œìš©, ì—†ë‹¤ë©´ [cid] ì¶”ê°€
                if not re.search(r'\d+', title):
                    display_title = f"[{cid}] {title}"
                else:
                    display_title = title
                
                chapters.append(Chapter(
                    cid=cid,
                    title=display_title,
                    subtitle="",
                    body=content,
                    length=len(content)
                ))
                cid += 1
            
            chapter_pattern = "EPUB_STRUCTURE"
            subtitle_pattern = None
            
            # EPUB fallback: Check chapter count against expected
            nums = re.findall(r'\d+', file_info["file_name"])
            expected_count = int(nums[-1]) if nums else 0
            
            if expected_count > 0 and len(chapters) != expected_count:
                logger.warning(f"   âš ï¸  EPUB chapter count mismatch ({len(chapters)}/{expected_count})")
                logger.info(f"   ğŸ”„ Attempting text-based fallback for EPUB...")
                
                # Try to extract text from EPUB and use text-based splitting
                try:
                    # Extract full text from EPUB
                    full_text = []
                    for item_id in spine_ids:
                        item = book.get_item_with_id(item_id)
                        if item and item.get_type() == 9:
                            content = item.get_content().decode('utf-8', errors='ignore')
                            text_only = re.sub(r'<[^>]*>', '', content).strip()
                            if text_only:
                                full_text.append(text_only)
                    
                    # Write to temp file for text-based processing
                    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as tmp:
                        tmp_path = tmp.name
                        tmp.write('\n\n'.join(full_text))
                    
                    # Try text-based advanced escalation
                    logger.info(f"   -> Extracted EPUB text to temp file, running text-based splitting...")
                    reconciliation_log = []
                    text_chapters = self._advanced_escalation_pipeline(
                        tmp_path,
                        expected_count,
                        'utf-8',
                        reconciliation_log
                    )
                    
                    # Clean up temp file
                    os.unlink(tmp_path)
                    
                    if text_chapters and len(text_chapters) == expected_count:
                        logger.info(f"   âœ… EPUB text-based fallback SUCCESS: {len(text_chapters)} chapters")
                        chapters = text_chapters
                        chapter_pattern = "EPUB_TEXT_FALLBACK"
                    else:
                        logger.warning(f"   âš ï¸  EPUB text-based fallback partial/failed")
                        logger.info(f"   -> Keeping original EPUB structure ({len(chapters)} chapters)")
                        
                except Exception as e:
                    logger.error(f"   âŒ EPUB text-based fallback error: {e}")
                    logger.info(f"   -> Keeping original EPUB structure ({len(chapters)} chapters)")
            
        else:
            # 1. ìƒ˜í”Œ ì¶”ì¶œ (M-16: Dynamic Encoding ì ìš©)
            logger.info(f"   -> ìƒ˜í”Œ ì¶”ì¶œ ì¤‘... (30ê°œ ê· ë“± ìƒ˜í”Œ, ì¸ì½”ë”©: {encoding})")
            samples = self.sampler.extract_samples(file_path, encoding=encoding)
            
            # 2. AI íŒ¨í„´ ë¶„ì„ (M-28: íŒŒì¼ëª… íŒíŠ¸ í™œìš©)
            logger.info(f"   -> AI íŒ¨í„´ ë¶„ì„ ì¤‘...")
            chapter_pattern, subtitle_pattern = self.pattern_manager.find_best_pattern(
                file_path,
                samples,
                filename=file_info["file_name"],
                encoding=encoding
            )
            
            if not chapter_pattern:
                raise ValueError("ì±•í„° íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # 3. ì±•í„° ë¶„í• 
            logger.info(f"   -> ì±•í„° ë¶„í•  ì¤‘...")
            chapters = list(self.splitter.split(file_path, chapter_pattern, subtitle_pattern, encoding=encoding))
            
            # 3-1. ì •í•©ì„± ê²€ì¦ ë° ìë™ ì¬ë¶„ì„ (M-29/45/49: Zero Tolerance 100% Match)
            # Enhanced with multi-signal recovery: pattern â†’ verify â†’ gaps â†’ title candidates â†’ consensus
            nums = re.findall(r'\d+', file_info["file_name"])
            expected_count = int(nums[-1]) if nums else 0
            
            reconciliation_log = []
            
            # Enhanced recovery loop with multi-signal detection
            # Under/Over detection: triggers detailed analysis for mismatch cases
            retry_count = 0
            title_candidates_used = False
            
            # Fix #4: Track chapter count history for stagnation detection
            chapter_count_history = []
            STAGNATION_THRESHOLD = 3  # Number of attempts with no meaningful change to trigger escalation
            
            # Requirement #2: Track consecutive pattern refinement rejections
            consecutive_rejection_count = 0
            REJECTION_THRESHOLD = 2  # Trigger escalation after 2 consecutive rejections
            
            while expected_count > 0 and len(chapters) != expected_count and retry_count < self.MAX_RETRIES:
                retry_count += 1
                logger.error(f"   âŒ [Mismatch] í™”ìˆ˜ ë¶ˆì¼ì¹˜ ê°ì§€ ({len(chapters)}/{expected_count}). ì¬ì‹œë„({retry_count}/{self.MAX_RETRIES})ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
                
                # Fix #4: Check for stagnation (no meaningful chapter count change for 3 consecutive attempts)
                chapter_count_history.append(len(chapters))
                if self._is_stagnant(chapter_count_history, STAGNATION_THRESHOLD):
                    logger.warning("=" * 60)
                    logger.warning(f"   ğŸš¨ Escalation reason: Stagnation detected")
                    logger.warning(f"      â†’ No meaningful change (+/-2 or less) for {STAGNATION_THRESHOLD} consecutive attempts")
                    logger.warning(f"      â†’ Chapter counts: {chapter_count_history[-STAGNATION_THRESHOLD:]}")
                    logger.warning(f"   ğŸš€ Triggering early escalation to advanced pipeline...")
                    logger.warning("=" * 60)
                    reconciliation_log.append(f"ì •ì²´ ê°ì§€: {STAGNATION_THRESHOLD}íšŒ ì—°ì† ë¯¸ë¯¸í•œ ë³€í™” ({chapter_count_history[-STAGNATION_THRESHOLD:]})")
                    break  # Exit retry loop and proceed to advanced escalation
                
                # ê°€ì´ë“œ íŒíŠ¸ ì¤€ë¹„
                missing = self._find_missing_episodes(chapters, expected_count)
                reconciliation_log.append(f"ì‹œë„ {retry_count}: {len(chapters)}í™” ì¶”ì¶œ (ê¸°ëŒ€ {expected_count})")
                
                # Get current match positions for gap analysis
                matches = self.splitter.find_matches_with_pos(file_path, chapter_pattern, encoding=encoding)
                
                # ë™ì  ê°­ ë¶„ì„ ë° íŒ¨í„´ ë³´ê°• (with rejection tracking)
                refined_pattern, rejection_count = self.pattern_manager.refine_pattern_with_goal_v3(
                    file_path,
                    chapter_pattern,
                    expected_count,
                    encoding=encoding,
                    max_gaps=self.MAX_GAPS_TO_ANALYZE
                )
                
                # Requirement #2: Track consecutive rejections
                if rejection_count > 0:
                    consecutive_rejection_count += rejection_count
                    if consecutive_rejection_count >= REJECTION_THRESHOLD:
                        logger.warning("=" * 60)
                        logger.warning(f"   ğŸš¨ Escalation reason: Consecutive pattern refinement rejections")
                        logger.warning(f"      â†’ {consecutive_rejection_count} consecutive rejections detected")
                        logger.warning(f"   ğŸš€ Triggering immediate escalation to advanced pipeline...")
                        logger.warning("=" * 60)
                        reconciliation_log.append(f"ì—°ì† ê±°ì ˆ: {consecutive_rejection_count}íšŒ íŒ¨í„´ ë³´ê°• ê±°ì ˆ")
                        break  # Exit retry loop and proceed to advanced escalation
                else:
                    consecutive_rejection_count = 0  # Reset on success
                
                if refined_pattern != chapter_pattern:
                    chapter_pattern = refined_pattern
                    logger.info("   -> [Self-Healing] ìˆ˜ì •ëœ íŒ¨í„´ìœ¼ë¡œ ì¬ë¶„í•  ì¤‘...")
                    chapters = list(self.splitter.split(file_path, chapter_pattern, subtitle_pattern, encoding=encoding))
                    
                    # If still missing after pattern refinement, try title candidates (on later retries)
                    if retry_count >= self.TITLE_CANDIDATE_RETRY_THRESHOLD and len(chapters) < expected_count:
                        logger.info("   -> [Fallback] íƒ€ì´í‹€ í›„ë³´ íƒì§€ ì‹œë„ ì¤‘...")
                        missing_count = expected_count - len(chapters)
                        
                        # Find gaps using dynamic detection
                        gaps = self.pattern_manager.find_dynamic_gaps(file_path, matches, expected_count)
                        
                        # Extract title candidates from top gaps (limited by MAX_GAPS_TO_ANALYZE)
                        all_candidates = []
                        for gap in gaps[:self.MAX_GAPS_TO_ANALYZE]:
                            sample = self.sampler.extract_samples_from(
                                file_path, gap['start'], length=30000, encoding=encoding
                            )
                            if sample:
                                candidates = self.pattern_manager.extract_title_candidates(
                                    sample, chapter_pattern
                                )
                                all_candidates.extend(candidates)
                        
                        if all_candidates:
                            # Try splitting with explicit title candidates
                            logger.info(f"   -> [Consensus] {len(all_candidates)} íƒ€ì´í‹€ í›„ë³´ë¡œ ì¬ë¶„í•  ì‹œë„...")
                            chapters = list(self.splitter.split(
                                file_path, chapter_pattern, subtitle_pattern, 
                                encoding=encoding, title_candidates=all_candidates
                            ))
                            title_candidates_used = True
                            reconciliation_log.append(f"íƒ€ì´í‹€ í›„ë³´ {len(all_candidates)}ê°œ ì‚¬ìš©")
                else:
                    logger.warning("   -> íŒ¨í„´ ë³´ê°•ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì‹œë„ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
            
            # [Stage 4 Advanced Escalation] - Activate if pattern-based methods failed
            if expected_count > 0 and len(chapters) != expected_count:
                logger.warning("=" * 60)
                logger.warning(f"   ğŸš¨ Pattern-based methods exhausted ({len(chapters)}/{expected_count})")
                logger.warning("=" * 60)
                
                # Step 1: Try Level 3 AI direct search first (faster and more accurate than Advanced Pipeline)
                logger.info(f"   ğŸš€ Step 1: Attempting Level 3 AI direct title search...")
                
                try:
                    # Get current matches for context
                    existing_matches = self.splitter.find_matches_with_pos(file_path, chapter_pattern, encoding=encoding)
                    
                    # Call Level 3 direct search
                    found_titles = self.pattern_manager.direct_ai_title_search(
                        file_path, chapter_pattern, expected_count, existing_matches, encoding
                    )
                    
                    if found_titles and len(found_titles) >= expected_count * 0.5:
                        logger.info(f"   âœ¨ [Level 3] Found {len(found_titles)} titles via AI direct search")
                        
                        # Build pattern from found titles (reverse extraction)
                        reverse_pattern = self.pattern_manager._build_pattern_from_examples(found_titles)
                        
                        if reverse_pattern:
                            # Combine with existing pattern
                            combined_pattern = f"{chapter_pattern}|{reverse_pattern}"
                            logger.info(f"   ğŸ”§ Testing combined pattern with reverse-extracted regex...")
                            
                            # Try splitting with combined pattern
                            level3_chapters = list(self.splitter.split(
                                file_path, combined_pattern, subtitle_pattern, encoding=encoding
                            ))
                            
                            # Check if Level 3 succeeded
                            if len(level3_chapters) == expected_count:
                                logger.info(f"   âœ… [Level 3 SUCCESS] Exact match: {len(level3_chapters)} chapters")
                                chapters = level3_chapters
                                chapter_pattern = combined_pattern
                                reconciliation_log.append(f"Level 3 AI ì§ì ‘ íƒìƒ‰ ì„±ê³µ: {len(chapters)}í™”")
                            elif abs(len(level3_chapters) - expected_count) < abs(len(chapters) - expected_count):
                                logger.info(f"   âœ¨ [Level 3 Improved] Better result: {len(chapters)} -> {len(level3_chapters)}")
                                chapters = level3_chapters
                                chapter_pattern = combined_pattern
                                reconciliation_log.append(f"Level 3 ê°œì„ : {len(chapters)}í™”")
                            else:
                                logger.info(f"   â„¹ï¸  [Level 3] No improvement ({len(level3_chapters)} vs {len(chapters)})")
                        else:
                            logger.warning(f"   âš ï¸  [Level 3] Failed to build reverse pattern")
                    else:
                        logger.info(f"   â„¹ï¸  [Level 3] Insufficient titles found ({len(found_titles) if found_titles else 0})")
                        
                except Exception as e:
                    logger.error(f"   âŒ [Level 3] Error during direct search: {e}")
                    import traceback
                    logger.debug(traceback.format_exc())
                
                # Step 2: If Level 3 didn't achieve exact match, try Advanced Pipeline as fallback
                if len(chapters) != expected_count:
                    logger.warning(f"   ğŸš€ Step 2: Activating Advanced Escalation Pipeline (fallback)...")
                    logger.warning("=" * 60)
                    
                    # Convert pattern-based matches to anchor boundaries
                    anchor_boundaries = None
                    if existing_matches:
                        logger.info(f"   ğŸ”§ Converting {len(existing_matches)} pattern matches to anchor boundaries...")
                        anchor_boundaries = []
                        
                        for match in existing_matches:
                            # Convert pos to line_num
                            line_num = self._pos_to_line_num(file_path, match['pos'], encoding)
                            anchor_boundaries.append({
                                'line_num': line_num,
                                'text': match['title'],
                                'confidence': 1.0,  # Pattern matches have high confidence
                                'byte_pos': match['pos']
                            })
                        
                        logger.info(f"   âœ… Created {len(anchor_boundaries)} anchor boundaries from pattern matches")
                    
                    # Try advanced escalation with anchors
                    advanced_chapters = self._advanced_escalation_pipeline(
                        file_path,
                        expected_count,
                        encoding,
                        reconciliation_log,
                        anchor_boundaries=anchor_boundaries
                    )
                    
                    if advanced_chapters and len(advanced_chapters) == expected_count:
                        logger.info(f"   âœ… Advanced escalation SUCCESS: {len(advanced_chapters)} chapters")
                        chapters = advanced_chapters
                        reconciliation_log.append(f"Advanced escalation ì„±ê³µ: {len(chapters)}í™” ì¶”ì¶œ")
                    elif advanced_chapters:
                        logger.warning(f"   âš ï¸  Advanced escalation partial: {len(advanced_chapters)}/{expected_count}")
                        # Use if closer to target than current
                        if abs(len(advanced_chapters) - expected_count) < abs(len(chapters) - expected_count):
                            logger.info("   -> ë¶€ë¶„ ì„±ê³µì´ì§€ë§Œ ê¸°ì¡´ë³´ë‹¤ ë‚˜ìŒ. ì ìš©í•©ë‹ˆë‹¤.")
                            chapters = advanced_chapters
                            reconciliation_log.append(f"Advanced escalation ë¶€ë¶„ ì„±ê³µ: {len(chapters)}í™”")
                    else:
                        logger.error("   âŒ Advanced escalation failed")
                        reconciliation_log.append("Advanced escalation ì‹¤íŒ¨")
            
            # ìµœì¢… ì •í•©ì„± ë¡œê·¸ ê¸°ë¡
            if expected_count > 0 and len(chapters) != expected_count:
                reason = f"ìµœì¢… í™”ìˆ˜ ë¶ˆì¼ì¹˜: ë³´ìœ  {len(chapters)} / ì›¹(ë˜ëŠ” íŒíŠ¸) {expected_count}"
                logger.error(f"   -> [Strict Match Fail] {reason}")
                reconciliation_log.append(reason)
                # ëˆ„ë½ëœ íšŒì°¨ ì •ë³´ ì¶”ê°€
                missing = self._find_missing_episodes(chapters, expected_count)
                if missing:
                    reconciliation_log.append(f"ëˆ„ë½ ì˜ì‹¬: {', '.join(map(str, missing[:10]))} ë“±")
                
                # Log recovery methods used
                if title_candidates_used:
                    reconciliation_log.append("ë³µêµ¬ ë°©ë²•: íŒ¨í„´ + íƒ€ì´í‹€ í›„ë³´ (consensus)")
            elif expected_count > 0:
                logger.info(f"   âœ… í™”ìˆ˜ 100% ì¼ì¹˜ í™•ì¸: {len(chapters)}í™” (Perfect Match)")
                reconciliation_log.append(f"ì •í•©ì„± 100% ì¼ì¹˜ ({len(chapters)}í™”)")
                if title_candidates_used:
                    reconciliation_log.append("ë³µêµ¬ ë°©ë²•: íƒ€ì´í‹€ í›„ë³´ (consensus) ì‚¬ìš©ë¨")
            
            file_info["reconciliation_log"] = "\n".join(reconciliation_log)
            self._verify_chapter_count(file_info["file_name"], len(chapters), chapters)
        
        logger.info(f"   âœ… ì´ {len(chapters)}ê°œ ì±•í„° í™•ì¸ ì™„ë£Œ")
        
        # 4. ì±•í„° ì œëª© ë¶„ì„ (ë³¸í¸/ì™¸ì „/ì—í•„ë¡œê·¸ ë¶„ë¥˜)
        summary = self._analyze_chapter_types(chapters)
        
        # 5. ê²°ê³¼ ì €ì¥
        result = {
            "chapters": [
                {
                    "cid": ch.cid,
                    "title": ch.title,
                    "subtitle": ch.subtitle,
                    "body": ch.body,  # Save full chapter body for Stage 5
                    "length": ch.length,
                    "chapter_type": ch.chapter_type
                }
                for ch in chapters
            ],
            "summary": summary,
            "patterns": {
                "chapter_pattern": chapter_pattern,
                "subtitle_pattern": subtitle_pattern
            },
            "reconciliation_log": file_info.get("reconciliation_log", "")
        }
        
        # ìºì‹œ ì €ì¥
        cache_path = self.cache_dir / f"{file_hash}.json"
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        logger.info(f"   âœ… ìºì‹œ ì €ì¥: {cache_path}")
        
        return result
    
    def _advanced_escalation_pipeline(
        self,
        file_path: str,
        expected_count: int,
        encoding: str,
        reconciliation_log: List[str],
        anchor_boundaries: Optional[List[Dict[str, Any]]] = None
    ) -> Optional[List[Chapter]]:
        """Advanced Stage 4 escalation pipeline with AI-scored candidates and global optimization
        
        Pipeline stages:
        1. Structural analysis: Generate transition point candidates
        2. AI scoring: Score each candidate for likelihood
        3. Topic change detection: Add semantic boundaries as fallback
        4. Global optimization: Select exactly expected_count boundaries
        5. Split using selected boundaries
        
        Args:
            file_path: Path to the novel file
            expected_count: Expected number of chapters
            encoding: File encoding
            reconciliation_log: Log list to append messages
            
        Returns:
            List of Chapter objects or None if failed
        """
        try:
            # Fix #5: Enhanced logging for pipeline execution
            logger.info("=" * 70)
            logger.info("   ğŸš€ ADVANCED ESCALATION PIPELINE ACTIVATED")
            logger.info("=" * 70)
            
            # Log anchor information if present
            if anchor_boundaries:
                logger.info(f"   ğŸ”’ Using {len(anchor_boundaries)} anchor boundaries from pattern matching")
                reconciliation_log.append(f"Anchors: {len(anchor_boundaries)} pattern matching results fixed")
            
            # Stage 1: Generate structural candidates
            logger.info("   ğŸ“Š [Pipeline Stage 1/5] Structural transition point analysis...")
            logger.info(f"      â†’ Analyzing file structure for chapter boundaries")
            candidates = self.structural_analyzer.generate_candidates(
                file_path,
                encoding=encoding,
                max_candidates=expected_count * 5  # Generate 5x for good coverage
            )
            
            if not candidates:
                logger.error("   âŒ [Stage 1 Failed] No structural candidates found")
                return None
            
            logger.info(f"   âœ… [Stage 1 Complete] Generated {len(candidates)} structural candidates")
            reconciliation_log.append(f"êµ¬ì¡° ë¶„ì„: {len(candidates)} í›„ë³´ ìƒì„±")
            
            # Filter out candidates near anchors to reduce AI scoring load
            if anchor_boundaries and len(candidates) > 200:
                logger.info(f"   ğŸ”§ Filtering candidates near anchors to reduce AI scoring load...")
                filtered_candidates = []
                
                for cand in candidates:
                    is_near_anchor = False
                    for anchor in anchor_boundaries:
                        if abs(cand['line_num'] - anchor['line_num']) < self.MIN_DISTANCE_FROM_ANCHOR:
                            is_near_anchor = True
                            break
                    if not is_near_anchor:
                        filtered_candidates.append(cand)
                
                logger.info(f"   ğŸ“Š Filtered from {len(candidates)} to {len(filtered_candidates)} candidates")
                candidates = filtered_candidates
            
            # Stage 2: AI scoring (optional, can be expensive for large candidate sets)
            # Only score if we have a reasonable number of candidates
            if len(candidates) <= 200:  # Limit to prevent excessive API calls
                logger.info("   ğŸ¤– [Pipeline Stage 2/5] AI likelihood scoring...")
                logger.info(f"      â†’ Scoring {len(candidates)} candidates with AI (batch_size=10)")
                candidates = self.ai_scorer.score_candidates(
                    file_path,
                    candidates,
                    encoding=encoding,
                    batch_size=10
                )
                logger.info("   âœ… [Stage 2 Complete] AI scoring complete")
                reconciliation_log.append("AI ìŠ¤ì½”ì–´ë§ ì™„ë£Œ")
            else:
                logger.warning(f"   âš ï¸  [Stage 2 Skipped] Too many candidates ({len(candidates)}), skipping AI scoring")
                reconciliation_log.append(f"AI ìŠ¤ì½”ì–´ë§ ìŠ¤í‚µ (í›„ë³´ ìˆ˜ ê³¼ë‹¤: {len(candidates)})")
            
            # Stage 3: Topic change detection (if we still need more coverage)
            logger.info("   ğŸ” [Pipeline Stage 3/5] Topic change detection...")
            if len(candidates) < expected_count * 2:
                logger.info(f"      â†’ Detecting semantic boundaries (need more coverage)")
                topic_candidates = self.topic_detector.detect_topic_boundaries(
                    file_path,
                    expected_count,
                    existing_candidates=candidates,
                    encoding=encoding
                )
                
                if topic_candidates:
                    logger.info(f"   âœ… [Stage 3 Complete] Added {len(topic_candidates)} topic-change candidates")
                    candidates.extend(topic_candidates)
                    reconciliation_log.append(f"í† í”½ ë³€í™” ê°ì§€: {len(topic_candidates)} í›„ë³´ ì¶”ê°€")
                else:
                    logger.info("   â„¹ï¸  [Stage 3 Complete] No topic-change candidates found")
            else:
                logger.info(f"   âœ… [Stage 3 Skipped] Sufficient candidates ({len(candidates)} >= {expected_count * 2})")
            
            # Stage 4: Global optimization
            logger.info("   ğŸ¯ [Pipeline Stage 4/5] Global optimization...")
            logger.info(f"      â†’ Selecting optimal {expected_count} boundaries from {len(candidates)} candidates")
            selected = self.global_optimizer.select_optimal_boundaries(
                candidates,
                expected_count,
                file_path,
                encoding=encoding,
                anchor_boundaries=anchor_boundaries
            )
            
            if not selected:
                logger.error("   âŒ [Stage 4 Failed] Optimization failed to select boundaries")
                return None
            
            if len(selected) != expected_count:
                logger.warning(f"   âš ï¸  [Stage 4 Partial] Optimizer returned {len(selected)}/{expected_count} boundaries")
            else:
                logger.info(f"   âœ… [Stage 4 Complete] Selected exactly {len(selected)} optimal boundaries")
            
            reconciliation_log.append(f"ìµœì í™”: {len(selected)}ê°œ ê²½ê³„ ì„ íƒ")
            
            # Stage 5: Split using selected boundaries directly (bypass regex patterns)
            logger.info("   ğŸ“ [Pipeline Stage 5/5] Splitting chapters using selected boundaries...")
            logger.info(f"      â†’ Boundary count: {len(selected)} (expected: {expected_count})")
            logger.info(f"      â†’ Boundary format: line_num={selected[0]['line_num']}, text='{selected[0]['text'][:20]}...'")
            
            # Validate boundaries before splitting
            if len(selected) != expected_count:
                logger.error(f"   âŒ [Stage 5 Failed] Boundary count mismatch: got {len(selected)}, expected {expected_count}")
                return None
            
            # Validate all boundaries have required fields
            for i, boundary in enumerate(selected):
                if not boundary.get('text', '').strip():
                    logger.error(f"   âŒ [Stage 5 Failed] Boundary {i} has empty text at line {boundary.get('line_num', '?')}")
                    return None
                if 'line_num' not in boundary:
                    logger.error(f"   âŒ [Stage 5 Failed] Boundary {i} missing line_num field")
                    return None
            
            # Use boundary-based split (bypasses regex pattern matching)
            try:
                chapters = list(self.splitter.split_by_boundaries(
                    file_path,
                    selected,
                    encoding=encoding
                ))
            except ValueError as e:
                logger.error(f"   âŒ [Stage 5 Failed] Boundary validation error: {e}")
                return None
            
            # Report creation results
            if len(chapters) == 0:
                logger.error(f"   âŒ [Stage 5 Failed] Created 0 chapters from {len(selected)} boundaries!")
                return None
            elif len(chapters) != len(selected):
                logger.warning(f"   âš ï¸  [Stage 5 Partial] Created {len(chapters)}/{len(selected)} chapters")
            else:
                logger.info(f"   âœ… [Stage 5 Complete] Created {len(chapters)} chapters from {len(selected)} boundaries")
            
            # Quality validation: check for too many empty chapters
            if chapters:
                empty_count = sum(1 for ch in chapters if ch.length < self.MIN_VALID_CHAPTER_LENGTH)
                empty_ratio = empty_count / len(chapters)
                if empty_ratio > self.MAX_EMPTY_CHAPTER_RATIO:
                    logger.error(f"   âŒ Quality check FAILED: {empty_count}/{len(chapters)} chapters <{self.MIN_VALID_CHAPTER_LENGTH} chars ({empty_ratio*100:.0f}%)")
                    logger.error(f"   ğŸš« Advanced pipeline rejected due to too many empty chapters")
                    return None
                
                avg_length = sum(ch.length for ch in chapters) / len(chapters)
                if avg_length < self.MIN_AVG_CHAPTER_LENGTH:
                    logger.error(f"   âŒ Quality check FAILED: avg chapter length = {avg_length:.0f} chars")
                    logger.error(f"   ğŸš« Advanced pipeline rejected due to low average chapter length")
                    return None
                
                logger.info(f"   âœ… Quality check PASSED: avg length = {avg_length:.0f} chars, empty ratio = {empty_ratio*100:.1f}%")
            
            logger.info("=" * 70)
            logger.info(f"   ğŸ‰ ADVANCED PIPELINE COMPLETE: {len(chapters)} chapters extracted")
            logger.info("=" * 70)
            
            return chapters
            
        except Exception as e:
            logger.error("=" * 70)
            logger.error(f"   âŒ ADVANCED ESCALATION PIPELINE FAILED: {e}")
            logger.error("=" * 70)
            traceback.print_exc()
            return None
    
    def _pos_to_line_num(self, file_path: str, pos: int, encoding: str = 'utf-8') -> int:
        """Convert byte position to line number
        
        Args:
            file_path: Path to the file
            pos: Byte position
            encoding: File encoding
            
        Returns:
            Line number (0-indexed) corresponding to the byte position
        """
        try:
            with open(file_path, 'r', encoding=encoding, errors='replace') as f:
                lines = f.readlines()
            
            current_pos = 0
            for i, line in enumerate(lines):
                line_bytes = len(line.encode(encoding, errors='replace'))
                if current_pos + line_bytes > pos:
                    return i
                current_pos += line_bytes
            
            # If position is beyond file, return last line
            return len(lines) - 1 if lines else 0
            
        except Exception as e:
            logger.warning(f"Could not convert pos to line_num: {e}")
            # Fallback: estimate line number based on average bytes per line
            return pos // self.ESTIMATED_AVG_LINE_BYTES
    
    def _analyze_chapter_types(self, chapters: List[Chapter]) -> Dict[str, Any]:
        """ì±•í„° ì œëª© ë¶„ì„í•˜ì—¬ ë³¸í¸/ì™¸ì „/ì—í•„ë¡œê·¸ ë¶„ë¥˜
        
        Args:
            chapters: ì±•í„° ë¦¬ìŠ¤íŠ¸
        
        Returns:
            {"ë³¸í¸": {"start": 1, "end": 340, "count": 340}, ...}
        """
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
        main_keywords = ["í™”", "chapter", "ì œ", "ep"]
        extra_keywords = ["ì™¸ì „", "ë²ˆì™¸", "íŠ¹ë³„í¸", "side story"]
        # "ì™„ê²°"ì€ ë³¸í¸ ë§ˆì§€ë§‰í™”ì— ìì£¼ ë¶™ìœ¼ë¯€ë¡œ ì—í•„ë¡œê·¸ í‚¤ì›Œë“œì—ì„œ ì œì™¸ (ë‹¨ë… ì‚¬ìš© ì‹œ ì—í•„ë¡œê·¸ ì·¨ê¸‰ ê³ ë ¤)
        epilogue_keywords = ["ì—í•„ë¡œê·¸", "epilogue", "í›„ì¼ë‹´"]
        author_keywords = ["ì‘ê°€ì˜ ë§", "ì‘ê°€ í›„ê¸°", "í›„ê¸°"]
        
        summary = {
            "ë³¸í¸": {"chapters": [], "count": 0},
            "ì™¸ì „": {"chapters": [], "count": 0},
            "ì—í•„ë¡œê·¸": {"chapters": [], "count": 0},
            "ì‘ê°€ì˜ ë§": {"chapters": [], "count": 0},
            "ê¸°íƒ€": {"chapters": [], "count": 0},
            "total": len(chapters)
        }
        
        for ch in chapters:
            title_lower = ch.title.lower()
            
            # ì‘ê°€ì˜ ë§
            if any(kw in title_lower for kw in author_keywords):
                ch.chapter_type = "ì‘ê°€ì˜ ë§"
                summary["ì‘ê°€ì˜ ë§"]["chapters"].append(ch.cid)
            
            # ì—í•„ë¡œê·¸
            elif any(kw in title_lower for kw in epilogue_keywords):
                ch.chapter_type = "ì—í•„ë¡œê·¸"
                summary["ì—í•„ë¡œê·¸"]["chapters"].append(ch.cid)
            
            # ì™¸ì „
            elif any(kw in title_lower for kw in extra_keywords):
                ch.chapter_type = "ì™¸ì „"
                summary["ì™¸ì „"]["chapters"].append(ch.cid)
            
            # ë³¸í¸ (ê¸°ë³¸ê°’)
            else:
                ch.chapter_type = "ë³¸í¸"
                summary["ë³¸í¸"]["chapters"].append(ch.cid)
        
        # ê° íƒ€ì…ë³„ ì‹œì‘/ë í™”ìˆ˜ ê³„ì‚°
        for type_name, info in summary.items():
            if type_name == "total":
                continue
            
            if info["chapters"]:
                info["count"] = len(info["chapters"])
                info["start"] = min(info["chapters"]) + 1  # cidëŠ” 0ë¶€í„°, í™”ìˆ˜ëŠ” 1ë¶€í„°
                info["end"] = max(info["chapters"]) + 1
            else:
                info["start"] = 0
                info["end"] = 0
        
        logger.info(f"   ğŸ“Š ì±•í„° ë¶„ë¥˜:")
        logger.info(f"      ë³¸í¸: {summary['ë³¸í¸']['count']}ê°œ ({summary['ë³¸í¸']['start']}~{summary['ë³¸í¸']['end']}í™”)")
        if summary['ì™¸ì „']['count'] > 0:
            logger.info(f"      ì™¸ì „: {summary['ì™¸ì „']['count']}ê°œ ({summary['ì™¸ì „']['start']}~{summary['ì™¸ì „']['end']}í™”)")
        if summary['ì—í•„ë¡œê·¸']['count'] > 0:
            logger.info(f"      ì—í•„ë¡œê·¸: {summary['ì—í•„ë¡œê·¸']['count']}ê°œ")
        if summary['ì‘ê°€ì˜ ë§']['count'] > 0:
            logger.info(f"      ì‘ê°€ì˜ ë§: {summary['ì‘ê°€ì˜ ë§']['count']}ê°œ")
        
        return summary
    
    def _verify_chapter_count(self, filename: str, actual_count: int, chapters: List[Chapter]) -> None:
        """íŒŒì¼ëª…ì˜ í™”ìˆ˜ íŒíŠ¸ì™€ ì‹¤ì œ ë¶„í• ëœ ì±•í„° ìˆ˜ ë¹„êµ ê²€ì¦ (M-28/45/48)"""
        nums = re.findall(r'\d+', filename)
        if not nums:
            return
        
        expected_count = int(nums[-1])
        if expected_count > 0:
            diff = actual_count - expected_count
            if diff < 0:
                logger.error("=" * 60)
                logger.error(f"âŒ [ì •í•©ì„± ì‹¤íŒ¨] í™”ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤! ({actual_count}/{expected_count})")
                missing = self._find_missing_episodes(chapters, expected_count)
                if missing:
                    logger.error(f"   - ëˆ„ë½ëœ íšŒì°¨ ì˜ˆìƒ: {missing[:20]}{'...' if len(missing)>20 else ''}")
                logger.error("=" * 60)
            elif diff > 0:
                logger.warning("=" * 60)
                logger.warning(f"âš ï¸  [ì •í•©ì„± ê²½ê³ ] í™”ìˆ˜ê°€ ê¸°ëŒ€ì¹˜ë³´ë‹¤ ë§ìŠµë‹ˆë‹¤. ({actual_count}/{expected_count})")
                logger.warning("   - ì¤‘ë³µ ë§¤ì¹­ì´ë‚˜ ì™¸ì „ì´ í¬í•¨ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                logger.warning("=" * 60)
            else:
                logger.info(f"   âœ… í™”ìˆ˜ 100% ì¼ì¹˜ í™•ì¸: {actual_count}í™” (Perfect Match)")

    def _find_missing_episodes(self, chapters: List[Chapter], expected_count: int) -> List[int]:
        """ì¶”ì¶œëœ ì±•í„°ë“¤ ì‚¬ì´ì—ì„œ ë¹ ì§„ ë²ˆí˜¸ íƒì§€ (M-48)"""
        found_nums = set()
        for ch in chapters:
            # ì œëª©ì—ì„œ ì²« ë²ˆì§¸ ìˆ«ì ì¶”ì¶œ
            match = re.search(r'(\d+)', ch.title)
            if match:
                found_nums.add(int(match.group(1)))
        
        missing = []
        for i in range(1, expected_count + 1):
            if i not in found_nums:
                missing.append(i)
        return missing
    
    def _is_stagnant(self, chapter_count_history: List[int], threshold: int = 3) -> bool:
        """Check if chapter count has stagnated (no meaningful change for N consecutive attempts)
        
        Treats +/-1 or +/-2 fluctuations as stagnant to reliably trigger escalation.
        
        Args:
            chapter_count_history: List of chapter counts from retry attempts
            threshold: Number of consecutive attempts with no meaningful change to consider stagnant
            
        Returns:
            True if stagnated, False otherwise
        """
        if len(chapter_count_history) < threshold:
            return False
        
        recent_counts = chapter_count_history[-threshold:]
        # Check if all counts are within +/-2 of each other (treat as stagnant)
        min_count = min(recent_counts)
        max_count = max(recent_counts)
        return (max_count - min_count) <= 2  # Fluctuations of +/-1 or +/-2 are stagnant

    def save_to_db(self, file_id: int, result: Dict[str, Any]) -> None:
        """DBì— ì €ì¥
        
        Args:
            file_id: íŒŒì¼ ID
            result: ë¶„í•  ê²°ê³¼
        """
        conn = self.db.connect()
        cursor = conn.cursor()
        
        summary = result["summary"]
        
        # novels í…Œì´ë¸” ì—…ë°ì´íŠ¸ (ì±•í„° ìˆ˜ ë° ì •í•©ì„± ë¡œê·¸ ì €ì¥)
        reconcile_log = result.get("reconciliation_log", "")
        cursor.execute("""
            UPDATE novels
            SET chapter_count = ?, reconciliation_log = ?
            WHERE id = (SELECT novel_id FROM files WHERE id = ?)
        """, (summary["total"], reconcile_log, file_id))
        
        # processing_state ì—…ë°ì´íŠ¸ (ì •í•©ì„± ë¡œê·¸ í¬í•¨)
        cursor.execute("""
            UPDATE processing_state
            SET stage4_split = 1, last_stage = 'stage4', reconciliation_log = ?
            WHERE file_id = ?
        """, (reconcile_log, file_id))
        
        conn.commit()
        
        # [M-49] ë¶„ì„ ì™„ë£Œ í›„ ì‹¤ë¬¼ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ íŒŒì¼ëª… ìµœì¢… ë™ê¸°í™” (Sync Original TXT)
        try:
            logger.info(f"   -> [Sync] ì‹¤ë¬¼ ê¸°ë°˜ íŒŒì¼ëª… ìµœì¢… ë™ê¸°í™” ì‹œë„ ì¤‘... (File ID: {file_id})")
            self.filename_generator.process_single_file(file_id)
        except Exception as e:
            logger.error(f"   âŒ [Sync Fail] íŒŒì¼ëª… ë™ê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def run(self, limit: Optional[int] = None) -> Dict[str, int]:
        """Stage 4 ì‹¤í–‰
        
        Args:
            limit: ì²˜ë¦¬í•  ìµœëŒ€ íŒŒì¼ ìˆ˜
        
        Returns:
            {"total": int, "success": int, "failed": int}
        """
        logger.info("=" * 50)
        logger.info("Stage 4: Chapter Splitting")
        logger.info("=" * 50)
        
        # ëŒ€ê¸° íŒŒì¼ ì¡°íšŒ
        files = self.get_pending_files(limit)
        
        if not files:
            logger.warning("No files to process")
            return {"total": 0, "success": 0, "failed": 0}
        
        # ì²˜ë¦¬
        success_count = 0
        failed_count = 0
        
        for i, file_info in enumerate(files):
            file_path_obj = Path(file_info['file_path'])
            logger.info(f"[{i+1}/{len(files)}] {file_path_obj.name}")
            
            if not file_path_obj.exists():
                logger.warning(f"   âš ï¸  íŒŒì¼ì´ ë””ìŠ¤í¬ì— ì—†ìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤: {file_path_obj}")
                failed_count += 1 # Treat as failed since it couldn't be processed
                continue
                
            try:
                result = self.split_chapters(file_info)
                self.save_to_db(file_info["file_id"], result)
                success_count += 1
            except Exception as e:
                logger.error(f"Failed to split chapters for {file_path_obj.name}: {e}")
                # [Hotfix v2] ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ì˜¤ì—¼ëœ ìºì‹œê°€ ìˆë‹¤ë©´ ì‚­ì œ (Stage 5 ì˜¤ì—¼ ë°©ì§€)
                cache_path = self.cache_dir / f"{file_info['file_hash']}.json"
                if cache_path.exists():
                    try:
                        cache_path.unlink()
                        logger.info(f"   ğŸ—‘ï¸  ì‹¤íŒ¨í•œ íŒŒì¼ì˜ ê¸°ì¡´ ìºì‹œë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
                    except: pass
                
                failed_count += 1
        
        logger.info("=" * 50)
        logger.info(f"âœ… Stage 4 Complete: {success_count} success, {failed_count} failed")
        logger.info("=" * 50)
        
        return {
            "total": len(files),
            "success": success_count,
            "failed": failed_count
        }
