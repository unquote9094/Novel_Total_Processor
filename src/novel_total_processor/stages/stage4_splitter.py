"""Stage 4: ì±•í„° ë¶„í• 

AI íŒ¨í„´ ë¶„ì„ â†’ ì •ê·œì‹ â†’ ì±•í„° ë¶„í•  â†’ ë³¸í¸/ì™¸ì „ ë¶„ë¥˜
NovelAIze-SSR v3.0 í¬íŒ… + ì±•í„° ì œëª© ë¶„ì„ ì¶”ê°€
"""

import json
import re
from pathlib import Path
from typing import Dict, Any, Optional, List
from novel_total_processor.utils.logger import get_logger
from novel_total_processor.db.schema import Database
from novel_total_processor.config.loader import get_config
from novel_total_processor.ai.gemini_client import GeminiClient
from novel_total_processor.stages.sampler import Sampler
from novel_total_processor.stages.pattern_manager import PatternManager
from novel_total_processor.stages.splitter import Splitter
from novel_total_processor.stages.chapter import Chapter
from novel_total_processor.stages.stage3_filename import FilenameGenerator

logger = get_logger(__name__)


class ChapterSplitRunner:
    """Stage 4: ì±•í„° ë¶„í•  ë©”ì¸ ì‹¤í–‰ê¸°"""
    
    # Enhanced recovery constants
    MAX_RETRIES = 5  # Increased from 3 to support more recovery attempts
    TITLE_CANDIDATE_RETRY_THRESHOLD = 2  # Start using title candidates after this many retries
    MAX_GAPS_TO_ANALYZE = 3  # Limit gap analysis to top N gaps for efficiency
    
    def __init__(self, db: Database):
        """
        Args:
            db: Database ì¸ìŠ¤í„´ìŠ¤
        """
        self.db = db
        self.config = get_config()
        self.client = GeminiClient()
        self.sampler = Sampler()
        self.pattern_manager = PatternManager(self.client)
        self.splitter = Splitter()
        self.filename_generator = FilenameGenerator(self.db)
        
        # ìºì‹œ ë””ë ‰í† ë¦¬
        self.cache_dir = Path("data/cache/chapter_split")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("ChapterSplitRunner initialized")
    
    def get_pending_files(self, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """Stage 4 ëŒ€ê¸° ì¤‘ì¸ íŒŒì¼ ì¡°íšŒ (M-45: Force/Retry ì§€ì›)"""
        conn = self.db.connect()
        cursor = conn.cursor()
        
        # [M-45 ë³´ê°•] stage1_metaê°€ 1ì¸ë°, stage4_splitì´ 0ì´ê±°ë‚˜, 
        # í˜¹ì€ í™”ìˆ˜ ì •í•©ì„±ì´ ì‹¤íŒ¨í•˜ì—¬ ì¬ì‘ì—…ì´ í•„ìš”í•œ íŒŒì¼ì„ ëª¨ë‘ ê°€ì ¸ì˜´
        query = """
            SELECT f.id, f.file_path, f.file_name, f.file_hash, f.encoding
            FROM files f
            JOIN processing_state ps ON f.id = ps.file_id
            WHERE ps.stage1_meta = 1 
            AND (ps.stage4_split = 0 OR ps.stage4_split = 1) -- í…ŒìŠ¤íŠ¸ ë° ì¬ë¶„ì„ì„ ìœ„í•´ ì™„ë£Œëœ íŒŒì¼ë„ í¬í•¨
            AND f.is_duplicate = 0 AND f.file_ext IN ('.txt', '.epub')
            ORDER BY ps.stage4_split ASC, f.id ASC -- ë¯¸ì™„ë£Œ íŒŒì¼ì„ ìš°ì„ ìˆœìœ„ë¡œ
        """
        
        if limit:
            query += f" LIMIT {limit}"
        
        cursor.execute(query)
        rows = cursor.fetchall()
        
        files = []
        for row in rows:
            files.append({
                "file_id": row[0],
                "file_path": row[1],
                "file_name": row[2],
                "file_hash": row[3],
                "encoding": row[4]
            })
        
        logger.info(f"Found {len(files)} files pending for Stage 4")
        return files
    
    def split_chapters(self, file_info: Dict[str, Any]) -> Dict[str, Any]:
        """ì±•í„° ë¶„í•  ì‹¤í–‰
        
        Args:
            file_info: íŒŒì¼ ì •ë³´
        
        Returns:
            ë¶„í•  ê²°ê³¼ {"chapters": List[Chapter], "summary": dict}
        """
        file_path = file_info["file_path"]
        file_hash = file_info["file_hash"]
        encoding = file_info.get("encoding", "utf-8") or "utf-8"
        
        if file_path.lower().endswith('.epub'):
            # 1. EPUB ë‚´ë¶€ ì±•í„° ë¶„ì„ (Duokan ë“± í‘œì¤€ êµ¬ì¡°)
            logger.info(f"   -> EPUB ë‚´ë¶€ êµ¬ì¡° ì •ë°€ ë¶„ì„ ì¤‘...")
            from ebooklib import epub
            book = epub.read_epub(file_path)
            chapters = []
            
            # Spine ìˆœì„œëŒ€ë¡œ ë³¸ë¬¸ ì•„ì´í…œë§Œ ì¶”ì¶œ
            cid = 1
            content_items = []
            
            # Spineì— ë“±ë¡ëœ ì•„ì´í…œ ID ëª©ë¡
            spine_ids = [s[0] for s in book.spine if isinstance(s, tuple)]
            
            for item_id in spine_ids:
                item = book.get_item_with_id(item_id)
                if not item or item.get_type() != 9: # ITEM_DOCUMENT
                    continue
                
                name = item.get_name().lower()
                # ë¹„ë³¸ë¬¸ ì„¹ì…˜ ì œì™¸ (M-32)
                if any(x in name for x in ['cover', 'nav', 'toc', 'titlepage', 'metadata']):
                    continue
                
                content = item.get_content().decode('utf-8', errors='ignore')
                
                # ë³¸ë¬¸ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì œì™¸ (ì˜ˆ: ë‹¨ìˆœ ì´ë¯¸ì§€ í˜ì´ì§€ë‚˜ ê³µë°±)
                text_only = re.sub(r'<[^>]*>', '', content).strip()
                if len(text_only) < 50 and 'img' not in content.lower():
                    continue
                
                # ì œëª© ì¶”ì¶œ
                title = item.get_name()
                match = re.search(r'<(?:h1|h2|title)[^>]*>(.*?)</(?:h1|h2|title)>', content, re.IGNORECASE | re.DOTALL)
                if match:
                    title = re.sub(r'<[^>]*>', '', match.group(1)).strip()
                
                # ì œëª©ì— ìˆœë²ˆ ë¶€ì—¬ (M-32)
                # ë§Œì•½ ì œëª©ì— ì´ë¯¸ ìˆ«ìê°€ ìˆë‹¤ë©´ ìµœëŒ€í•œ í™œìš©, ì—†ë‹¤ë©´ [cid] ì¶”ê°€
                if not re.search(r'\d+', title):
                    display_title = f"[{cid}] {title}"
                else:
                    display_title = title
                
                chapters.append(Chapter(
                    cid=cid,
                    title=display_title,
                    subtitle="",
                    body=content,
                    length=len(content)
                ))
                cid += 1
            
            chapter_pattern = "EPUB_STRUCTURE"
            subtitle_pattern = None
        else:
            # 1. ìƒ˜í”Œ ì¶”ì¶œ (M-16: Dynamic Encoding ì ìš©)
            logger.info(f"   -> ìƒ˜í”Œ ì¶”ì¶œ ì¤‘... (30ê°œ ê· ë“± ìƒ˜í”Œ, ì¸ì½”ë”©: {encoding})")
            samples = self.sampler.extract_samples(file_path, encoding=encoding)
            
            # 2. AI íŒ¨í„´ ë¶„ì„ (M-28: íŒŒì¼ëª… íŒíŠ¸ í™œìš©)
            logger.info(f"   -> AI íŒ¨í„´ ë¶„ì„ ì¤‘...")
            chapter_pattern, subtitle_pattern = self.pattern_manager.find_best_pattern(
                file_path,
                samples,
                filename=file_info["file_name"],
                encoding=encoding
            )
            
            if not chapter_pattern:
                raise ValueError("ì±•í„° íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # 3. ì±•í„° ë¶„í• 
            logger.info(f"   -> ì±•í„° ë¶„í•  ì¤‘...")
            chapters = list(self.splitter.split(file_path, chapter_pattern, subtitle_pattern, encoding=encoding))
            
            # 3-1. ì •í•©ì„± ê²€ì¦ ë° ìë™ ì¬ë¶„ì„ (M-29/45/49: Zero Tolerance 100% Match)
            # Enhanced with multi-signal recovery: pattern â†’ verify â†’ gaps â†’ title candidates â†’ consensus
            nums = re.findall(r'\d+', file_info["file_name"])
            expected_count = int(nums[-1]) if nums else 0
            
            reconciliation_log = []
            
            # Enhanced recovery loop with multi-signal detection
            # ë¶€ì¡±í•˜ê±°ë‚˜(Under) ë„˜ì¹  ë•Œ(Over) ëª¨ë‘ ì •ë°€ ë¶„ì„ íŠ¸ë¦¬ê±°
            retry_count = 0
            title_candidates_used = False
            
            while expected_count > 0 and len(chapters) != expected_count and retry_count < self.MAX_RETRIES:
                retry_count += 1
                logger.error(f"   âŒ [Mismatch] í™”ìˆ˜ ë¶ˆì¼ì¹˜ ê°ì§€ ({len(chapters)}/{expected_count}). ì¬ì‹œë„({retry_count}/{self.MAX_RETRIES})ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
                
                # ê°€ì´ë“œ íŒíŠ¸ ì¤€ë¹„
                missing = self._find_missing_episodes(chapters, expected_count)
                reconciliation_log.append(f"ì‹œë„ {retry_count}: {len(chapters)}í™” ì¶”ì¶œ (ê¸°ëŒ€ {expected_count})")
                
                # Get current match positions for gap analysis
                matches = self.splitter.find_matches_with_pos(file_path, chapter_pattern, encoding=encoding)
                
                # ë™ì  ê°­ ë¶„ì„ ë° íŒ¨í„´ ë³´ê°•
                refined_pattern = self.pattern_manager.refine_pattern_with_goal_v3(
                    file_path,
                    chapter_pattern,
                    expected_count,
                    encoding=encoding
                )
                
                if refined_pattern != chapter_pattern:
                    chapter_pattern = refined_pattern
                    logger.info("   -> [Self-Healing] ìˆ˜ì •ëœ íŒ¨í„´ìœ¼ë¡œ ì¬ë¶„í•  ì¤‘...")
                    chapters = list(self.splitter.split(file_path, chapter_pattern, subtitle_pattern, encoding=encoding))
                    
                    # If still missing after pattern refinement, try title candidates (on later retries)
                    if retry_count >= self.TITLE_CANDIDATE_RETRY_THRESHOLD and len(chapters) < expected_count:
                        logger.info("   -> [Fallback] íƒ€ì´í‹€ í›„ë³´ íƒì§€ ì‹œë„ ì¤‘...")
                        missing_count = expected_count - len(chapters)
                        
                        # Find gaps using dynamic detection
                        gaps = self.pattern_manager.find_dynamic_gaps(file_path, matches, expected_count)
                        
                        # Extract title candidates from top gaps
                        all_candidates = []
                        for gap in gaps[:self.MAX_GAPS_TO_ANALYZE]:
                            sample = self.sampler.extract_samples_from(
                                file_path, gap['start'], length=30000, encoding=encoding
                            )
                            if sample:
                                candidates = self.pattern_manager.extract_title_candidates(
                                    sample, chapter_pattern
                                )
                                all_candidates.extend(candidates)
                        
                        if all_candidates:
                            # Try splitting with explicit title candidates
                            logger.info(f"   -> [Consensus] {len(all_candidates)} íƒ€ì´í‹€ í›„ë³´ë¡œ ì¬ë¶„í•  ì‹œë„...")
                            chapters = list(self.splitter.split(
                                file_path, chapter_pattern, subtitle_pattern, 
                                encoding=encoding, title_candidates=all_candidates
                            ))
                            title_candidates_used = True
                            reconciliation_log.append(f"íƒ€ì´í‹€ í›„ë³´ {len(all_candidates)}ê°œ ì‚¬ìš©")
                else:
                    logger.warning("   -> íŒ¨í„´ ë³´ê°•ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì‹œë„ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
            
            # ìµœì¢… ì •í•©ì„± ë¡œê·¸ ê¸°ë¡
            if expected_count > 0 and len(chapters) != expected_count:
                reason = f"ìµœì¢… í™”ìˆ˜ ë¶ˆì¼ì¹˜: ë³´ìœ  {len(chapters)} / ì›¹(ë˜ëŠ” íŒíŠ¸) {expected_count}"
                logger.error(f"   -> [Strict Match Fail] {reason}")
                reconciliation_log.append(reason)
                # ëˆ„ë½ëœ íšŒì°¨ ì •ë³´ ì¶”ê°€
                missing = self._find_missing_episodes(chapters, expected_count)
                if missing:
                    reconciliation_log.append(f"ëˆ„ë½ ì˜ì‹¬: {', '.join(map(str, missing[:10]))} ë“±")
                
                # Log recovery methods used
                if title_candidates_used:
                    reconciliation_log.append("ë³µêµ¬ ë°©ë²•: íŒ¨í„´ + íƒ€ì´í‹€ í›„ë³´ (consensus)")
            elif expected_count > 0:
                logger.info(f"   âœ… í™”ìˆ˜ 100% ì¼ì¹˜ í™•ì¸: {len(chapters)}í™” (Perfect Match)")
                reconciliation_log.append(f"ì •í•©ì„± 100% ì¼ì¹˜ ({len(chapters)}í™”)")
                if title_candidates_used:
                    reconciliation_log.append("ë³µêµ¬ ë°©ë²•: íƒ€ì´í‹€ í›„ë³´ (consensus) ì‚¬ìš©ë¨")
            
            file_info["reconciliation_log"] = "\n".join(reconciliation_log)
            self._verify_chapter_count(file_info["file_name"], len(chapters), chapters)
        
        logger.info(f"   âœ… ì´ {len(chapters)}ê°œ ì±•í„° í™•ì¸ ì™„ë£Œ")
        
        # 4. ì±•í„° ì œëª© ë¶„ì„ (ë³¸í¸/ì™¸ì „/ì—í•„ë¡œê·¸ ë¶„ë¥˜)
        summary = self._analyze_chapter_types(chapters)
        
        # 5. ê²°ê³¼ ì €ì¥
        result = {
            "chapters": [
                {
                    "cid": ch.cid,
                    "title": ch.title,
                    "subtitle": ch.subtitle,
                    "length": ch.length,
                    "chapter_type": ch.chapter_type
                }
                for ch in chapters
            ],
            "summary": summary,
            "patterns": {
                "chapter_pattern": chapter_pattern,
                "subtitle_pattern": subtitle_pattern
            },
            "reconciliation_log": file_info.get("reconciliation_log", "")
        }
        
        # ìºì‹œ ì €ì¥
        cache_path = self.cache_dir / f"{file_hash}.json"
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        logger.info(f"   âœ… ìºì‹œ ì €ì¥: {cache_path}")
        
        return result
    
    def _analyze_chapter_types(self, chapters: List[Chapter]) -> Dict[str, Any]:
        """ì±•í„° ì œëª© ë¶„ì„í•˜ì—¬ ë³¸í¸/ì™¸ì „/ì—í•„ë¡œê·¸ ë¶„ë¥˜
        
        Args:
            chapters: ì±•í„° ë¦¬ìŠ¤íŠ¸
        
        Returns:
            {"ë³¸í¸": {"start": 1, "end": 340, "count": 340}, ...}
        """
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
        main_keywords = ["í™”", "chapter", "ì œ", "ep"]
        extra_keywords = ["ì™¸ì „", "ë²ˆì™¸", "íŠ¹ë³„í¸", "side story"]
        # "ì™„ê²°"ì€ ë³¸í¸ ë§ˆì§€ë§‰í™”ì— ìì£¼ ë¶™ìœ¼ë¯€ë¡œ ì—í•„ë¡œê·¸ í‚¤ì›Œë“œì—ì„œ ì œì™¸ (ë‹¨ë… ì‚¬ìš© ì‹œ ì—í•„ë¡œê·¸ ì·¨ê¸‰ ê³ ë ¤)
        epilogue_keywords = ["ì—í•„ë¡œê·¸", "epilogue", "í›„ì¼ë‹´"]
        author_keywords = ["ì‘ê°€ì˜ ë§", "ì‘ê°€ í›„ê¸°", "í›„ê¸°"]
        
        summary = {
            "ë³¸í¸": {"chapters": [], "count": 0},
            "ì™¸ì „": {"chapters": [], "count": 0},
            "ì—í•„ë¡œê·¸": {"chapters": [], "count": 0},
            "ì‘ê°€ì˜ ë§": {"chapters": [], "count": 0},
            "ê¸°íƒ€": {"chapters": [], "count": 0},
            "total": len(chapters)
        }
        
        for ch in chapters:
            title_lower = ch.title.lower()
            
            # ì‘ê°€ì˜ ë§
            if any(kw in title_lower for kw in author_keywords):
                ch.chapter_type = "ì‘ê°€ì˜ ë§"
                summary["ì‘ê°€ì˜ ë§"]["chapters"].append(ch.cid)
            
            # ì—í•„ë¡œê·¸
            elif any(kw in title_lower for kw in epilogue_keywords):
                ch.chapter_type = "ì—í•„ë¡œê·¸"
                summary["ì—í•„ë¡œê·¸"]["chapters"].append(ch.cid)
            
            # ì™¸ì „
            elif any(kw in title_lower for kw in extra_keywords):
                ch.chapter_type = "ì™¸ì „"
                summary["ì™¸ì „"]["chapters"].append(ch.cid)
            
            # ë³¸í¸ (ê¸°ë³¸ê°’)
            else:
                ch.chapter_type = "ë³¸í¸"
                summary["ë³¸í¸"]["chapters"].append(ch.cid)
        
        # ê° íƒ€ì…ë³„ ì‹œì‘/ë í™”ìˆ˜ ê³„ì‚°
        for type_name, info in summary.items():
            if type_name == "total":
                continue
            
            if info["chapters"]:
                info["count"] = len(info["chapters"])
                info["start"] = min(info["chapters"]) + 1  # cidëŠ” 0ë¶€í„°, í™”ìˆ˜ëŠ” 1ë¶€í„°
                info["end"] = max(info["chapters"]) + 1
            else:
                info["start"] = 0
                info["end"] = 0
        
        logger.info(f"   ğŸ“Š ì±•í„° ë¶„ë¥˜:")
        logger.info(f"      ë³¸í¸: {summary['ë³¸í¸']['count']}ê°œ ({summary['ë³¸í¸']['start']}~{summary['ë³¸í¸']['end']}í™”)")
        if summary['ì™¸ì „']['count'] > 0:
            logger.info(f"      ì™¸ì „: {summary['ì™¸ì „']['count']}ê°œ ({summary['ì™¸ì „']['start']}~{summary['ì™¸ì „']['end']}í™”)")
        if summary['ì—í•„ë¡œê·¸']['count'] > 0:
            logger.info(f"      ì—í•„ë¡œê·¸: {summary['ì—í•„ë¡œê·¸']['count']}ê°œ")
        if summary['ì‘ê°€ì˜ ë§']['count'] > 0:
            logger.info(f"      ì‘ê°€ì˜ ë§: {summary['ì‘ê°€ì˜ ë§']['count']}ê°œ")
        
        return summary
    
    def _verify_chapter_count(self, filename: str, actual_count: int, chapters: List[Chapter]) -> None:
        """íŒŒì¼ëª…ì˜ í™”ìˆ˜ íŒíŠ¸ì™€ ì‹¤ì œ ë¶„í• ëœ ì±•í„° ìˆ˜ ë¹„êµ ê²€ì¦ (M-28/45/48)"""
        nums = re.findall(r'\d+', filename)
        if not nums:
            return
        
        expected_count = int(nums[-1])
        if expected_count > 0:
            diff = actual_count - expected_count
            if diff < 0:
                logger.error("=" * 60)
                logger.error(f"âŒ [ì •í•©ì„± ì‹¤íŒ¨] í™”ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤! ({actual_count}/{expected_count})")
                missing = self._find_missing_episodes(chapters, expected_count)
                if missing:
                    logger.error(f"   - ëˆ„ë½ëœ íšŒì°¨ ì˜ˆìƒ: {missing[:20]}{'...' if len(missing)>20 else ''}")
                logger.error("=" * 60)
            elif diff > 0:
                logger.warning("=" * 60)
                logger.warning(f"âš ï¸  [ì •í•©ì„± ê²½ê³ ] í™”ìˆ˜ê°€ ê¸°ëŒ€ì¹˜ë³´ë‹¤ ë§ìŠµë‹ˆë‹¤. ({actual_count}/{expected_count})")
                logger.warning("   - ì¤‘ë³µ ë§¤ì¹­ì´ë‚˜ ì™¸ì „ì´ í¬í•¨ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                logger.warning("=" * 60)
            else:
                logger.info(f"   âœ… í™”ìˆ˜ 100% ì¼ì¹˜ í™•ì¸: {actual_count}í™” (Perfect Match)")

    def _find_missing_episodes(self, chapters: List[Chapter], expected_count: int) -> List[int]:
        """ì¶”ì¶œëœ ì±•í„°ë“¤ ì‚¬ì´ì—ì„œ ë¹ ì§„ ë²ˆí˜¸ íƒì§€ (M-48)"""
        found_nums = set()
        for ch in chapters:
            # ì œëª©ì—ì„œ ì²« ë²ˆì§¸ ìˆ«ì ì¶”ì¶œ
            match = re.search(r'(\d+)', ch.title)
            if match:
                found_nums.add(int(match.group(1)))
        
        missing = []
        for i in range(1, expected_count + 1):
            if i not in found_nums:
                missing.append(i)
        return missing

    def save_to_db(self, file_id: int, result: Dict[str, Any]) -> None:
        """DBì— ì €ì¥
        
        Args:
            file_id: íŒŒì¼ ID
            result: ë¶„í•  ê²°ê³¼
        """
        conn = self.db.connect()
        cursor = conn.cursor()
        
        summary = result["summary"]
        
        # novels í…Œì´ë¸” ì—…ë°ì´íŠ¸ (ì±•í„° ìˆ˜ ë° ì •í•©ì„± ë¡œê·¸ ì €ì¥)
        reconcile_log = result.get("reconciliation_log", "")
        cursor.execute("""
            UPDATE novels
            SET chapter_count = ?, reconciliation_log = ?
            WHERE id = (SELECT novel_id FROM files WHERE id = ?)
        """, (summary["total"], reconcile_log, file_id))
        
        # processing_state ì—…ë°ì´íŠ¸ (ì •í•©ì„± ë¡œê·¸ í¬í•¨)
        cursor.execute("""
            UPDATE processing_state
            SET stage4_split = 1, last_stage = 'stage4', reconciliation_log = ?
            WHERE file_id = ?
        """, (reconcile_log, file_id))
        
        conn.commit()
        
        # [M-49] ë¶„ì„ ì™„ë£Œ í›„ ì‹¤ë¬¼ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ íŒŒì¼ëª… ìµœì¢… ë™ê¸°í™” (Sync Original TXT)
        try:
            logger.info(f"   -> [Sync] ì‹¤ë¬¼ ê¸°ë°˜ íŒŒì¼ëª… ìµœì¢… ë™ê¸°í™” ì‹œë„ ì¤‘... (File ID: {file_id})")
            self.filename_generator.process_single_file(file_id)
        except Exception as e:
            logger.error(f"   âŒ [Sync Fail] íŒŒì¼ëª… ë™ê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def run(self, limit: Optional[int] = None) -> Dict[str, int]:
        """Stage 4 ì‹¤í–‰
        
        Args:
            limit: ì²˜ë¦¬í•  ìµœëŒ€ íŒŒì¼ ìˆ˜
        
        Returns:
            {"total": int, "success": int, "failed": int}
        """
        logger.info("=" * 50)
        logger.info("Stage 4: Chapter Splitting")
        logger.info("=" * 50)
        
        # ëŒ€ê¸° íŒŒì¼ ì¡°íšŒ
        files = self.get_pending_files(limit)
        
        if not files:
            logger.warning("No files to process")
            return {"total": 0, "success": 0, "failed": 0}
        
        # ì²˜ë¦¬
        success_count = 0
        failed_count = 0
        
        for i, file_info in enumerate(files):
            file_path_obj = Path(file_info['file_path'])
            logger.info(f"[{i+1}/{len(files)}] {file_path_obj.name}")
            
            if not file_path_obj.exists():
                logger.warning(f"   âš ï¸  íŒŒì¼ì´ ë””ìŠ¤í¬ì— ì—†ìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤: {file_path_obj}")
                failed_count += 1 # Treat as failed since it couldn't be processed
                continue
                
            try:
                result = self.split_chapters(file_info)
                self.save_to_db(file_info["file_id"], result)
                success_count += 1
            except Exception as e:
                logger.error(f"Failed to split chapters for {file_path_obj.name}: {e}")
                # [Hotfix v2] ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ì˜¤ì—¼ëœ ìºì‹œê°€ ìˆë‹¤ë©´ ì‚­ì œ (Stage 5 ì˜¤ì—¼ ë°©ì§€)
                cache_path = self.cache_dir / f"{file_info['file_hash']}.json"
                if cache_path.exists():
                    try:
                        cache_path.unlink()
                        logger.info(f"   ğŸ—‘ï¸  ì‹¤íŒ¨í•œ íŒŒì¼ì˜ ê¸°ì¡´ ìºì‹œë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
                    except: pass
                
                failed_count += 1
        
        logger.info("=" * 50)
        logger.info(f"âœ… Stage 4 Complete: {success_count} success, {failed_count} failed")
        logger.info("=" * 50)
        
        return {
            "total": len(files),
            "success": success_count,
            "failed": failed_count
        }
